[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nAll participants in TileDB spaces are expected to adhere to high standards of professionalism in all interactions. These standards include, but are not limited to, the specific behaviors outlined below. Upholding these standards is fundamental to our commitment to create a welcoming, positive, and inclusive environment for everyone. We as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\nOur Standards\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nAll of these serve to help the make the project better, but also serve to make the experience of participating in the project better as well.\nExamples of unacceptable behavior by participants include:\n\nSexist, racist, and other exclusionary language.\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment or intimidation\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\nResponsibilities\nProject maintainers are responsible for maintaining, upholding, and observing these standards.\n\n\nReporting\nPlease contact conduct@tiledb.com. All code of conduct reports will be kept in confidence.\n\n\nAttribution\nThis document is adapted from the Bokeh code of conduct, which is in turn adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4"
  },
  {
    "objectID": "external/blosc/README-VERSIONS.html",
    "href": "external/blosc/README-VERSIONS.html",
    "title": "",
    "section": "",
    "text": "The blosc files here were copied from release 1.21.0 of the c-blosc repository of the Blosc project. All we are using here is the byte shuffle code, so we do not have the library itself as an external dependency. We only use a selected subset of the sources; they are used without modification.\n\nhttps://github.com/Blosc/c-blosc\nhttps://github.com/Blosc/c-blosc/releases/tag/v1.21.0"
  },
  {
    "objectID": "tiledb/sm/query/DELETES_AND_UPDATES.html",
    "href": "tiledb/sm/query/DELETES_AND_UPDATES.html",
    "title": "Deletes",
    "section": "",
    "text": "Deletes\nDeletes are implemented as a query type, TILEDB_QUERY_TYPE_DELETE, which requires a QueryCondition to be set on the Query. Upon query submission, TileDB creates a delete commit file containing a serialized version of the query condition. All attribute values matching the query condition expression are considered to be deleted as of the timestamp of the commit file. During reads, any delete commit files within the query range are loaded and deserialized, then the negation of the stored query condition is applied to the final result."
  },
  {
    "objectID": "tiledb/api/c_api_test_support/storage_manager_stub/DIRECTORY.html",
    "href": "tiledb/api/c_api_test_support/storage_manager_stub/DIRECTORY.html",
    "title": "",
    "section": "",
    "text": "The class StorageManager is too large and unwieldy to support unit testing, yet there’s member variable of that type in each Context. This directory defines a stub class for StorageManager that allows testing with Context in cases that don’t require anything but a stub.\n\n\nThe name of the header may not change; they’re fixed in other headers. The file storage_manager_override declares the class StorageManagerStub and puts that declaration into storage_manager.h. The file storage_manager_declaration_override.h specializes a template that has the effect of assigning the alias StorageManager to StorageManagerStub`.\n\n\n\nIn order to activate the override, the directory path must be modified to reach this directory so that the override header will be found on the include path. The file storage_manager_override.cmake defines an object library that sets up the include path.\n\n\n\nThe file storage_manager_stub.cc is empty, but is needed to define an object library for the overridden storage manager."
  },
  {
    "objectID": "tiledb/api/c_api_test_support/storage_manager_stub/DIRECTORY.html#storage-manager-stub",
    "href": "tiledb/api/c_api_test_support/storage_manager_stub/DIRECTORY.html#storage-manager-stub",
    "title": "",
    "section": "",
    "text": "The class StorageManager is too large and unwieldy to support unit testing, yet there’s member variable of that type in each Context. This directory defines a stub class for StorageManager that allows testing with Context in cases that don’t require anything but a stub.\n\n\nThe name of the header may not change; they’re fixed in other headers. The file storage_manager_override declares the class StorageManagerStub and puts that declaration into storage_manager.h. The file storage_manager_declaration_override.h specializes a template that has the effect of assigning the alias StorageManager to StorageManagerStub`.\n\n\n\nIn order to activate the override, the directory path must be modified to reach this directory so that the override header will be found on the include path. The file storage_manager_override.cmake defines an object library that sets up the include path.\n\n\n\nThe file storage_manager_stub.cc is empty, but is needed to define an object library for the overridden storage manager."
  },
  {
    "objectID": "tiledb/api/DIRECTORY.html",
    "href": "tiledb/api/DIRECTORY.html",
    "title": "",
    "section": "",
    "text": "This directory is the (future) home of the C API and the C++ API. The API’s are the leaves of the dependency tree insofar as code organization is concerned. API code may reference anything in the code base, but nothing here should be referenced by anything but user code.\nAlso included here are API support functions and API-specific tests. The scope of tests are those that exercise the API narrowly, that is, unit tests of the API code itself insofar as it’s possible to write them. They should be more than unit tests of the major classes they use but less than full integration tests of the library.\nFunctions outside of the API itself should be defined in namespace tiledb::api.\n\n\nThe C API has started to move to this directory and is currently in transition.\nThis directory is for declaration and definition of C API functions, but is not for any common infrastructure they share.\n\n\n\nThis directory contains support code shared generally between implementations of C API functions.\n\n\n\nThe C++ API has not yet moved to this directory."
  },
  {
    "objectID": "tiledb/api/DIRECTORY.html#api",
    "href": "tiledb/api/DIRECTORY.html#api",
    "title": "",
    "section": "",
    "text": "This directory is the (future) home of the C API and the C++ API. The API’s are the leaves of the dependency tree insofar as code organization is concerned. API code may reference anything in the code base, but nothing here should be referenced by anything but user code.\nAlso included here are API support functions and API-specific tests. The scope of tests are those that exercise the API narrowly, that is, unit tests of the API code itself insofar as it’s possible to write them. They should be more than unit tests of the major classes they use but less than full integration tests of the library.\nFunctions outside of the API itself should be defined in namespace tiledb::api.\n\n\nThe C API has started to move to this directory and is currently in transition.\nThis directory is for declaration and definition of C API functions, but is not for any common infrastructure they share.\n\n\n\nThis directory contains support code shared generally between implementations of C API functions.\n\n\n\nThe C++ API has not yet moved to this directory."
  },
  {
    "objectID": "tiledb/api/c_api_support/DIRECTORY.html",
    "href": "tiledb/api/c_api_support/DIRECTORY.html",
    "title": "",
    "section": "",
    "text": "This directory contains support code shared generally between implementations of C API functions.\n\nclass Handle is the means through which user code refers to objects inside the library. These are managed wrappers around pointers to internal objects."
  },
  {
    "objectID": "tiledb/api/c_api_support/DIRECTORY.html#c-api-support",
    "href": "tiledb/api/c_api_support/DIRECTORY.html#c-api-support",
    "title": "",
    "section": "",
    "text": "This directory contains support code shared generally between implementations of C API functions.\n\nclass Handle is the means through which user code refers to objects inside the library. These are managed wrappers around pointers to internal objects."
  },
  {
    "objectID": "format_spec/array_file_hierarchy.html",
    "href": "format_spec/array_file_hierarchy.html",
    "title": "Array File Hierarchy",
    "section": "",
    "text": "An array is a folder with the following structure:\nmy_array                                # array folder\n    |_ __schema                         # array schema folder\n    |_ __fragments                      # array fragments folder\n          |_ &lt;timestamped_name&gt;         # fragment folder\n          |_ ...\n    |_ __commits                        # array commits folder\n          |_ &lt;timestamped_name&gt;.wrt     # fragment write file\n          |_ ...\n          |_ &lt;timestamped_name&gt;.del     # delete commit file\n          |_ ...\n          |_ &lt;timestamped_name&gt;.upd     # update commit file\n          |_ ...\n          |_ &lt;timestamped_name&gt;.vac     # fragment vacuum file\n          |_ ...\n          |_ &lt;timestamped_name&gt;.con     # consolidated commits file\n          |_ ...\n          |_ &lt;timestamped_name&gt;.ign     # ignore file for consolidated commits file\n    |_ __fragment_meta                  \n          |_ &lt;timestamped_name&gt;.meta    # consol. fragment meta file\n          |_ ...                  \n    |_ __meta                           # array metadata folder\n    |_ __labels                         # dimension label folder\n\nA &lt;timestamped_name&gt; above has format __t1_t2_uuid_v, where\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nInside the array folder, you can find the following:\n\nArray schema folder __schema.\nInside of a fragments folder, any number of fragment folders &lt;timestamped_name&gt;.\nInside of a commit folder, an empty file &lt;timestamped_name&gt;.wrt associated with every fragment folder &lt;timestamped_name&gt;, where &lt;timestamped_name&gt; is common for the folder and the WRT file. This is used to indicate that fragment &lt;timestamped_name&gt; has been committed (i.e., its write process finished successfully) and it is ready for use by TileDB. If the WRT file does not exist, the corresponding fragment folder is ignored by TileDB during the reads.\nInside the same commit folder, any number of delete commit files of the form &lt;timestamped_name&gt;.del.\nInside the same commit folder, any number of update commit files of the form &lt;timestamped_name&gt;.upd.\nInside the same commit folder, any number of consolidated commits files of the form &lt;timestamped_name&gt;.con.\nInside the same commit folder, any number of ignore files of the form &lt;timestamped_name&gt;.ign.\nInside of a fragment metadata folder, any number of consolidated fragment metadata files of the form &lt;timestamped_name&gt;.meta.\nArray metadata folder __meta.\nInside of a labels folder, additional TileDB arrays storing dimension label data."
  },
  {
    "objectID": "format_spec/ignore_file.html",
    "href": "format_spec/ignore_file.html",
    "title": "Ignore File",
    "section": "",
    "text": "A ignore file has name __t1_t2_uuid_v.ign and is located in the array commit folder:\nmy_array                           # array folder\n   |_ ....\n   |_ __commits                    # array commit folder\n         |___t1_t2_uuid_v.ign      # ignore file\nor in the array metadata folder:\nIn the file name:\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nThe ignore file is a simple text file where each line contains a URI string. The URI is the relative URI based on the top level array URI.\nIgnored files are used to specify commit files that should be ignored. The timestamps for the file is used to identify the timestamp range that is included inside of the file (with the smaller/bigger timestamp from contained elements). That way if an array is opened at a time that doesn’t intersect the timestamp range of the file, it can be omitted. All ignore files that intersect the array open start/end times will be loaded when the array is opened. There are a few situations where those files are required, all linked to consolidated commits files. One of them is when fragments are vacuumed after consolidation. If any fragments were included a consolidated commits file, they now should be added to an ignore file so that the system doesn’t try to load that fragments. Another is when a fragment that is included in a consolidated commits file is deleted, it needs to be added to an ignore file so that the array can still be opened.\nOnce a new consolidated commits file get generated, it will not include any of the fragments that were deleted or vacuumed. At that point, the ignore files and the other consolidated commits files can be vacuumed.\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nURI 1 followed by a new line character\nuint8_t[]\nURI 1 to be ignored\n\n\n…\n…\n…\n\n\nURI N followed by a new line character\nuint8_t[]\nURI N to be ignored"
  },
  {
    "objectID": "format_spec/group_file_hierarchy.html",
    "href": "format_spec/group_file_hierarchy.html",
    "title": "Group File Hierarchy",
    "section": "",
    "text": "A TileDB group is a folder with a single file in it:\nmy_group                       # Group folder\n    |_ __tiledb_group.tdb      # Empty group file\n    |_ __group                 # Group folder\n        |_ &lt;timestamped_name&gt;  # Timestamped group file detailing members\n    |_ __meta                  # group metadata folder\nFile __tiledb_group.tdb is empty and it is merely used to indicate that my_group is a TileDB group.\nInside the group folder, you can find the following:\n\nGroup details folder __group.\nGroup metadata folder __meta."
  },
  {
    "objectID": "format_spec/FORMAT_SPEC.html",
    "href": "format_spec/FORMAT_SPEC.html",
    "title": "Format Specification",
    "section": "",
    "text": "Notes:"
  },
  {
    "objectID": "format_spec/FORMAT_SPEC.html#table-of-contents",
    "href": "format_spec/FORMAT_SPEC.html#table-of-contents",
    "title": "Format Specification",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nArray\n\nFile hierarchy\nArray Schema\nFragment\nArray Metadata\nTile\nGeneric Tile\n\nGroup\n\nFile hierarchy\n\nOther\n\nConsolidated Fragment Metadata File\nFilter Pipeline\nVacuum Pipeline"
  },
  {
    "objectID": "format_spec/filter_pipeline.html",
    "href": "format_spec/filter_pipeline.html",
    "title": "Filter Pipeline",
    "section": "",
    "text": "The filter pipeline has internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nMax chunk size\nuint32_t\nMaximum chunk size within a tile\n\n\nNum filters\nuint32_t\nNumber of filters in pipeline\n\n\nFilter 1\nFilter\nFirst filter\n\n\n…\n…\n…\n\n\nFilter N\nFilter\nNth filter\n\n\n\nFor var size data, the filter pipeline tries to fit integral cells in a chunk. It uses the following heuristic if the cell doesn’t fit:\n\nIf the chunk is not yet at 50% capacity, add the cell to the current chunk.\nIf the chunk is over 50% capacity and adding the cell would make it less than 150% of the maximum chunk size, add it to this chunk.\nElse, start a new chunk."
  },
  {
    "objectID": "format_spec/filter_pipeline.html#main-structure",
    "href": "format_spec/filter_pipeline.html#main-structure",
    "title": "Filter Pipeline",
    "section": "",
    "text": "The filter pipeline has internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nMax chunk size\nuint32_t\nMaximum chunk size within a tile\n\n\nNum filters\nuint32_t\nNumber of filters in pipeline\n\n\nFilter 1\nFilter\nFirst filter\n\n\n…\n…\n…\n\n\nFilter N\nFilter\nNth filter\n\n\n\nFor var size data, the filter pipeline tries to fit integral cells in a chunk. It uses the following heuristic if the cell doesn’t fit:\n\nIf the chunk is not yet at 50% capacity, add the cell to the current chunk.\nIf the chunk is over 50% capacity and adding the cell would make it less than 150% of the maximum chunk size, add it to this chunk.\nElse, start a new chunk."
  },
  {
    "objectID": "format_spec/filter_pipeline.html#filter",
    "href": "format_spec/filter_pipeline.html#filter",
    "title": "Filter Pipeline",
    "section": "Filter",
    "text": "Filter\nThe filter has internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nFilter type\nuint8_t\nType of filter (e.g. TILEDB_FILTER_BZIP2)\n\n\nFilter metadata size\nuint32_t\nNumber of bytes in filter metadata — may be 0.\n\n\nFilter metadata\nFilter Metadata\nFilter metadata, specific to each filter. E.g. compression level for compression filters.\n\n\n\nTileDB supports the following filters: * Float Scaling Filter * XOR Filter * Dictionary Encoding Filter * WEBP Filter"
  },
  {
    "objectID": "format_spec/filter_pipeline.html#filter-options",
    "href": "format_spec/filter_pipeline.html#filter-options",
    "title": "Filter Pipeline",
    "section": "Filter Options",
    "text": "Filter Options\nThe filter options are configuration parameters for the filters that do not change once the array schema has been created.\n\nMain Compressor Options\nFor the compression filters (any of the filter types TILEDB_FILTER_{GZIP,ZSTD,LZ4,RLE,BZIP2,DOUBLE_DELTA,DELTA,DICTIONARY}) the filter options have internal format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nCompressor type\nuint8_t\nType of compression (e.g. TILEDB_BZIP2)\n\n\nCompression level\nint32_t\nCompression level used (ignored by some compressors).\n\n\nReinterpret datatype\nuint8_t\nType to reinterpret data prior to compression. Used for DOUBLE_DELTA and DELTA only.\n\n\n\n\n\nBit-width Reduction Options\nThe filter options for TILEDB_FILTER_BIT_WIDTH_REDUCTION has internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nMax window size\nuint32_t\nMaximum window size in bytes\n\n\n\n\n\nFloat Scale Filter Reduction Options\nThe filter options for TILEDB_FILTER_SCALE_FLOAT has internal format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nScale\ndouble\nScale parameter used for float scaling filter conversion\n\n\nOffset\ndouble\nOffset parameter used for float scaling filter conversion\n\n\nByte width\nuint64_t\nWidth of the stored integer data in bytes\n\n\n\n\n\nPositive Delta Options\nThe filter options for TILEDB_FILTER_POSITIVE_DELTA has internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nMax window size\nuint32_t\nMaximum window size in bytes\n\n\n\n\n\nOther Filter Options\nThe remaining filters (TILEDB_FILTER_{BITSHUFFLE,BYTESHUFFLE,CHECKSUM_MD5,CHECKSUM_256,XOR,DICTIONARY}) do not serialize any options."
  },
  {
    "objectID": "format_spec/consolidated_commits_file.html",
    "href": "format_spec/consolidated_commits_file.html",
    "title": "Consolidated Commits File",
    "section": "",
    "text": "A consolidated commits file has name &lt;timestamped_name&gt;.con and is located here:\nmy_array                              # array folder\n   |_ ....\n   |_ __commits                       # array commits folder\n         |_ &lt;timestamped_name&gt;.con    # consolidated commits file\n         |_ ...\n&lt;timestamped_name&gt; has format __t1_t2_uuid_v, where:\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nThere may be multiple such files in the array commits folder. Each consolidated commits file combines a list of fragments commits, delete or update commits that will be considered together when opening an array. They are timestamped (with the smaller/bigger timestamp from contained elements) so that they might be filtered if the array open start/end time don’t intersect anything inside of the file, but any file where there is any intersection will be loaded. There are two situations when these files are added. The first one is when a user decides to improve the open performance of their array (by reducing the listing size) through commits consolidation. The second one is when a user has specified a maximum fragment size for consolidation and multiple fragments needed to be committed at once, this file format will be used so that an atomic file system operation can be performed to do so.\nWhen fragments that are included inside of a consolidated file are removed, either through vacuuming or other fragment deletion, an ignore file needs to be written so that those commits file can be ignored when opening an array and not cause unnecessary file system operations.\nThere are three types of objects that can be in the file: commits, deletes or updates. For each of those, the first field is the URI of the object, which determines the subsequent serialized content of the data block: - a URI ending with .ok or .wrt is a commit object containing only the fragment URI - a URI ending with .del is a delete object containing the serialized delete condition - a URI ending with .upd is an update object containing the serialized update condition\n\nFragment commit:\n\n\n\n\n\n\n\n\n\nField\nType\nExtension\nDescription\n\n\n\n\nCommit URI followed by a new line character\nuint8_t[]\n.ok, .wrt\nURI\n\n\n\n\n\nDelete commit (see delete commit file for the format of the serialized delete condition):\n\n\n\n\n\n\n\n\n\nField\nType\nExtension\nDescription\n\n\n\n\nURI followed by a new line character\nuint8_t[]\n.del\nURI\n\n\nSerialized delete condition tile size\nuint64_t\n\nDelete condition tile size\n\n\nSerialized delete condition tile\nuint8_t[]\n\nDelete condition tile\n\n\n\n\n\nUpdate commit (see update commit file for the format of the serialized update condition):\n\n\n\n\n\n\n\n\n\nField\nType\nExtension\nDescription\n\n\n\n\nURI followed by a new line character\nuint8_t[]\n.upd\nURI\n\n\nSerialized update condition tile size\nuint64_t\n\nUpdate condition tile size\n\n\nSerialized update condition tile\nuint8_t[]\n\nUpdate condition tile"
  },
  {
    "objectID": "format_spec/enumeration.html",
    "href": "format_spec/enumeration.html",
    "title": "Enumerations",
    "section": "",
    "text": "``my_array                                    # array folder    |  ...    |_ schema                      # ArraySchema directory named__schema|_ enumerations          # Enumeration directory named__enumerations|_ enumeration     # enumeration data with names__uuid_v`\nEnumeration data is stored in a subdirectory of the [array schema][./array_schema.md] directory. Enumerations are stored using [Generic Tiles][./generic_tile.md].\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nVersion number\nuint32_t\nEnumerations version number\n\n\nDatatype\nuint8_t\nThe datatype of the enumeration values\n\n\nCell Val Num\nuint32_t\nThe cell val num of the enumeration values\n\n\nOrdered\nbool\nWhether the enumeration values should be considered ordered\n\n\nData Size\nuint64_t\nThe number of bytes used to store the values\n\n\nData\nuint8_t * Data Size\nThe data for the enumeration values\n\n\nOffsets Size\nuint64_t\nThe number of bytes used to store offsets if cell_var_num is TILEDB_VAR_NUM\n\n\nOffsets\nuint8_t * Offsets Size\nThe offsets data for the enumeration if cell_var_num is TILEDB_VAR_NUM"
  },
  {
    "objectID": "format_spec/enumeration.html#main-structure",
    "href": "format_spec/enumeration.html#main-structure",
    "title": "Enumerations",
    "section": "",
    "text": "``my_array                                    # array folder    |  ...    |_ schema                      # ArraySchema directory named__schema|_ enumerations          # Enumeration directory named__enumerations|_ enumeration     # enumeration data with names__uuid_v`\nEnumeration data is stored in a subdirectory of the [array schema][./array_schema.md] directory. Enumerations are stored using [Generic Tiles][./generic_tile.md].\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nVersion number\nuint32_t\nEnumerations version number\n\n\nDatatype\nuint8_t\nThe datatype of the enumeration values\n\n\nCell Val Num\nuint32_t\nThe cell val num of the enumeration values\n\n\nOrdered\nbool\nWhether the enumeration values should be considered ordered\n\n\nData Size\nuint64_t\nThe number of bytes used to store the values\n\n\nData\nuint8_t * Data Size\nThe data for the enumeration values\n\n\nOffsets Size\nuint64_t\nThe number of bytes used to store offsets if cell_var_num is TILEDB_VAR_NUM\n\n\nOffsets\nuint8_t * Offsets Size\nThe offsets data for the enumeration if cell_var_num is TILEDB_VAR_NUM"
  },
  {
    "objectID": "format_spec/consolidated_fragment_metadata_file.html",
    "href": "format_spec/consolidated_fragment_metadata_file.html",
    "title": "Consolidated Fragment Metadata File",
    "section": "",
    "text": "A consolidated fragment metadata file has name &lt;timestamped_name&gt;.meta and is located here:\nmy_array                              # array folder\n   |_ ....\n   |_ __fragment_meta                 # array fragment metadata folder\n         |_ &lt;timestamped_name&gt;.meta   # consolidated fragment metadata file\n         |_ ...\n&lt;timestamped_name&gt; has format __t1_t2_uuid_v, where:\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nThere may be multiple such files in the array folder. Each consolidated fragment metadata file combines the metadata footers of a set of fragments. It has the following on-disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nURI 1 length\nuint64_t\nNumber of bytes in the string of URI 1\n\n\nURI 1\nuint8_t[]\nURI 1\n\n\nURI 1 offset\nuint64_t\nThe offset in the file where the URI 1 footer begins\n\n\n…\n…\n…\n\n\nURI N length\nuint64_t\nNumber of bytes in the string of URI N\n\n\nURI N\nuint8_t[]\nURI N\n\n\nURI N offset\nuint64_t\nThe offset in the file where the URI N footer begins\n\n\nURI 1 footer\nFooter\nSerialized footer of URI (fragment) 1\n\n\n…\n…\n…\n\n\nURI N footer\nFooter\nSerialized footer of URI (fragment) N"
  },
  {
    "objectID": "format_spec/filters/dictionary_encoding.html",
    "href": "format_spec/filters/dictionary_encoding.html",
    "title": "Dictionary Encoding Filter",
    "section": "",
    "text": "The Dictionary Encoding filter compresses losslessly string data by creating a set of unique strings (called the dictionary) from the input data and substituting the string data on disk with their position in the dictionary. As an example in pseudocode:\ninput_data = \"HG543232\", \"HG543232\", \"HG543232\", \"HG54\", \"HG54\", \"A\", \"HG543232\", \"HG54\"]\n# apply dictionary encoding -&gt;\ndictionary = [\"HG543232\", \"HG54\", \"A\"]\noutput_data = [0, 0, 0, 1, 1, 2, 0, 1]\n\nFilter Enum Value\nThe filter enum value for the Dictionary Encoding filter is 14 (TILEDB_FILTER_DICTIONARY enum).\n\n\nInput and Output Layout\n\nInput is an array of strings [str1|...|strN].\nOutput is an array of integers [num1|...|numN], that are actually the indices in the dictionary of the strings that appeared in input, in the order that they appeared. The integer datasize depends on the input and is chosen between uint8_t, uint16_t, uint32_t or uint64_t as the minimum datasize that can hold the size of the dictionary, aka the number of unique strings in input and stored in output metadata.\nOutput metadata are in the form: [output_datasize|dict_string_datasize|dictionary] where dictionary is in the form: [num_of_strings|size_str1|str1|...|size_strN|strN]. output_datasize refers to the chosen datasize of the integers in output while dict_string_datasize refers to the minimum integer size that can hold the maximum length string in input and is also chosen between uint8_t, uint16_t, uint32_t or uint64_t.\n\nAll the above integers are stored in big-endian format."
  },
  {
    "objectID": "format_spec/filters/float_scale.html",
    "href": "format_spec/filters/float_scale.html",
    "title": "Float Scaling Filter",
    "section": "",
    "text": "The float scale filter converts floating point type data to integer data after first shifting and scaling the data. It is a lossy filter that is often used in combination with other filters that perform well on integer data. The float scale filter stores its input data (with floating point type) as integer values for more compressed storage. For example, given data as a NumPy float64 array, with scale/offset parameters, and a specified byte width of 4: ``` data = np.random.rand(npts) data_b = data.view(np.float64) new_data_b = np.zeros(npts, dtype=np.int32) # byte width is 4 scale, offset\nfor i in range(0, len(data)): # round works like ’s round new_data_b[i] = round((data_b[i] - offset) / scale) ```\nOn read, the float scaling filter will reverse the scale factor and offset, restoring the original floating point data, with a potential loss of precision. Given the previous example: restored_data = np.zeros(npts, dtype=np.float64)    for i in range(0, len(data)):      restored_data[i] = (new_data_b[i] * scale) + offset\nHere’s a small example of the float scaling filter’s behavior on write, then read. Given the parameters scale = 0.25, offset = 10.0, and byte_size = 2, this is how the data is stored and retrieved.\n  input_data (double) = [10.0, 10.2500, 10.7540, 11.0001]\n  stored_data (16 bit integer) = [0, 1, 3, 4]\n  output_data (double) = [10.0, 10.2500, 10.7500, 11.0]\n\n\nThe filter enum value for the float scaling filter is 15 (TILEDB_FILTER_SCALE_FLOAT enum).\n\n\n\nThe input data layout will be an array of floating point numbers. The output data layout will be an array of integer numbers with the pre-specified byte width."
  },
  {
    "objectID": "format_spec/filters/float_scale.html#float-scaling-filter",
    "href": "format_spec/filters/float_scale.html#float-scaling-filter",
    "title": "Float Scaling Filter",
    "section": "",
    "text": "The float scale filter converts floating point type data to integer data after first shifting and scaling the data. It is a lossy filter that is often used in combination with other filters that perform well on integer data. The float scale filter stores its input data (with floating point type) as integer values for more compressed storage. For example, given data as a NumPy float64 array, with scale/offset parameters, and a specified byte width of 4: ``` data = np.random.rand(npts) data_b = data.view(np.float64) new_data_b = np.zeros(npts, dtype=np.int32) # byte width is 4 scale, offset\nfor i in range(0, len(data)): # round works like ’s round new_data_b[i] = round((data_b[i] - offset) / scale) ```\nOn read, the float scaling filter will reverse the scale factor and offset, restoring the original floating point data, with a potential loss of precision. Given the previous example: restored_data = np.zeros(npts, dtype=np.float64)    for i in range(0, len(data)):      restored_data[i] = (new_data_b[i] * scale) + offset\nHere’s a small example of the float scaling filter’s behavior on write, then read. Given the parameters scale = 0.25, offset = 10.0, and byte_size = 2, this is how the data is stored and retrieved.\n  input_data (double) = [10.0, 10.2500, 10.7540, 11.0001]\n  stored_data (16 bit integer) = [0, 1, 3, 4]\n  output_data (double) = [10.0, 10.2500, 10.7500, 11.0]\n\n\nThe filter enum value for the float scaling filter is 15 (TILEDB_FILTER_SCALE_FLOAT enum).\n\n\n\nThe input data layout will be an array of floating point numbers. The output data layout will be an array of integer numbers with the pre-specified byte width."
  },
  {
    "objectID": "format_spec/array_schema.html",
    "href": "format_spec/array_schema.html",
    "title": "Array Schema",
    "section": "",
    "text": "The current array schema version(&gt;=10) is a folder called __schema located here:\nmy_array                            # array folder\n   |  ...\n   |_ __schema                      # array schema folder\n         |_ &lt;timestamped_name&gt;      # array schema file\n         |_ ...\n&lt;timestamped_name&gt; has format __timestamp_timestamp_uuid, where: * timestamp is timestamp in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC) * uuid is a unique identifier\nThe array schema folder can contain:\n\nAny number of array schema files"
  },
  {
    "objectID": "format_spec/array_schema.html#current-array-schema-version",
    "href": "format_spec/array_schema.html#current-array-schema-version",
    "title": "Array Schema",
    "section": "",
    "text": "The current array schema version(&gt;=10) is a folder called __schema located here:\nmy_array                            # array folder\n   |  ...\n   |_ __schema                      # array schema folder\n         |_ &lt;timestamped_name&gt;      # array schema file\n         |_ ...\n&lt;timestamped_name&gt; has format __timestamp_timestamp_uuid, where: * timestamp is timestamp in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC) * uuid is a unique identifier\nThe array schema folder can contain:\n\nAny number of array schema files"
  },
  {
    "objectID": "format_spec/array_schema.html#previous-array-schema-version",
    "href": "format_spec/array_schema.html#previous-array-schema-version",
    "title": "Array Schema",
    "section": "Previous Array Schema Version",
    "text": "Previous Array Schema Version\nThe previous array schema version(&lt;=9) has a file named __array_schema.tdb and is located here:\nmy_array                   # array folder\n   |_ ....\n   |_ __array_schema.tdb   # array schema file\n   |_ ..."
  },
  {
    "objectID": "format_spec/array_schema.html#array-schema-file",
    "href": "format_spec/array_schema.html#array-schema-file",
    "title": "Array Schema",
    "section": "Array Schema File",
    "text": "Array Schema File\nThe array schema file consists of a single generic tile, with the following data:\n\n\n\nField\nType\nDescription\n\n\n\n\nArray version\nuint32_t\nFormat version number of the array schema\n\n\nAllows dups\nbool\nWhether or not the array allows duplicate cells\n\n\nArray type\nuint8_t\nDense or sparse\n\n\nTile order\nuint8_t\nRow or column major\n\n\nCell order\nuint8_t\nRow or column major\n\n\nCapacity\nuint64_t\nFor sparse fragments, the data tile capacity\n\n\nCoords filters\nFilter Pipeline\nThe filter pipeline used as default for coordinate tiles\n\n\nOffsets filters\nFilter Pipeline\nThe filter pipeline used for cell var-len offset tiles\n\n\nValidity filters\nFilter Pipeline\nThe filter pipeline used for cell validity tiles\n\n\nDomain\nDomain\nThe array domain\n\n\nNum attributes\nuint32_t\nNumber of attributes in the array\n\n\nAttribute 1\nAttribute\nFirst attribute\n\n\n…\n…\n…\n\n\nAttribute N\nAttribute\nNth attribute\n\n\nNum labels\nuint32_t\nNumber of dimension labels in the array\n\n\nLabel 1\nDimension Label\nFirst dimension label\n\n\n…\n…\n…\n\n\nLabel N\nDimension Label\nNth dimension label"
  },
  {
    "objectID": "format_spec/array_schema.html#domain",
    "href": "format_spec/array_schema.html#domain",
    "title": "Array Schema",
    "section": "Domain",
    "text": "Domain\nThe domain has internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum dimensions\nuint32_t\nDimensionality/rank of the domain\n\n\nDimension 1\nDimension\nFirst dimension\n\n\n…\n…\n…\n\n\nDimension N\nDimension\nNth dimension"
  },
  {
    "objectID": "format_spec/array_schema.html#dimension",
    "href": "format_spec/array_schema.html#dimension",
    "title": "Array Schema",
    "section": "Dimension",
    "text": "Dimension\nThe dimension has internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nDimension name length\nuint32_t\nNumber of characters in dimension name\n\n\nDimension name\nuint8_t[]\nDimension name character array\n\n\nDimension datatype\nuint8_t\nDatatype of the coordinate values\n\n\nCell val num\nuint32_t\nNumber of coordinate values per cell. For variable-length dimensions, this is std::numeric_limits&lt;uint32_t&gt;::max()\n\n\nFilters\nFilter Pipeline\nThe filter pipeline used on coordinate value tiles\n\n\nDomain size\nuint64_t[]\nThe domain size in bytes\n\n\nDomain\nuint8_t[]\nByte array of length equal to domain size above, storing the min, max values of the dimension.\n\n\nNull tile extent\nuint8_t\n1 if the dimension has a null tile extent, else 0.\n\n\nTile extent\nuint8_t[]\nByte array of length equal to the dimension datatype size, storing the space tile extent of this dimension."
  },
  {
    "objectID": "format_spec/array_schema.html#attribute",
    "href": "format_spec/array_schema.html#attribute",
    "title": "Array Schema",
    "section": "Attribute",
    "text": "Attribute\nThe attribute has internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nAttribute name length\nuint32_t\nNumber of characters in attribute name\n\n\nAttribute name\nuint8_t[]\nAttribute name character array\n\n\nAttribute datatype\nuint8_t\nDatatype of the attribute values\n\n\nCell val num\nuint32_t\nNumber of attribute values per cell. For variable-length attributes, this is std::numeric_limits&lt;uint32_t&gt;::max()\n\n\nFilters\nFilter Pipeline\nThe filter pipeline used on attribute value tiles\n\n\nFill value size\nuint64_t\nThe size in bytes of the fill value\n\n\nFill value\nuint8_t[]\nThe fill value\n\n\nNullable\nbool\nWhether or not the attribute can be null\n\n\nFill value validity\nuint8_t\nThe validity fill value\n\n\nOrder\nuint8_t\nOrder of the data stored in the attribute. This may be unordered, increasing or decreasing"
  },
  {
    "objectID": "format_spec/array_schema.html#dimension-label",
    "href": "format_spec/array_schema.html#dimension-label",
    "title": "Array Schema",
    "section": "Dimension Label",
    "text": "Dimension Label\nThe dimension label has internal format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nDimension index\nuint32_t\nIndex of the dimension the label is for\n\n\nLabel order\nuint8_t\nOrder of the label data\n\n\nDimension label name length\nuint64_t\nNumber of characters in the dimension label name\n\n\nDimension label name\nuint8_t []\nThe name of the dimension label\n\n\nRelative URI\nuint8_t\nIf the URI of the array the label data is stored in is relative to this array\n\n\nURI length\nuint64_t\nThe number of characters in the URI\n\n\nURI\nuint8_t []\nThe URI the label data is stored in\n\n\nLabel attribute name length\nuint32_t\nThe length of the attribute name of the label data\n\n\nLabel attribute name\nuint8_t []\nThe name of the attribute the label data is stored in\n\n\nLabel datatype\nuint8_t\nThe datatype of the label data\n\n\nLabel cell_val_num\nuint32_t\nThe number of values per cell of the label data. For variable-length labels, this is std::numeric_limits&lt;uint32_t&gt;::max()\n\n\nLabel domain size\nuint64_t\nThe size of the label domain\n\n\nLabel domain start size\nuint64_t\nThe size of the first value of the domain for variable-lenght datatypes. For fixed-lenght labels, this is 0\n\n\nLabel domain data\nuint8_t[]\nByte array of length equal to domain size above, storing the min, max values of the dimension\n\n\nIs external\nuint8_t\nIf the URI is not stored as part of this array"
  },
  {
    "objectID": "experimental/tiledb/common/dag/nodes/doc/api.html",
    "href": "experimental/tiledb/common/dag/nodes/doc/api.html",
    "title": "Task Graph Node API",
    "section": "",
    "text": "A task graph node is a computational unit that applies user-provided functions to data flowing through the graph. Data flows through the graph (and through the nodes) via task graph ports. A function node takes input data from its input port, applies a user-defined function and creates output data on its output port. A producer node only creates output; a consumer node only takes input. Output ports from one node connect to the input ports of another via an edge. A simple node takes one piece of data and produces on piece of data for each input. A simple MIMO node may take multiple inputs and produce multiple outputs; it creates its outputs using one piece of data from each input.\n\n\n\nsimple_node\n\n\n\n\n\nmimo_node\n\n\nAPI\n\n****Note:*** Based on feedback from Isaiah, the following updated API is proposed. The current API will be updated to reflect this new one over the course of the current sprint.*\n\n\n\n\n\ntemplate &lt;class Item&gt;\nclass ProducerNode;\n\nItem the type of data to be produced by the ProducerNode\n\n\n\n  template &lt;class Function&gt;\n  ProducerNode(Function&& f);\n\nFunction must meet the requirements of std::function&lt;Item())&gt; or of std::function&lt;Item(std::stop_source))&gt;.\nIf the latter constructor is used, a stop_source variable is passed to the enclosed function. If the enclosed function invokes stop_source::request_stop(), the ProducerNode will enter the stopping state, which will propagate throught its output port to the connected input port. The connected node will the enter its shutdown state, and so on, thus stopping computation throughout the entire task graph.\nClass Template Argument Deduction (CTAD) guides have been specified so that explicit template arguments are not required to construct objects with the above constructor. The CTAD guides work in the cases where Function is a regular function, a lambda, or a function object. The current CTAD guides do not work when Function is a bind object; a template argument is still required in the case of std::bind.\nNote that in the case of a lambda, the unary + operator is required to decay the lambda into a normal function.\n\n\n\n\n\n    // Create a `ProducerNode` with a function\n    size_t source_function(std::stop_source &);\n    auto a = ProducerNode {source_function};\n\n    // Nodes can also be created with function objects\n    class source_function_object {\n      size_t operator()(std::stop_source &);\n    }\n    source_function_object f;\n    auto b = ProducerNode {f};\n\n    // And with lambdas.  IMPORTANT: Note the unary `+`!\n    auto c = ProducerNode {+[]() { return 0UL; }};\n\n    // Bind requires specifying node type (so it seems)\n    auto g = std::bind([](double i) -&gt;size_t { return 0UL;}, 0.0);\n    auto d = ProducerNode&lt;size_t&gt; (bb);\n\n    // Emulating bind will work, not requiring template type\n    size_t bind_function(double) { return 0UL; }\n    auto e = ProducerNode(+[]() -&gt; size_t { return bind_function(0.0); });\n\nNote: Future enhancements to the API will include variants to support one-shot or multi-shot (for a specified number of runs) operation, with an implicit stop following execution. Using one-shot or multi-shot variants will obviate the need for the user to invoke the stop_source.\n\nIn the following, we omit all of the variations for creating different kinds of nodes. But it should be understood that all the constructor variants may be used for the node types below.\n\n\n\ntemplate &lt;class Item&gt;\nclass ConsumerNode;\n\nItem the type of data to be produced by the ConsumerNode\n\n\n\n  template &lt;class Function&gt;\n  ProducerNode(Function&& f);\n  - `Function` must meet the requirements of `std::function&lt;void(const Item&)&gt;`.\n\n\n\n\n    void sink_function(const size_t&);\n    Consumer Node b{sink_function};\n\n\n\ntemplate &lt;class InItem, class OutItem = InItem&gt;\nclass FunctionNode;\n\nInItem the type of data to be consumed on the input of the FunctionNode\nOutItem the type of data to be created on the output of the FunctionNode\n\n\n\n  template &lt;class Function&gt;\n  ProducerNode(Function&& f);\n\nFunction must meet the requirements of std::function&lt;OutItem(const InItem&)&gt;.\n\n\n\n\n    size_t function(const size_t&);\n    FunctionNode b{function};\n\n\n\n\ntemplate &lt;\n    template &lt;class&gt; class SinkMover, class... ItemsIn,\n    template &lt;class&gt; class SourceMover, class... ItemsOut&gt;\nclass MimoFunctionNode;\n\n\n  template &lt;class Function&gt;\n  explicit MimoFunctionNode(Function&& f);\n\nf must meet the requirements of std::function&lt;std::tuple&lt;ItemsOut...(std::tuple&lt;ItemsIn...&gt;)&gt;f\n\n\n\n\n\n\n\n\n\n\ntemplate &lt;template &lt;class&gt; class Mover, class Item&gt;\nclass Edge;\n\nMover the type of data item mover to be transferred by the Edge\nItem the type of data to be transferred by the Edge\nClass Template Argument Deduction rules have been specified so that template argument for Item is not required. The type of the Mover must still be explicitly specified.\n\nNote: When connecting an output port to an input port, the Item types of the input port, output port, and edge must be the same.\n\n\n\nProducerNode a{source_function{}};\nConsumerNode b{sink_function{}};\n\nEdge&lt;AsyncMover3&gt; e{a, b};\n\ndouble other_source_function() {\n  return 0.0;\n}\n\nProducerNode c {other_source_function{}};\nConsumerNode d{sink_function{}};\n\nEdge&lt;AsyncMover3&gt; f{c, d};  // This will fail to compile because Item types of Producer and Consumer don't match.\n\nIf we decide to use the same type of Edge throughout a given graph, we can make a type alias such as\n\ntemplate &lt;class Item&gt;\nusing GraphEdge = Edge&lt;AsyncMover3, Item&gt;;\nin which case we could simply invoke\nProducerNode a{source_function{}};\nConsumerNode b{sink_function{}};\n\nGraphEdge e{a, b};\n\n(Note that AsyncMover3 is itself a type alias, using some specific types given to the more general ItemMover class. This will likely be further generalized to include a Scheduler. However, since a given graph will use a specified scheduler, we can likely hide this behind type aliases and specific types for certain template parameters. In general, users will not need to see the Scheduler type in the API.\n\n\n\n\n\n\n\n\nclass Scheduler;\n\nBase class for schedulers.\n\n\n\nScheduler(size_t concurrency_level = std::thread::hardware_concurrency();\n\n\n\ntemplate &lt;class... Tasks&gt;\nScheduler::submit(Tasks&&... tasks);\n\nSubmits tasks to scheduler\nNote: Task execution is lazy. Tasks do not begin execution until a wait function is called.\n\n\n\n\ntemplate &lt;class... Tasks&gt;\nScheduler::sync_wait_all();\n\nSynchronously wait for tasks to complete.\nNote: Task execution is lazy. Tasks previously submitted to submit will be launched on calling sync_wait_all()\n\n\n\n\nSee also full example below;\n// Define nodes\nProducerNode a;\nConsumerNode b;\n\n// Connect nodes\nEdge {a, b};\n\nBountifulScheduler sched;\n\n// Submit jobs\nsched.submit(a, b);\n\n// Wait on their completion\nsched.sync_wait_all();\n\n\n\nA node is repeatedly run until it is stopped. It is expected that a simple node will produce one data item each time it is run. Suppose we wish to have a ProducerNode that generates a finite sequence of numbers. Consider the following.\nsize_t source_function_1(std::stop_source& stop_source) {\n  for (size_t i = 0; i &lt; 42; ++i) {\n    return i;\n  }\n  stop_source.request_stop();\n  return 0UL;\n}\nsource_function_1 src_1;\nauto a = ProducerNode { src_1; );\nThis node will not produce the desired result. Each time it is called, it will return 10. Moreover, it will never invoke request_stop().\nA correctly behaving function would be something like\nsize_t source_function_2(std::stop_source& stop_source) {\n  static i = 0;\n  if (i &lt; 42) {\n    return i++;\n  }\n  stop_source.request_stop();\n  return 0UL;\n}\nsource_function_2 src_2;\nauto a = ProducerNode { src_2; );\nHere. we make i static so that its value is saved across invocations of the node. It will generate numbers from 0 to 41 and then stop. (Again note that the value returned after request_stop() is ignored.\n\n\n\n\n\n\n\n\n// This function object generates a sequence of numbers up to a specified limit\n// (non-inclusive), aftter which it invokes `std::stop_source::request_stop()`.\n// The stop will propagate and shut down the rest of the nodes. (The stop will\n// essentially propagate behind the data items in flight.  All of the\n// data items will complete transmission through the graph and all nodes\n// will continue to run until the data items are exhausted.\nclass source_function {\n public:\n  size_t operator()(std::stop_source& stopper) {\n    static size_t i = 0;\n    if (i &lt; 42) {\n      return i++;\n    stopper.request_stop();\n    return 0UL;\n  }\n};\n\n// Function object to transform data items\n// This trivial function simply propagates its input to its output\ntemplate &lt;class InItem = size_t, class OutItem = InItem&gt;\nclass function {\n public:\n  OutItem operator()(const InItem& in) {\n    return in;\n  }\n};\n\n// Sink function object to absorb data items\ntemplate &lt;class Item = size_t&gt;\nclass sink_function {\n public:\n  void operator()(Item) {\n  }\n};\n\n// Function to generate items.  Generates a sequence of numbers until a \n// limit is reached, at which point a stop is requested.  See above\n// for description of stopping.\nsize_t actual_source_function(std::stop_source stop_source) {\n  static size_t i = 0;\n  if (i &lt; 42) {}\n    return i++;\n  }\n\n  // Initiate stopping execution of the graph\n  stop_source.request_stop();\n\n  // Return to make compiler happy. The return value is ignored and\n  // not sent into the graph\n  return 0;\n}\n\n\n\n// Create nodes\nProducerNode a{actual_source_function};\nFunctionNode b{function{}};\nFunctionNode c{[](size_t i) ( return i; };\nConsumerNode d{sink_function{}};\n\n\n\n// Connect a to b\nEdge f{a, b};\n\n// Connect b to c\nEdge g{b, c};\n\n// Connect c to d\nEdge h{c, d};\n\n\n\n// Run the primitive graph\nBountifulScheduler sched;  // Could also use throw_catchSchedular\n\n// Until we have an actual `graph` class, we can submit a primitive\n// graph for execution by submitting its nodes\nsched.submit(a, b, c, d);\n\n// Wait on completion of the graph's execution\nsched.sync_wait_all();\n\n\n\nWe can also emulate a bountiful scheduler, where we run each node inside of an asynchronous task. The run member function of a node invokes the node continually until stop is invoked.\n\n// Task to invoke `ProducerNode a`.  Invoke stop after `rounds` iterations.\nauto fun_a = [&]() {\n  a.run();\n};\n\n// Task to invoke `FunctionNode b`.  Run until stopped.\nauto fun_b = [&]() {\n  b.run();\n};\n\n// Task to invoke `ConsumerNode c`.  Run until stopped.\nauto fun_c = [&]() {\n  c.run();\n};\n\n// Task to invoke `ConsumerNode c`.  Run until stopped.\nauto fun_d = [&]() {\n  d.run();\n};\n\nauto fut_a = std::async(std::launch::async, fun_a);\nauto fut_b = std::async(std::launch::async, fun_b);\nauto fut_c = std::async(std::launch::async, fun_c);\nauto fut_d = std::async(std::launch::async, fun_d);\n\n// Wait for completion\nfut_a.get();\nfut_b.get();\nfut_c.get();\nfut_d.get();\n\n\n\n\nThe basic idea of the updated API is to allow task graph nodes to be created with following simplified syntax:\n  template &lt;class Item&gt;\n  class ProducerNode;\n  auto node = ProducerNode{function};\nwhere function is an appropriately defined function for execution in the task graphs. Note that the template parameter for ProducerNode does not have to be explicitly specified. Rather, it is deduced (via a library-defined deduction rule) from the type of function.\nThe previous API for task graph nodes was\n  template &lt;template &lt;class&gt; class Mover, class Item&gt;\n  class ProducerNode;\n  auto node = ProducerNode{function};\nwhich would require the Mover type to be specified (Item could still be deduced). There are two ways to achieve the simplified API. One, as above, is to define task graph nodes as taking only one interface, and allowing the Mover to be an abstract base class within the node class, which would be set when the Edge connects two nodes. This will add a small bit of overhead to each call to the Mover, which may or may not be significant. There is also a slight entanglement of all of the node classes with the Mover class (a slight lessening of the separation of concerns). In the templated case, the node classes do not need to “know” about the Mover class until instantiation time.\nWith the templated API, the Mover does not need to be an abstract base class. To eliminate having to specify the Mover at each node instantiation, we could do something like the following: The previous API for task graph nodes was\n  template &lt;template &lt;class&gt; class Mover, class Item&gt;\n  class GraphProducerNode;\n\n  template &lt;class Item&gt;\n  using ProducerNode = GraphProducerNode&lt;DefaultMover, Item&gt;;\n\n  auto node = ProducerNode{function};\nHere, DefaultMover is a concrete class. With this approach, if one wanted to used different types of concrete Mover classes, one would need define type aliases for different instantiations with the GraphProducerNode.\nAll in all, the first approach (using Mover as an abstract base class) seems to be the cleanest and will be the approach taken. The task graph API will be revised accordingly."
  },
  {
    "objectID": "experimental/tiledb/common/dag/nodes/doc/api.html#terminology",
    "href": "experimental/tiledb/common/dag/nodes/doc/api.html#terminology",
    "title": "Task Graph Node API",
    "section": "",
    "text": "A task graph node is a computational unit that applies user-provided functions to data flowing through the graph. Data flows through the graph (and through the nodes) via task graph ports. A function node takes input data from its input port, applies a user-defined function and creates output data on its output port. A producer node only creates output; a consumer node only takes input. Output ports from one node connect to the input ports of another via an edge. A simple node takes one piece of data and produces on piece of data for each input. A simple MIMO node may take multiple inputs and produce multiple outputs; it creates its outputs using one piece of data from each input.\n\n\n\nsimple_node\n\n\n\n\n\nmimo_node\n\n\nAPI\n\n****Note:*** Based on feedback from Isaiah, the following updated API is proposed. The current API will be updated to reflect this new one over the course of the current sprint.*"
  },
  {
    "objectID": "experimental/tiledb/common/dag/nodes/doc/api.html#nodes",
    "href": "experimental/tiledb/common/dag/nodes/doc/api.html#nodes",
    "title": "Task Graph Node API",
    "section": "",
    "text": "template &lt;class Item&gt;\nclass ProducerNode;\n\nItem the type of data to be produced by the ProducerNode\n\n\n\n  template &lt;class Function&gt;\n  ProducerNode(Function&& f);\n\nFunction must meet the requirements of std::function&lt;Item())&gt; or of std::function&lt;Item(std::stop_source))&gt;.\nIf the latter constructor is used, a stop_source variable is passed to the enclosed function. If the enclosed function invokes stop_source::request_stop(), the ProducerNode will enter the stopping state, which will propagate throught its output port to the connected input port. The connected node will the enter its shutdown state, and so on, thus stopping computation throughout the entire task graph.\nClass Template Argument Deduction (CTAD) guides have been specified so that explicit template arguments are not required to construct objects with the above constructor. The CTAD guides work in the cases where Function is a regular function, a lambda, or a function object. The current CTAD guides do not work when Function is a bind object; a template argument is still required in the case of std::bind.\nNote that in the case of a lambda, the unary + operator is required to decay the lambda into a normal function.\n\n\n\n\n\n    // Create a `ProducerNode` with a function\n    size_t source_function(std::stop_source &);\n    auto a = ProducerNode {source_function};\n\n    // Nodes can also be created with function objects\n    class source_function_object {\n      size_t operator()(std::stop_source &);\n    }\n    source_function_object f;\n    auto b = ProducerNode {f};\n\n    // And with lambdas.  IMPORTANT: Note the unary `+`!\n    auto c = ProducerNode {+[]() { return 0UL; }};\n\n    // Bind requires specifying node type (so it seems)\n    auto g = std::bind([](double i) -&gt;size_t { return 0UL;}, 0.0);\n    auto d = ProducerNode&lt;size_t&gt; (bb);\n\n    // Emulating bind will work, not requiring template type\n    size_t bind_function(double) { return 0UL; }\n    auto e = ProducerNode(+[]() -&gt; size_t { return bind_function(0.0); });\n\nNote: Future enhancements to the API will include variants to support one-shot or multi-shot (for a specified number of runs) operation, with an implicit stop following execution. Using one-shot or multi-shot variants will obviate the need for the user to invoke the stop_source.\n\nIn the following, we omit all of the variations for creating different kinds of nodes. But it should be understood that all the constructor variants may be used for the node types below.\n\n\n\ntemplate &lt;class Item&gt;\nclass ConsumerNode;\n\nItem the type of data to be produced by the ConsumerNode\n\n\n\n  template &lt;class Function&gt;\n  ProducerNode(Function&& f);\n  - `Function` must meet the requirements of `std::function&lt;void(const Item&)&gt;`.\n\n\n\n\n    void sink_function(const size_t&);\n    Consumer Node b{sink_function};\n\n\n\ntemplate &lt;class InItem, class OutItem = InItem&gt;\nclass FunctionNode;\n\nInItem the type of data to be consumed on the input of the FunctionNode\nOutItem the type of data to be created on the output of the FunctionNode\n\n\n\n  template &lt;class Function&gt;\n  ProducerNode(Function&& f);\n\nFunction must meet the requirements of std::function&lt;OutItem(const InItem&)&gt;.\n\n\n\n\n    size_t function(const size_t&);\n    FunctionNode b{function};\n\n\n\n\ntemplate &lt;\n    template &lt;class&gt; class SinkMover, class... ItemsIn,\n    template &lt;class&gt; class SourceMover, class... ItemsOut&gt;\nclass MimoFunctionNode;\n\n\n  template &lt;class Function&gt;\n  explicit MimoFunctionNode(Function&& f);\n\nf must meet the requirements of std::function&lt;std::tuple&lt;ItemsOut...(std::tuple&lt;ItemsIn...&gt;)&gt;f"
  },
  {
    "objectID": "experimental/tiledb/common/dag/nodes/doc/api.html#edges",
    "href": "experimental/tiledb/common/dag/nodes/doc/api.html#edges",
    "title": "Task Graph Node API",
    "section": "",
    "text": "template &lt;template &lt;class&gt; class Mover, class Item&gt;\nclass Edge;\n\nMover the type of data item mover to be transferred by the Edge\nItem the type of data to be transferred by the Edge\nClass Template Argument Deduction rules have been specified so that template argument for Item is not required. The type of the Mover must still be explicitly specified.\n\nNote: When connecting an output port to an input port, the Item types of the input port, output port, and edge must be the same.\n\n\n\nProducerNode a{source_function{}};\nConsumerNode b{sink_function{}};\n\nEdge&lt;AsyncMover3&gt; e{a, b};\n\ndouble other_source_function() {\n  return 0.0;\n}\n\nProducerNode c {other_source_function{}};\nConsumerNode d{sink_function{}};\n\nEdge&lt;AsyncMover3&gt; f{c, d};  // This will fail to compile because Item types of Producer and Consumer don't match.\n\nIf we decide to use the same type of Edge throughout a given graph, we can make a type alias such as\n\ntemplate &lt;class Item&gt;\nusing GraphEdge = Edge&lt;AsyncMover3, Item&gt;;\nin which case we could simply invoke\nProducerNode a{source_function{}};\nConsumerNode b{sink_function{}};\n\nGraphEdge e{a, b};\n\n(Note that AsyncMover3 is itself a type alias, using some specific types given to the more general ItemMover class. This will likely be further generalized to include a Scheduler. However, since a given graph will use a specified scheduler, we can likely hide this behind type aliases and specific types for certain template parameters. In general, users will not need to see the Scheduler type in the API."
  },
  {
    "objectID": "experimental/tiledb/common/dag/nodes/doc/api.html#schedulers",
    "href": "experimental/tiledb/common/dag/nodes/doc/api.html#schedulers",
    "title": "Task Graph Node API",
    "section": "",
    "text": "class Scheduler;\n\nBase class for schedulers.\n\n\n\nScheduler(size_t concurrency_level = std::thread::hardware_concurrency();\n\n\n\ntemplate &lt;class... Tasks&gt;\nScheduler::submit(Tasks&&... tasks);\n\nSubmits tasks to scheduler\nNote: Task execution is lazy. Tasks do not begin execution until a wait function is called.\n\n\n\n\ntemplate &lt;class... Tasks&gt;\nScheduler::sync_wait_all();\n\nSynchronously wait for tasks to complete.\nNote: Task execution is lazy. Tasks previously submitted to submit will be launched on calling sync_wait_all()\n\n\n\n\nSee also full example below;\n// Define nodes\nProducerNode a;\nConsumerNode b;\n\n// Connect nodes\nEdge {a, b};\n\nBountifulScheduler sched;\n\n// Submit jobs\nsched.submit(a, b);\n\n// Wait on their completion\nsched.sync_wait_all();\n\n\n\nA node is repeatedly run until it is stopped. It is expected that a simple node will produce one data item each time it is run. Suppose we wish to have a ProducerNode that generates a finite sequence of numbers. Consider the following.\nsize_t source_function_1(std::stop_source& stop_source) {\n  for (size_t i = 0; i &lt; 42; ++i) {\n    return i;\n  }\n  stop_source.request_stop();\n  return 0UL;\n}\nsource_function_1 src_1;\nauto a = ProducerNode { src_1; );\nThis node will not produce the desired result. Each time it is called, it will return 10. Moreover, it will never invoke request_stop().\nA correctly behaving function would be something like\nsize_t source_function_2(std::stop_source& stop_source) {\n  static i = 0;\n  if (i &lt; 42) {\n    return i++;\n  }\n  stop_source.request_stop();\n  return 0UL;\n}\nsource_function_2 src_2;\nauto a = ProducerNode { src_2; );\nHere. we make i static so that its value is saved across invocations of the node. It will generate numbers from 0 to 41 and then stop. (Again note that the value returned after request_stop() is ignored."
  },
  {
    "objectID": "experimental/tiledb/common/dag/nodes/doc/api.html#extended-example-constructing-and-executing-a-primitive-graph",
    "href": "experimental/tiledb/common/dag/nodes/doc/api.html#extended-example-constructing-and-executing-a-primitive-graph",
    "title": "Task Graph Node API",
    "section": "",
    "text": "// This function object generates a sequence of numbers up to a specified limit\n// (non-inclusive), aftter which it invokes `std::stop_source::request_stop()`.\n// The stop will propagate and shut down the rest of the nodes. (The stop will\n// essentially propagate behind the data items in flight.  All of the\n// data items will complete transmission through the graph and all nodes\n// will continue to run until the data items are exhausted.\nclass source_function {\n public:\n  size_t operator()(std::stop_source& stopper) {\n    static size_t i = 0;\n    if (i &lt; 42) {\n      return i++;\n    stopper.request_stop();\n    return 0UL;\n  }\n};\n\n// Function object to transform data items\n// This trivial function simply propagates its input to its output\ntemplate &lt;class InItem = size_t, class OutItem = InItem&gt;\nclass function {\n public:\n  OutItem operator()(const InItem& in) {\n    return in;\n  }\n};\n\n// Sink function object to absorb data items\ntemplate &lt;class Item = size_t&gt;\nclass sink_function {\n public:\n  void operator()(Item) {\n  }\n};\n\n// Function to generate items.  Generates a sequence of numbers until a \n// limit is reached, at which point a stop is requested.  See above\n// for description of stopping.\nsize_t actual_source_function(std::stop_source stop_source) {\n  static size_t i = 0;\n  if (i &lt; 42) {}\n    return i++;\n  }\n\n  // Initiate stopping execution of the graph\n  stop_source.request_stop();\n\n  // Return to make compiler happy. The return value is ignored and\n  // not sent into the graph\n  return 0;\n}\n\n\n\n// Create nodes\nProducerNode a{actual_source_function};\nFunctionNode b{function{}};\nFunctionNode c{[](size_t i) ( return i; };\nConsumerNode d{sink_function{}};\n\n\n\n// Connect a to b\nEdge f{a, b};\n\n// Connect b to c\nEdge g{b, c};\n\n// Connect c to d\nEdge h{c, d};\n\n\n\n// Run the primitive graph\nBountifulScheduler sched;  // Could also use throw_catchSchedular\n\n// Until we have an actual `graph` class, we can submit a primitive\n// graph for execution by submitting its nodes\nsched.submit(a, b, c, d);\n\n// Wait on completion of the graph's execution\nsched.sync_wait_all();\n\n\n\nWe can also emulate a bountiful scheduler, where we run each node inside of an asynchronous task. The run member function of a node invokes the node continually until stop is invoked.\n\n// Task to invoke `ProducerNode a`.  Invoke stop after `rounds` iterations.\nauto fun_a = [&]() {\n  a.run();\n};\n\n// Task to invoke `FunctionNode b`.  Run until stopped.\nauto fun_b = [&]() {\n  b.run();\n};\n\n// Task to invoke `ConsumerNode c`.  Run until stopped.\nauto fun_c = [&]() {\n  c.run();\n};\n\n// Task to invoke `ConsumerNode c`.  Run until stopped.\nauto fun_d = [&]() {\n  d.run();\n};\n\nauto fut_a = std::async(std::launch::async, fun_a);\nauto fut_b = std::async(std::launch::async, fun_b);\nauto fut_c = std::async(std::launch::async, fun_c);\nauto fut_d = std::async(std::launch::async, fun_d);\n\n// Wait for completion\nfut_a.get();\nfut_b.get();\nfut_c.get();\nfut_d.get();"
  },
  {
    "objectID": "experimental/tiledb/common/dag/nodes/doc/api.html#appendix",
    "href": "experimental/tiledb/common/dag/nodes/doc/api.html#appendix",
    "title": "Task Graph Node API",
    "section": "",
    "text": "The basic idea of the updated API is to allow task graph nodes to be created with following simplified syntax:\n  template &lt;class Item&gt;\n  class ProducerNode;\n  auto node = ProducerNode{function};\nwhere function is an appropriately defined function for execution in the task graphs. Note that the template parameter for ProducerNode does not have to be explicitly specified. Rather, it is deduced (via a library-defined deduction rule) from the type of function.\nThe previous API for task graph nodes was\n  template &lt;template &lt;class&gt; class Mover, class Item&gt;\n  class ProducerNode;\n  auto node = ProducerNode{function};\nwhich would require the Mover type to be specified (Item could still be deduced). There are two ways to achieve the simplified API. One, as above, is to define task graph nodes as taking only one interface, and allowing the Mover to be an abstract base class within the node class, which would be set when the Edge connects two nodes. This will add a small bit of overhead to each call to the Mover, which may or may not be significant. There is also a slight entanglement of all of the node classes with the Mover class (a slight lessening of the separation of concerns). In the templated case, the node classes do not need to “know” about the Mover class until instantiation time.\nWith the templated API, the Mover does not need to be an abstract base class. To eliminate having to specify the Mover at each node instantiation, we could do something like the following: The previous API for task graph nodes was\n  template &lt;template &lt;class&gt; class Mover, class Item&gt;\n  class GraphProducerNode;\n\n  template &lt;class Item&gt;\n  using ProducerNode = GraphProducerNode&lt;DefaultMover, Item&gt;;\n\n  auto node = ProducerNode{function};\nHere, DefaultMover is a concrete class. With this approach, if one wanted to used different types of concrete Mover classes, one would need define type aliases for different instantiations with the GraphProducerNode.\nAll in all, the first approach (using Mover as an abstract base class) seems to be the cleanest and will be the approach taken. The task graph API will be revised accordingly."
  },
  {
    "objectID": "experimental/tiledb/common/dag/state_machine/doc/fsm2.html",
    "href": "experimental/tiledb/common/dag/state_machine/doc/fsm2.html",
    "title": "Analysis of Port State Machine",
    "section": "",
    "text": "The file fsm.h implements a state machine for two communicating ports, Source and Sink. Each port has two states, empty or full. There are two transition events associated with the source: fill, which transitions from empty to full and tells the state machine there is an item in the Source, and and push, which initiates transfer to the Sink and transitions from full to empty.\nSimilarly, there are two events associated with the sink: drain, which transitions from full to empty and tells the Sink node that its item has been removed, and and pull, which attempts to transfer an item from the Source and transitions from empty to full. For simplicity, though we may need them in the future, we not consider events for startup, stop, forced shutdown, or abort.\nThe following diagram shows the state transitions for the port state machine. The state is represented with two bits, one for the source and one for the sink, e.g., “00” meaning that the source state is 0 and the sink state is 1.\n\n\nTo provide the functionality of the state machine for the purposes of safely transferring data from a Source to a Sink, there are exit and entry actions associated with selected states and events.\nOne particular aspect of this is the pull event from state 00 and the push event from state 11. In these cases, the exit action is “wait”, since the only valid state to complete a push or a pull is from state 01. Accordingly, we perform notifications as entry actions to state 10.\nWhen a source or sink thread is released from its wait, it is still in the push or pull event. To enable it to complete that desired operation, we restart the event processing for the push or pull event in the current state.\n(As a slight optimization, we also perform notifications on entry to states 00 and 11. The waiter doesn’t get to move in that case, but does get to leave the wait and produce or consume an item and then try to push or pull again.)\nOur basic goal for the Source and Sink ports is to transfer a data item from a Source to a connected (bound) Sink. At a high level, the way a client would use the Source to do this is the following: - create a data item - insert the data item into a Source port - invoke the fill event - invoke the push event\nSimilarly, the desired usage of a Sink port is also to transfer a data item from a Source to a bound Sink. At a high level, the way a client would use the Source is the following - invoke the pull event - extract the data item from the Sink port - consume the item - invoke the drain event\nBased on these product states and the four above events, the state transition table for the product state machine (which we will just refer to as the “state machine” below is the following:\n\n\n\n\n\n\n\n\n\n\n\nState\n\n\nEvent\n\n\n\n\n\n\n\nfill\npush\ndrain\npull\nstop\n\n\n00\n10\n00\n\n01\n\n\n\n01\n11\n01\n00\n01\n\n\n\n10\n\n01\n\n01\n\n\n\n11\n\n01\n10\n11\n\n\n\n\nUsing this table, we can include the states as predicates to create an initial “proof outline” statements for the Source operation:\n   while (not done) {\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n     do produce and insert item\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = full } */\n     do fill\n     /* { state = 10 ∨ state = 11 } ∧ { source_item = full } */\n     do push\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n   }\nSimilarly for Sink:\n   while (not done) {\n     /* { state = 00 ∨ state = 10 } ∧ { sink_item = empty } */\n     do pull\n     /* { state = 01 ∨ state = 11 } ∧ { sink_item = full } */\n     do extract and consume item  \n     /* { state = 01 ∨ state = 11 } ∧ { sink_item = full } */\n     do drain\n     /* { state = 00 ∨ state = 10 } ∧ { sink_item = empty } */\n   }\nNow, the Source and Sink need to coordinate push and pull so there is not a race condition (nor a deadlock) when making transitions in the state machine. Moreover, we have to make sensible transitions. That is, we only be able to succesfully do a push when the Sink state is empty (and the Sink item itself is empty). This is why we insert a new item and then invoke fill. Until the state has transitioned to indicate the state of the Source is full, the Sink will not attempt to transfer the item. Similarly, we empty the sink_item and then signal that the Sink is in the empty state.\nTo do this, we associate exit and entry actions with each state transition, some of which will synchronize between Source and Sink. These actions are used with the state transition thusly:\n\nbegin_transition: given old_state and event\nexecute exit(old_state, event)\nnew_state = transition(old_state, event)\nexecute entry(new_state, event)\n\nNote that the exit action is called before the state transition. Note also that the entry action is called with the new state (the post transition state).\nThe tables for exit actions to be perfomed on state transitions is:\n\n\n\n\n\n\n\n\n\n\n\nState\n\n\nEvents\n\n\n\n\n\n\n\nfill\npush\ndrain\npull\nstop\n\n\n00\n\nreturn\n\nsink_wait\n\n\n\n01\n\nreturn\n\nreturn\n\n\n\n10\n\nsource_swap\n\nsink_swap\n\n\n\n11\n\nsource_wait\n\nreturn\n\n\n\n\nThe table for entry actions to be performend on state transitions is:\n\n\n\n\n\n\n\n\n\n\n\nState\n\n\nEvents\n\n\n\n\n\n\n\nfill\npush\ndrain\npull\nshutdown\n\n\n00\n\n\nnotify_source\n\n\n\n\n01\n\n\n\n\n\n\n\n10\nnotify_sink\n\nnotify_source\n\n\n\n\n11\nnotify_sink\n\n\n\n\n\n\n\nThe source_swap function is used to potentially transfer the data items associated with Source and Sink from the Source to the Sink (as well as changing the state if data transfer is carried out). The source_swap function is invoked whenever the state is 10 (which is when there is an item in Source and space available to transfer to the Sink. The data transfer is carried out by swapping the Source and Sink items and changing the state of 10 to 01.\nWhen the state is 00, the Sink will wait for the Source to become full. The Source will notify the Sink when it becomes full. Similarly, if the state is 11, the Source will wait until it is signalled by the Sink that the Sink is empty.\n\n\nTo prove correctness of the port state machine, we use proof outline techniques from e.g., Concurrent Programming by Greg Andrews. For this proof outline, we represent the overall state of a connected Source and Sink with two boolean arrays, each with two elements, shared by the Source and Sink: state and items.\n\n\nFor the purposes of a proof outline, the Source has three operations:\n1. inject: items[0] ← 1\n2. fill: state[0] ← 1\n3. push: 〈 await ¬{ state = 11 } :\n            if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\nSimilarly, for the purposes of a proof outline, the Sink has three operations:\n1. extract: item[1] ← 0\n2. drain: state[1] ← 0\n3. pull: 〈 await ¬{ state = 00 } :\n            if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n\n\n\nThe critical operation of the Source is the push, which blocks (awaits) until ¬{ state = 11 }, that is, until the state is not equal to 11. In that case, states 00, 01, and 10 could pass through. However, in addition to the await, push includes a data transfer step (a “move”) that will perform the following predicate transformation for { state = 10 }:\n/* { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } */\nThis represents moving a data item from the Source to the Sink. Note that both the state machine state and the actual data items being held are (atomically) changed in the move.\nBased on these properties of the push, we have the following valide states that can occur directly following a push:\n     /* { state = 00 ∧ items = 00 } ∨ */\n     /* { state = 01 ∧ items = 01 }   */\nThe push itself is atomic, but as soon as it has completed, the state of the port can be changed by the Sink operating asynchronously to the Source. Given the three operations that the Sink can perform, the above states can be subject to pull, extract, or extract+drain. The pull operation will not change the state. The extract operation can change { items = 01 } to { items = 00 }. Similarly, drain can change { state = 01 } to { state = 00 }. Note that extract always precedes drain – the Sink cannot change { state = 01 ∧ items = 01 } into { state = 00 ∧ items = 01 }.\nThus, the entire set of possible configurations following push are\n     /* { state = 00 ∧ items = 00 }                  ∨ */\n     /* { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */\nFollowing the push, the Source may inject, which will change the above states to:\n     /* { state = 00 ∧ items = 10 }                  ∨ */\n     /* { state = 01 ∧ ( items = 11 ∨ items = 10 ) }   */\nAgain, the Sink may extract, drain, or pull. However, the only change that can occur is that extract may change { items = 11 } to { items = 10 }, which is already part of the above configuration. A subsequent drain may then change\n{ state = 01 ∧ items = 10 } to { state = 00 ∧ items = 10 }, which again, is already part of the above configuration. Thus, the Sink operating asynchronously will not change the system predicate between inject and fill. In other words, the sequence inject then fill can be considered to be atomic.\nFor the fill operation, the above predicate will be changed to\n     /* { state = 10 ∧ items = 10 }                  ∨ */\n     /* { state = 11 ∧ ( items = 11 ∨ items = 10 ) }   */\nAsynchronous operation of the Sink can change this predicate to\n     /* { state = 00 ∧ items = 00 }                  ∨ */\n     /* { state = 01 ∧ ( items = 00 ∨ items = 01 ) } ∨ */\n     /* { state = 10 ∧ items = 10 }                  ∨ */\n     /* { state = 11 ∧ ( items = 10 ∨ items = 11 ) }   */\n\n\nThus, the proof outline for the Source is\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ ( items = 10 ∨ items = 11 ) }   */\n     fill: state[0] ← 1\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } ∨ */\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ ( items = 10 ∨ items = 11 ) }   */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */     \n   }\n\n\n\n\nWe can follow a similar process to derive the proof outline for the Sink.\nThe critical operation of the Sink is the pull, which blocks (awaits) until ¬{ state = 00 }, that is, until the state is not equal to 00. In that case, states 01, 10, and 11 could pass through. However, in addition to the await, pull includes a data transfer step (a “move”) that will perform the following predicate transformation for { state = 10 }:\n/* { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } */\nThis represents moving a data item from the Source to the Sink. Note that both the state machine state and the actual data items being held are (atomically) changed in the move.\nBased on these properties of the push, we have the following valide states that can occur directly following a push:\n     /* { state = 01 ∧ items = 01 } ∨ */\n     /* { state = 11 ∧ items = 11 }   */\nThe pull itself is atomic, but as soon as it has completed, the state of the port can be changed by the Source operating asynchronously to the Sink. Given the three operations that the Source can perform, the above states can be subject to push, inject, or inject+fill. The push operation will not change the predicate. The inject operation can change { items = 01 } to { items = 11 }. Similarly, fill can change { state = 01 } to { state = 11 }. Note that inject always precedes fill – the Source cannot change { state = 01 ∧ items = 01 } into { state = 11 ∧ items = 01 }.\nThus, the entire set of possible configurations following push are\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ */\n     /* { state = 11 ∧ items = 11 }                    */\nFollowing the pull, the Sink may extract, which will change the above predicate to:\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ */\n     /* { state = 11 ∧ items = 10 }                    */\nAgain, the Source may inject, fill, or push. However, the only change that can occur is that inject may change { items = 00 } to { items = 10 }, which is already part of the above configuration. A subsequent fill may then change\n{ state = 01 ∧ items = 10 } to { state = 11 ∧ items = 10 }, which again, is already part of the above configuration. Thus, the Source operating asynchronously will not change the system predicate between extract and drain. In other words, the sequence extract then drain can be considered to be atomic.\nFor the drain operation, the above predicate will be changed to\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ */\n     /* { state = 10 ∧ items = 10 }                    */\nAsynchronous operation of the Source can change this predicate to\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ */\n     /* { state = 10 ∧ items = 10 }                  ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ */\n     /* { state = 11 ∧ items = 11 }                    */\n\n\nThus, the complete Sink proof outline iw\n   while (not done) {\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 }   ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 }     */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) }   */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 11 ∧ items = 01 ) }   */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 }   ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) }   */\n   }\n\n\n\n\nThe proof outlines shows some important characteristics of the port state machine.\n\nIf we begin with the valid state { state = 00 ∧ items = 00 } (which is the only sensible state with which to begin), the state machine will never enter any BAD state. We define a BAD state to be any of the following:\n\n\npredicate with { items = 10 ∨ items = 11 } prior to inject\npredicate with { state = 10 ∨ state = 11 } prior to fill\npredicate with { items = 00 ∨ items = 10 } prior to extract\npredicate with { state = 00 ∨ state = 10 } prior to drain\n\n\nIf we begin with the valid state { state = 00 ∧ items = 00 }, and use the following steps for the Source\n  inject\n  fill\n  push\nThe Source will always be ready to accept an item for injection. Hence we do not have to check whether the Source is ready prior to invoking inject. No concurrent Sink action will change the state into one that is BAD prior to inject.\nSimilarly, if we use the following steps for the Sink\n  pull\n  extract\n  drain\nThere will always be an item ready to extract after pull completes.\nThere are no race conditions between inject and fill, nor between extract and drain. Concurrent actions from the Sink cannot cause a BAD state between inject and fill, nor can concurrent actions from the Source introduce a BAD state between extract and drain.\nIn fact, there are no race conditions between any of the steps in the Source or the Sink. As a result, we do not need to introduce any locking mechanism to make any pairs of actions atomic. (As a reminder, all actions executed by the state machine are atomic.)\nOperations carried out directly by the state machine are protected by a lock. When the Source or Sink wait, they do so on a condition variable using that same lock. Note that when the Source and Sink both exit their loops that the Source will have { state = 00 ∨ state = 01 } while the Sink will have { state = 00 ∨ state = 10 }. The final state of the state machine is therefore { state = 00 ∨ state = 01 } ∧ { state = 00 ∨ state = 10 } ∧ { item = empty }, i.e., { state = 00 } ∧ { item = empty }. (This assumes that both Source and Sink perform the same number of operations, otherwise one of them will be left in a wait.)\nNB: The “move” portion of push and the “move” portion of pull (source_swap and sink_swap, respectively). Each checks to see if the state is equal to 10, if so, they swap the state to 01 (and perform a swap of the items associated with the source and sink), and notifies the other. If the state is not equal to 10, the swap function notifies the other and goes into a wait. Thus, we may not need separate swaps for Source and Sink, nor separate condition variables, nor separate notification functions. I have verified that this works experimentally, but I am leaving things separate for now.\n\n\n\n\n\nFor much of what TileDB will be doing with our task graph library, we will be using multi-stage edges between nodes. Edges, as they are currently implemented, still provide a control connection from the Sink to the Source connected by the Edge. In developing a proof outline for Source and Sink connected by an Edge with one (or more) buffered stages, we must account for the buffered data in both the state and the items.\nWe can follow a similar development for thi proof outline as we did for the unbuffered case.\nRevisiting the Source and Sink operations that we previously presented, we have the following actions for a three-stage port state machine:\n\n\n1. inject: items[0] ← 1\n2. fill: state[0] ← 1\n3. push: 〈 await ¬{ state = 111 } :\n            if { state = 010 ∧ items = 010 } → { state = 001 ∧ items = 001 } ⟩\n            if { state = 100 ∧ items = 100 } → { state = 001 ∧ items = 001 } ⟩\n            if { state = 101 ∧ items = 101 } → { state = 011 ∧ items = 011 } ⟩\n            if { state = 110 ∧ items = 110 } → { state = 011 ∧ items = 011 } ⟩\n\n\n\nSimilarly, for the purposes of a proof outline, the Sink has three operations:\n1. extract: item[2] ← 0\n2. drain: state[2] ← 0\n3. pull: 〈 await ¬{ state = 000 } :\n            if { state = 010 ∧ items = 010 } → { state = 001 ∧ items = 001 } ⟩\n            if { state = 100 ∧ items = 100 } → { state = 001 ∧ items = 001 } ⟩\n            if { state = 101 ∧ items = 101 } → { state = 011 ∧ items = 011 } ⟩\n            if { state = 110 ∧ items = 110 } → { state = 011 ∧ items = 011 } ⟩\n\n\n\nConsider first the effect of the push operation. Given the await condition, the state of the system must be ¬{ state = 111 }. Moreover, the push operation includes a “move” operation such that there are no “holes” in the data being transferred. That is, immediately after the (atomic) push operation completes, we have\n     /* { state = 000 ∧ items = 000 } ∨ */\n     /* { state = 001 ∧ items = 001 } ∨ */\n     /* { state = 011 ∧ items = 011 }   */\nActions from the asynchronous Sink can then occur, i.e., an extract, drain, and/or pull.\nGiven the operations shown above, we can see that the predicate\n     /* { state = 001 ∧ items = 001 } */\ncan become\n     /* { state = 001 ∧ ( items = 001 ∨ 000 } */\nif an extract occurs. A drain may then cause { state = 001 } to become { state = 000 }. But note that a drain can only follow an extract, so the predicate\n     /* { state = 000 ∧ items = 001 } */\ncannot occur. Applying the possible occurrences of extract-drain-pull, we have the following potential predicate that can occur due to an asynchronous Sink following the Source push:\n     /* { state = 000 ∧ items = 000 }                   ∨ */ \n     /* { state = 001 ∧ ( items = 001 ∨ items = 000 ) } ∨ */\n     /* { state = 010 ∧ items = 010 }                   ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 010 ) }   */\nThis is a “stable” predicate, meaning there are no other possible states that could occur due to asynchronous Sink actions.\n\n\nThe final state of the push operation will also be the initial state prior to inject. If we begin there and apply the state changes that would be caused by inject and fill (along with the asynchronous Sink), we arrive at the following complete proof outline for the Source:\n   while (not done) {\n     /* { state = 000 ∧ items = 000 }                   ∨ */ \n     /* { state = 001 ∧ ( items = 001 ∨ items = 000 ) } ∨ */\n     /* { state = 010 ∧ items = 010 }                   ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 010 ) }   */\n\n     inject: items[0] ← 1\n\n     /* { state = 000 ∧ items = 100 }                   ∨ */\n     /* { state = 001 ∧ ( items = 101 ∨ items = 100 ) } ∨ */\n     /* { state = 010 ∧ items = 110 }                   ∨ */\n     /* { state = 011 ∧ ( items = 111 ∨ items = 110 ) }   */\n\n     fill: state[0] ← 1\n\n     /* { state = 000 ∧ items = 000 }                   ∨ */ \n     /* { state = 001 ∧ ( items = 001 ∨ items = 000 ) } ∨ */\n     /* { state = 010 ∧ items = 010 }                   ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 010 ) } ∨ */\n     /* { state = 100 ∧ items = 100 }                   ∨ */ \n     /* { state = 101 ∧ ( items = 101 ∨ items = 100 ) } ∨ */\n     /* { state = 110 ∧ items = 110 }                   ∨ */\n     /* { state = 111 ∧ ( items = 111 ∨ items = 110 ) }   */\n\n     push: 〈 await ¬{ state = 111 } \n\n     /* { state = 000 ∧ items = 000 }                   ∨ */ \n     /* { state = 001 ∧ ( items = 001 ∨ items = 000 ) } ∨ */\n     /* { state = 010 ∧ items = 010 }                   ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 010 ) }   */\n   }\n\n\n\n\nWe can apply a similar process to derive the Sink proof outline. In this case, we begin with the system state immediately following the pull operation, which is ¬{ state = 000 }. The pull operation will also move the items, in similar fashion to push above, i.e.:\n     /* { state = 001 ∧ items = 001 } ∨ */\n     /* { state = 011 ∧ items = 011 } ∨ */\n     /* { state = 111 ∧ items = 111 }   */\nNow we consider the changes to this state that may occur due to asynchronous operation of the Source, i.e., inject, fill, and/or push. Given the operations shown above, we can see that the predicate\n     /* { state = 001 ∧ items = 001 } */\ncan become\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 } */\nif an inject occurs.\nA fill will cause { state = 001 } to become { state = 101 }. But note that a fill can only follow an inject, so the predicate\n     /* { state = 101 ∧ items = 001 } */\ncannot occur. Applying the possible occurrences of inject-fill-push, we have the following potential states that can occur due to an asynchronous Source following the Sink pull:\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 ) } ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 111 ) } ∨ */\n     /* { state = 101 ∧ items = 101 }                   ∨ */\n     /* { state = 111 ∧ items = 111 }                     */\nThese are stable states, meaning there are no other possible states that could occur due to the Source.\n\n\n\nEvolving the states from there based on operations of the Sink and taking into account asynchronous operations of the Source we obtain the following proof outline:\n   while (not done) {\n\n     /* { state = 000 ∧ ( items = 000 ∨ items = 100 ) } ∨ */\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 ) } ∨ */\n     /* { state = 010 ∧ ( items = 010 ∨ items = 110 ) } ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 111 ) } ∨ */\n     /* { state = 100 ∧ items = 100 }                   ∨ */\n     /* { state = 101 ∧ items = 101 }                   ∨ */\n     /* { state = 110 ∧ items = 110 }                   ∨ */\n     /* { state = 111 ∧ items = 111 }                     */\n\n     pull: 〈 await ¬{ state = 000 } :\n\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 ) } ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 111 ) } ∨ */\n     /* { state = 101 ∧ items = 101 }                   ∨ */\n     /* { state = 111 ∧ items = 111 }                     */\n\n     extract: extract: item[2] ← 0\n\n     /* { state = 001 ∧ ( items = 000 ∨ items = 100 ) } ∨ */\n     /* { state = 011 ∧ ( items = 010 ∨ items = 110 ) } ∨ */\n     /* { state = 101 ∧ items = 100 }                   ∨ */\n     /* { state = 111 ∧ items = 110 }                     */\n\n     drain: state[2] ← 0\n\n     /* { state = 000 ∧ ( items = 000 ∨ items = 100 ) } ∨ */\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 ) } ∨ */\n     /* { state = 010 ∧ ( items = 010 ∨ items = 110 ) } ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 111 ) } ∨ */\n     /* { state = 100 ∧ items = 100 }                   ∨ */\n     /* { state = 101 ∧ items = 101 }                   ∨ */\n     /* { state = 110 ∧ items = 110 }                   ∨ */\n     /* { state = 111 ∧ items = 111 }                     */\n   }\n\n\n\nAs with the unbuffered case, the the buffered proof outlines shows the same important characteristics of the port state machine.\n\nIf we begin with the valid state { state = 000 ∧ items = 000 } (which is the only sensible state with which to begin), the state machine will never enter any BAD state. We define a BAD state to be any of the following:\n\n\npredicate with { items = 1x0 ∨ items = 1x1 } prior to inject\npredicate with { state = 1x0 ∨ state = 1x1 } prior to fill\npredicate with { items = 0x0 ∨ items = 1x0 } prior to extract\npredicate with { state = 0x0 ∨ state = 1x0 } prior to drain\n\n\nIf we begin with the valid state { state = 000 ∧ items = 000 }, and use the following steps for the Source\n  inject\n  fill\n  push\nThe Source will always be ready to accept an item for injection. Hence we do not have to check whether the Source is ready prior to invoking inject. No concurrent Sink action will change the state into one that is BAD prior to inject.\nSimilarly, if we begin with the valid state { state = 000 ∧ items = 000 } and use the following steps for the Sink\n  pull\n  extract\n  drain\nThere will always be an item ready to extract after pull completes.\nThere are no race conditions between inject and fill, nor between extract and drain. Concurrent actions from the Sink cannot cause a BAD state between inject and fill, nor can concurrent actions from the Source introduce a BAD state between extract and drain.\nIn fact, there are no race conditions between any of the steps in the Source or the Sink. As a result, we do not need to introduce any locking mechanism to make any pairs of actions atomic. (As a reminder, all actions executed by the state machine are atomic.)\n\n\n\n\n\n\n\nIn more detail, we can describe the Source behavior (including proof outline predicates). The steps of Source operation are pseudocode in normal text, while the associated state of the state machine are given in comments, with predicates in curly braces.\n  init: /* { state = 00 ∧ source_item = empty } */\n  while (not done)\n\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n     client of the source inserts an item  /* Note that although the Sink can execute and potentially change the\n                                              state here, the allowable transitions do not end up changing it */\n\n\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = full } */\n     client invokes fill event to transition from empty to full.\n\n     state machine locks mutex\n     /* { mutex = locked } */\n     state machine invokes exit action\n     if { state = 00 ∨ state = 01 } → none\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = full } */\n     state machine performs transition\n     /* { state = 00 } → { state = 10 } ∧ { source_item = full } */\n     /* { state = 01 } → { state = 11 } ∧ { source_item = full } */\n     /* { state = 10 ∨ state = 11 } ∧ { source_item = full } */\n     Source notifies Sink that it is full\n     /* { state = 10 ∨ state = 11 } ∧ { source_item = full } */\n     Source returns\n     state machine unlocks mutex\n     /* { mutex = unlocked } */\n\n     /* Before the Source begins the push, the Sink may pull, drain, do both, or do nothing */\n\n     /* { state = 10 ∨ state = 11 ∨ state = 01 ∨ state = 00 } ∧ { source_item = empty ∨ source_item = full } */\n     client invokes push event\n     state machine locks the mutex\n     /* { mutex = locked */\n\n     /* { state = 10 ∨ state = 11 ∨ state = 01 ∨ state = 00 } ∧ { source_item = empty ∨ source_item = full } */\n     state machine executes push exit action, which may be one of the following, depending on the state\n     restart:\n       if { state = 00 ∨ state = 01 } → none\n       if state = 10 → execute source_swap\n       if state = 11 → execute source_wait\n         pre_source_swap: /* { state = 10 } ∧ { source_item = full } */\n            state machine swaps source_item and sink_item -- swap does not change state\n         post_source_swap: /* { state = 10 } ∧ { source_item = empty } */\n\n       if { state = 11 } → execute source_wait  \n          pre_source_wait: /* { state = 11 } */\n          /* unlock mutex and wait for Sink to become empty */\n          /* Important! When the state machine comes back from wait, it is now no longer in the state it was when it started the wait. */\n          /* We therefore restart event processing for the push event, given the state present when coming back from wait: goto restart.*/\n       /* { mutex = locked } */\n\n       /* { state = 00 ∨ state = 01 ∨ state = 10 } ∧ { source_item = empty } */      \n       make state transition according to state transition table and next_state set by most recent event\n         { state = 00 } → { state = 00 }\n         { state = 01 ∨ state = 10 } → { state = 01 }\n\n       /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n       state machine invokes entry action (none) \n\n       post_entry: /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n       state machine unlocks mutex\n       /* { mutex = unlocked } */\n\n      post_push: /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n    end_loop: /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n  post_loop: /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n\n\n\nThe Sink is the dual of the Source. Note that we start with pull. We can describe the Sink behavior (including proof outline predicates):\n  init: /* { state = 00 ∧ sink_item = empty } */\n  while (not done)\n     /* { state = 00 ∨ state = 01 } ∧ { sink_item = empty ∨ sink_item = full } */\n     /* Before client invokes the pull event, the source could have filled, filled and pushed, or done nothing */\n       if source filled: 00 → 10\n       if source filled and pushed 00 → 01\n       if source filled and pushed and filled 00 → 11\n       if source did nothing state does not change\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 10 } ∧ { sink_item = empty ∨ sink_item = full } */\n     client invokes pull event\n     state machine locks mutex\n     /* mutex = locked */\n\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧ { sink_item = empty ∨ sink_item = full } */\n     state machine executes pull exit action, which may be one of the following, depending on the state\n     restart:\n       { state = 01 ∨ state = 11 } → none\n       { state = 10 } → sink_swap \n       { state = 00 } → sink_wait\n         pre_sink_swap: /* { state = 10 } ∧ { sink_item = empty } */\n         post_sink_swap: /* { state = 10 } ∧ { sink_item = full } */\n\n         if { state = 00 } → execute sink_wait\n         pre_sink_wait: /* { state = 00 } */ \n           /* unlock mutex and wait for Source to become full */\n           /* Important! When the state machine comes back from wait, it is now no longer in the state it was when it started the wait. */\n           /* We therefore restart event processing for the pull event, given the state present when coming back from wait: goto restart.*/\n       /* { mutex = locked */\n\n       /* { state = 01 ∨ state = 10 ∨ state = 11 } ∧ { sink_item = full } */      \n       make state transition according to state transition table and state and next_state set by most recent event\n         { state = 01 ∨ state = 10 } → { state = 01 }\n         { state = 11 } → { state = 11 }\n\n       /* { state = 01 ∨ state = 11 } ∧ { sink_item = full } */\n       state machine invokes pull entry action (none)\n       /* post_entry: { state = 01 ∨ state = 11 } ∧ { sink_item = full } */ \n     state machine unlocks mutex\n     /* { mutex = unlocked */\n\n     /* post_pull: { state = 01 ∨ state = 11 } ∧ { sink_item = full } */\n\n     client of the sink extracts the item  /* Note that although the Source can execute and potentially change the\n                                                state here, the allowable transitions do not end up changing it */\n     /* { state = 01 ∨ state = 11 } ∧ { sink_item = empty } */\n     client invokes drain event to transition from full to empty \n     state machine locks mutex\n     /* { state = 01 ∨ state = 11 } ∧ { sink_item = empty } */\n     state machine performs exit action\n     if { state = 01 ∨ state = 11 } → none\n     { state = 01 ∨ state = 11 }\n     state machine performs transition\n     { state = 01 } → { state = 00 }\n     { state = 11 } → { state = 10 }\n     /* { state = 00 ∨ state = 10 } ∧ { sink_item = empty } */\n     state machine performs entry action\n       { state = 00 } → notify_source\n       { state = 10 } → notify_source\n     Sink returns\n     state machine unlocks mutex\n     /* end_loop: { state = 00 ∨ state = 10 } ∧ { sink_item = empty } */\n     At this point, the source could inject, fill, push\n        { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧ { sink_item = empty ∨ sink_item = full} */\n  /* post_loop: { state = 00 ∨ state = 10 } ∧ { sink_item = empty } */\n\n\n\nFrom the above analysis, we can summarize the Source proof outline below. In a manner similar to how states are represented, we indicate whether items are empty or full using 0 or 1.\n\n\n   while (not done) {\n     /* { state = 00 ∨ state = 01 } ∧ ( items = 00 ∨ items = 01 ) }   */\n     inject\n     /* { state = 00 ∨ state = 01 } ∧ ( items = 10 ∨ items = 11 ) }   */\n     fill\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧       */\n     /* { items = 00 ∨ items = 01 ∨ items = 10 ∨ state = 11 }         */\n     push\n     /* { state = 00 ∨ state = 01 } ∧ ( items = 00 ∨ items = 01 ) }   */\n   }\n\n\n\nFrom the above analysis, we can summarize the Sink proof outline as follows:\n   while (not done) {\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧       */\n     /* { items = 00 ∨ items = 01 ∨ items = 10 ∨ state = 11 }         */\n     pull\n     /* { state = 01 ∨ state = 11 } ∧ { items = 01 ∨ items = 11 }     */\n     extract\n     /* { state = 01 ∨ state = 11 } ∧ { items = 00 ∨ items = 10 }     */\n     drain\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧       */\n     /* { items = 00 ∨ items = 01 ∨ items = 10 ∨ state = 11 }         */\n   }\n\n\n\n\nIf we just consider the Source actions without considering concurrent Sink operations, we have the following proof outline:\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } */\n     fill: state[0] ← 1\n     /* { state = 10 ∧ items = 10 } */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ { items = 01 } */\n   }\nWe can make a second iteration of this proof outline, adding the condition { state = 01 ∧ items = 01 } to the beginning, since that is the final state of the previous iteration:\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ items = 01 } */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ items = 11 } */\n     fill: state[0] ← 1\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ items = 11 } */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ { items = 01 } */\n   }\nNote that if we enter push with { state = 11 }, we will not exit until that state changes, which will require action from the Sink.\n\n\n\nIf we just consider the Sink actions without considering concurrent Source operations, we have the following proof outline:\n   while (not done) {\n     /* { state = 10 ∧ items = 10 } */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ items = 01 } */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ items = 00 } */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ items = 00 } */\n   }\nA second iteration of the Sink actions would result in\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 10 ∧ items = 10 } */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ { items = 01 } */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ { items = 00 } */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ { items = 00 } */\n   }\nHere, we are stopped in the await statement, pending action by the Source.and still result in a valid proof outline.\n\n\n\nWe now work through the Source proof, allowing arbitrary Sink actions to occur between any two Source action. The allowable Sink actions at any point must have the same predicate at the point where the Sink action occurs. For example, the predicate prior to Source inject includes { state = 01 ∧ items = 01 }, which is also a predicate prior to extract. Thus, extract could also occur prior to inject. We can therefore include the predicate resulting from the extract action as part of the predicate prior to inject. Applying allowable Sink actions in this fashion to all of the Source predicates, we obtain:\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } */\n\n        /* extract: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 00 } */\n        /* drain: { state = 01 ∧ items = 00 } → { state = 00 ∧ items = 00 } */\n        /* pull: no change\n\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ ( items = 10 ∨ items = 11 ) } */\n\n        /* extract: { state = 01 ∧ items = 11 } → { state = 01 ∧ items = 10 } */\n        /* drain: { state = 01 ∧ items = 10 } → { state = 00 ∧ items = 10 } */\n        /* pull: no change\n\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ ( items = 10 ∨ items = 11 ) } */\n     fill: state[0] ← 1\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ ( items = 10 ∨ items = 11 ) } */\n\n        /* extract: { state = 11 ∧ items = 11 } → { state = 11 ∧ items = 10 } */\n        /* drain: { state = 11 ∧ items = 10 } → { state = 10 ∧ items = 10 } */\n        /* pull: { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } */\n        /* pull+extract: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 00 } */\n        /* pull+extract+drain: { state = 01 ∧ items = 00 } → { state = 00 ∧ items = 00 } */\n\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } ∨ */\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ ( items = 10 ∨ items = 11 ) } */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } */\n   }\n\n\n\nApplying the same process to the Sink proof outline as we did for the Source proof outline:\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 10 ∧ items = 10 } */\n\n       /* inject: { state = 00 ∧ items = 00 } → { state = 00 ∧ items = 10 }\n       /* inject+fill: { state = 00 ∧ items = 10 } → { state = 10 ∧ items = 10 }\n       /* inject+fill+push: { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 }\n       /* inject+fill+push+inject: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 11 }\n       /* inject+fill+push+inject+fill: { state = 01 ∧ items = 11 } → { state = 11 ∧ items = 11 }\n\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 } ∨  */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) }  */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n\n       /* inject: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 11 } */\n       /* fill: { state = 01 ∧ items = 01 } → { state = 11 ∧ items = 11 } */\n       /* push: no change \n\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 11 ∧ items = 01 ) } */\n\n       /* inject: { state = 01 ∧ items = 00 } → { state = 01 ∧ items = 10 } */\n       /* fill: { state = 01 ∧ items = 11 } → { state = 11 ∧ items = 10 } */\n       /* push: no change \n\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 11 ∧ items = 01 ) } */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 01 ) } */\n\n       /* inject: { state = 00 ∧ items = 00 } → { state = 00 ∧ items = 10 }\n       /* inject+fill: { state = 00 ∧ items = 10 } → { state = 10 ∧ items = 10 }\n       /* inject+fill+push: { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 }\n       /* inject+fill+push+inject: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 11 }\n       /* inject+fill+push+inject+fill: { state = 01 ∧ items = 11 } → { state = 11 ∧ items = 11 }\n\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 } ∨  */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) }  */\n   }\n\n\n\nSummarizing the proof outlines above (including only the relevant predicates), we obtain the proof outlines below. Note that these are compatible with the previously derived proof outlines, but the predicates exclude certain combinations of states and items. Where helpful, we include these predicates as comments in the dag source code files.\n\n\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ ( items = 10 ∨ items = 11 ) }   */\n     fill: state[0] ← 1\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } ∨ */\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ ( items = 10 ∨ items = 11 ) }   */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */     \n   }\n\n\n\n   while (not done) {\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 } ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 11 ∧ items = 01 ) } */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 } ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n   }"
  },
  {
    "objectID": "experimental/tiledb/common/dag/state_machine/doc/fsm2.html#proof-outline",
    "href": "experimental/tiledb/common/dag/state_machine/doc/fsm2.html#proof-outline",
    "title": "Analysis of Port State Machine",
    "section": "",
    "text": "To prove correctness of the port state machine, we use proof outline techniques from e.g., Concurrent Programming by Greg Andrews. For this proof outline, we represent the overall state of a connected Source and Sink with two boolean arrays, each with two elements, shared by the Source and Sink: state and items.\n\n\nFor the purposes of a proof outline, the Source has three operations:\n1. inject: items[0] ← 1\n2. fill: state[0] ← 1\n3. push: 〈 await ¬{ state = 11 } :\n            if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\nSimilarly, for the purposes of a proof outline, the Sink has three operations:\n1. extract: item[1] ← 0\n2. drain: state[1] ← 0\n3. pull: 〈 await ¬{ state = 00 } :\n            if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n\n\n\nThe critical operation of the Source is the push, which blocks (awaits) until ¬{ state = 11 }, that is, until the state is not equal to 11. In that case, states 00, 01, and 10 could pass through. However, in addition to the await, push includes a data transfer step (a “move”) that will perform the following predicate transformation for { state = 10 }:\n/* { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } */\nThis represents moving a data item from the Source to the Sink. Note that both the state machine state and the actual data items being held are (atomically) changed in the move.\nBased on these properties of the push, we have the following valide states that can occur directly following a push:\n     /* { state = 00 ∧ items = 00 } ∨ */\n     /* { state = 01 ∧ items = 01 }   */\nThe push itself is atomic, but as soon as it has completed, the state of the port can be changed by the Sink operating asynchronously to the Source. Given the three operations that the Sink can perform, the above states can be subject to pull, extract, or extract+drain. The pull operation will not change the state. The extract operation can change { items = 01 } to { items = 00 }. Similarly, drain can change { state = 01 } to { state = 00 }. Note that extract always precedes drain – the Sink cannot change { state = 01 ∧ items = 01 } into { state = 00 ∧ items = 01 }.\nThus, the entire set of possible configurations following push are\n     /* { state = 00 ∧ items = 00 }                  ∨ */\n     /* { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */\nFollowing the push, the Source may inject, which will change the above states to:\n     /* { state = 00 ∧ items = 10 }                  ∨ */\n     /* { state = 01 ∧ ( items = 11 ∨ items = 10 ) }   */\nAgain, the Sink may extract, drain, or pull. However, the only change that can occur is that extract may change { items = 11 } to { items = 10 }, which is already part of the above configuration. A subsequent drain may then change\n{ state = 01 ∧ items = 10 } to { state = 00 ∧ items = 10 }, which again, is already part of the above configuration. Thus, the Sink operating asynchronously will not change the system predicate between inject and fill. In other words, the sequence inject then fill can be considered to be atomic.\nFor the fill operation, the above predicate will be changed to\n     /* { state = 10 ∧ items = 10 }                  ∨ */\n     /* { state = 11 ∧ ( items = 11 ∨ items = 10 ) }   */\nAsynchronous operation of the Sink can change this predicate to\n     /* { state = 00 ∧ items = 00 }                  ∨ */\n     /* { state = 01 ∧ ( items = 00 ∨ items = 01 ) } ∨ */\n     /* { state = 10 ∧ items = 10 }                  ∨ */\n     /* { state = 11 ∧ ( items = 10 ∨ items = 11 ) }   */\n\n\nThus, the proof outline for the Source is\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ ( items = 10 ∨ items = 11 ) }   */\n     fill: state[0] ← 1\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } ∨ */\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ ( items = 10 ∨ items = 11 ) }   */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */     \n   }\n\n\n\n\nWe can follow a similar process to derive the proof outline for the Sink.\nThe critical operation of the Sink is the pull, which blocks (awaits) until ¬{ state = 00 }, that is, until the state is not equal to 00. In that case, states 01, 10, and 11 could pass through. However, in addition to the await, pull includes a data transfer step (a “move”) that will perform the following predicate transformation for { state = 10 }:\n/* { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } */\nThis represents moving a data item from the Source to the Sink. Note that both the state machine state and the actual data items being held are (atomically) changed in the move.\nBased on these properties of the push, we have the following valide states that can occur directly following a push:\n     /* { state = 01 ∧ items = 01 } ∨ */\n     /* { state = 11 ∧ items = 11 }   */\nThe pull itself is atomic, but as soon as it has completed, the state of the port can be changed by the Source operating asynchronously to the Sink. Given the three operations that the Source can perform, the above states can be subject to push, inject, or inject+fill. The push operation will not change the predicate. The inject operation can change { items = 01 } to { items = 11 }. Similarly, fill can change { state = 01 } to { state = 11 }. Note that inject always precedes fill – the Source cannot change { state = 01 ∧ items = 01 } into { state = 11 ∧ items = 01 }.\nThus, the entire set of possible configurations following push are\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ */\n     /* { state = 11 ∧ items = 11 }                    */\nFollowing the pull, the Sink may extract, which will change the above predicate to:\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ */\n     /* { state = 11 ∧ items = 10 }                    */\nAgain, the Source may inject, fill, or push. However, the only change that can occur is that inject may change { items = 00 } to { items = 10 }, which is already part of the above configuration. A subsequent fill may then change\n{ state = 01 ∧ items = 10 } to { state = 11 ∧ items = 10 }, which again, is already part of the above configuration. Thus, the Source operating asynchronously will not change the system predicate between extract and drain. In other words, the sequence extract then drain can be considered to be atomic.\nFor the drain operation, the above predicate will be changed to\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ */\n     /* { state = 10 ∧ items = 10 }                    */\nAsynchronous operation of the Source can change this predicate to\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ */\n     /* { state = 10 ∧ items = 10 }                  ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ */\n     /* { state = 11 ∧ items = 11 }                    */\n\n\nThus, the complete Sink proof outline iw\n   while (not done) {\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 }   ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 }     */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) }   */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 11 ∧ items = 01 ) }   */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 }   ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) }   */\n   }\n\n\n\n\nThe proof outlines shows some important characteristics of the port state machine.\n\nIf we begin with the valid state { state = 00 ∧ items = 00 } (which is the only sensible state with which to begin), the state machine will never enter any BAD state. We define a BAD state to be any of the following:\n\n\npredicate with { items = 10 ∨ items = 11 } prior to inject\npredicate with { state = 10 ∨ state = 11 } prior to fill\npredicate with { items = 00 ∨ items = 10 } prior to extract\npredicate with { state = 00 ∨ state = 10 } prior to drain\n\n\nIf we begin with the valid state { state = 00 ∧ items = 00 }, and use the following steps for the Source\n  inject\n  fill\n  push\nThe Source will always be ready to accept an item for injection. Hence we do not have to check whether the Source is ready prior to invoking inject. No concurrent Sink action will change the state into one that is BAD prior to inject.\nSimilarly, if we use the following steps for the Sink\n  pull\n  extract\n  drain\nThere will always be an item ready to extract after pull completes.\nThere are no race conditions between inject and fill, nor between extract and drain. Concurrent actions from the Sink cannot cause a BAD state between inject and fill, nor can concurrent actions from the Source introduce a BAD state between extract and drain.\nIn fact, there are no race conditions between any of the steps in the Source or the Sink. As a result, we do not need to introduce any locking mechanism to make any pairs of actions atomic. (As a reminder, all actions executed by the state machine are atomic.)\nOperations carried out directly by the state machine are protected by a lock. When the Source or Sink wait, they do so on a condition variable using that same lock. Note that when the Source and Sink both exit their loops that the Source will have { state = 00 ∨ state = 01 } while the Sink will have { state = 00 ∨ state = 10 }. The final state of the state machine is therefore { state = 00 ∨ state = 01 } ∧ { state = 00 ∨ state = 10 } ∧ { item = empty }, i.e., { state = 00 } ∧ { item = empty }. (This assumes that both Source and Sink perform the same number of operations, otherwise one of them will be left in a wait.)\nNB: The “move” portion of push and the “move” portion of pull (source_swap and sink_swap, respectively). Each checks to see if the state is equal to 10, if so, they swap the state to 01 (and perform a swap of the items associated with the source and sink), and notifies the other. If the state is not equal to 10, the swap function notifies the other and goes into a wait. Thus, we may not need separate swaps for Source and Sink, nor separate condition variables, nor separate notification functions. I have verified that this works experimentally, but I am leaving things separate for now."
  },
  {
    "objectID": "experimental/tiledb/common/dag/state_machine/doc/fsm2.html#proof-outlines-for-buffered-edges-between-source-and-sink",
    "href": "experimental/tiledb/common/dag/state_machine/doc/fsm2.html#proof-outlines-for-buffered-edges-between-source-and-sink",
    "title": "Analysis of Port State Machine",
    "section": "",
    "text": "For much of what TileDB will be doing with our task graph library, we will be using multi-stage edges between nodes. Edges, as they are currently implemented, still provide a control connection from the Sink to the Source connected by the Edge. In developing a proof outline for Source and Sink connected by an Edge with one (or more) buffered stages, we must account for the buffered data in both the state and the items.\nWe can follow a similar development for thi proof outline as we did for the unbuffered case.\nRevisiting the Source and Sink operations that we previously presented, we have the following actions for a three-stage port state machine:\n\n\n1. inject: items[0] ← 1\n2. fill: state[0] ← 1\n3. push: 〈 await ¬{ state = 111 } :\n            if { state = 010 ∧ items = 010 } → { state = 001 ∧ items = 001 } ⟩\n            if { state = 100 ∧ items = 100 } → { state = 001 ∧ items = 001 } ⟩\n            if { state = 101 ∧ items = 101 } → { state = 011 ∧ items = 011 } ⟩\n            if { state = 110 ∧ items = 110 } → { state = 011 ∧ items = 011 } ⟩\n\n\n\nSimilarly, for the purposes of a proof outline, the Sink has three operations:\n1. extract: item[2] ← 0\n2. drain: state[2] ← 0\n3. pull: 〈 await ¬{ state = 000 } :\n            if { state = 010 ∧ items = 010 } → { state = 001 ∧ items = 001 } ⟩\n            if { state = 100 ∧ items = 100 } → { state = 001 ∧ items = 001 } ⟩\n            if { state = 101 ∧ items = 101 } → { state = 011 ∧ items = 011 } ⟩\n            if { state = 110 ∧ items = 110 } → { state = 011 ∧ items = 011 } ⟩\n\n\n\nConsider first the effect of the push operation. Given the await condition, the state of the system must be ¬{ state = 111 }. Moreover, the push operation includes a “move” operation such that there are no “holes” in the data being transferred. That is, immediately after the (atomic) push operation completes, we have\n     /* { state = 000 ∧ items = 000 } ∨ */\n     /* { state = 001 ∧ items = 001 } ∨ */\n     /* { state = 011 ∧ items = 011 }   */\nActions from the asynchronous Sink can then occur, i.e., an extract, drain, and/or pull.\nGiven the operations shown above, we can see that the predicate\n     /* { state = 001 ∧ items = 001 } */\ncan become\n     /* { state = 001 ∧ ( items = 001 ∨ 000 } */\nif an extract occurs. A drain may then cause { state = 001 } to become { state = 000 }. But note that a drain can only follow an extract, so the predicate\n     /* { state = 000 ∧ items = 001 } */\ncannot occur. Applying the possible occurrences of extract-drain-pull, we have the following potential predicate that can occur due to an asynchronous Sink following the Source push:\n     /* { state = 000 ∧ items = 000 }                   ∨ */ \n     /* { state = 001 ∧ ( items = 001 ∨ items = 000 ) } ∨ */\n     /* { state = 010 ∧ items = 010 }                   ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 010 ) }   */\nThis is a “stable” predicate, meaning there are no other possible states that could occur due to asynchronous Sink actions.\n\n\nThe final state of the push operation will also be the initial state prior to inject. If we begin there and apply the state changes that would be caused by inject and fill (along with the asynchronous Sink), we arrive at the following complete proof outline for the Source:\n   while (not done) {\n     /* { state = 000 ∧ items = 000 }                   ∨ */ \n     /* { state = 001 ∧ ( items = 001 ∨ items = 000 ) } ∨ */\n     /* { state = 010 ∧ items = 010 }                   ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 010 ) }   */\n\n     inject: items[0] ← 1\n\n     /* { state = 000 ∧ items = 100 }                   ∨ */\n     /* { state = 001 ∧ ( items = 101 ∨ items = 100 ) } ∨ */\n     /* { state = 010 ∧ items = 110 }                   ∨ */\n     /* { state = 011 ∧ ( items = 111 ∨ items = 110 ) }   */\n\n     fill: state[0] ← 1\n\n     /* { state = 000 ∧ items = 000 }                   ∨ */ \n     /* { state = 001 ∧ ( items = 001 ∨ items = 000 ) } ∨ */\n     /* { state = 010 ∧ items = 010 }                   ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 010 ) } ∨ */\n     /* { state = 100 ∧ items = 100 }                   ∨ */ \n     /* { state = 101 ∧ ( items = 101 ∨ items = 100 ) } ∨ */\n     /* { state = 110 ∧ items = 110 }                   ∨ */\n     /* { state = 111 ∧ ( items = 111 ∨ items = 110 ) }   */\n\n     push: 〈 await ¬{ state = 111 } \n\n     /* { state = 000 ∧ items = 000 }                   ∨ */ \n     /* { state = 001 ∧ ( items = 001 ∨ items = 000 ) } ∨ */\n     /* { state = 010 ∧ items = 010 }                   ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 010 ) }   */\n   }\n\n\n\n\nWe can apply a similar process to derive the Sink proof outline. In this case, we begin with the system state immediately following the pull operation, which is ¬{ state = 000 }. The pull operation will also move the items, in similar fashion to push above, i.e.:\n     /* { state = 001 ∧ items = 001 } ∨ */\n     /* { state = 011 ∧ items = 011 } ∨ */\n     /* { state = 111 ∧ items = 111 }   */\nNow we consider the changes to this state that may occur due to asynchronous operation of the Source, i.e., inject, fill, and/or push. Given the operations shown above, we can see that the predicate\n     /* { state = 001 ∧ items = 001 } */\ncan become\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 } */\nif an inject occurs.\nA fill will cause { state = 001 } to become { state = 101 }. But note that a fill can only follow an inject, so the predicate\n     /* { state = 101 ∧ items = 001 } */\ncannot occur. Applying the possible occurrences of inject-fill-push, we have the following potential states that can occur due to an asynchronous Source following the Sink pull:\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 ) } ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 111 ) } ∨ */\n     /* { state = 101 ∧ items = 101 }                   ∨ */\n     /* { state = 111 ∧ items = 111 }                     */\nThese are stable states, meaning there are no other possible states that could occur due to the Source.\n\n\n\nEvolving the states from there based on operations of the Sink and taking into account asynchronous operations of the Source we obtain the following proof outline:\n   while (not done) {\n\n     /* { state = 000 ∧ ( items = 000 ∨ items = 100 ) } ∨ */\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 ) } ∨ */\n     /* { state = 010 ∧ ( items = 010 ∨ items = 110 ) } ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 111 ) } ∨ */\n     /* { state = 100 ∧ items = 100 }                   ∨ */\n     /* { state = 101 ∧ items = 101 }                   ∨ */\n     /* { state = 110 ∧ items = 110 }                   ∨ */\n     /* { state = 111 ∧ items = 111 }                     */\n\n     pull: 〈 await ¬{ state = 000 } :\n\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 ) } ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 111 ) } ∨ */\n     /* { state = 101 ∧ items = 101 }                   ∨ */\n     /* { state = 111 ∧ items = 111 }                     */\n\n     extract: extract: item[2] ← 0\n\n     /* { state = 001 ∧ ( items = 000 ∨ items = 100 ) } ∨ */\n     /* { state = 011 ∧ ( items = 010 ∨ items = 110 ) } ∨ */\n     /* { state = 101 ∧ items = 100 }                   ∨ */\n     /* { state = 111 ∧ items = 110 }                     */\n\n     drain: state[2] ← 0\n\n     /* { state = 000 ∧ ( items = 000 ∨ items = 100 ) } ∨ */\n     /* { state = 001 ∧ ( items = 001 ∨ items = 101 ) } ∨ */\n     /* { state = 010 ∧ ( items = 010 ∨ items = 110 ) } ∨ */\n     /* { state = 011 ∧ ( items = 011 ∨ items = 111 ) } ∨ */\n     /* { state = 100 ∧ items = 100 }                   ∨ */\n     /* { state = 101 ∧ items = 101 }                   ∨ */\n     /* { state = 110 ∧ items = 110 }                   ∨ */\n     /* { state = 111 ∧ items = 111 }                     */\n   }\n\n\n\nAs with the unbuffered case, the the buffered proof outlines shows the same important characteristics of the port state machine.\n\nIf we begin with the valid state { state = 000 ∧ items = 000 } (which is the only sensible state with which to begin), the state machine will never enter any BAD state. We define a BAD state to be any of the following:\n\n\npredicate with { items = 1x0 ∨ items = 1x1 } prior to inject\npredicate with { state = 1x0 ∨ state = 1x1 } prior to fill\npredicate with { items = 0x0 ∨ items = 1x0 } prior to extract\npredicate with { state = 0x0 ∨ state = 1x0 } prior to drain\n\n\nIf we begin with the valid state { state = 000 ∧ items = 000 }, and use the following steps for the Source\n  inject\n  fill\n  push\nThe Source will always be ready to accept an item for injection. Hence we do not have to check whether the Source is ready prior to invoking inject. No concurrent Sink action will change the state into one that is BAD prior to inject.\nSimilarly, if we begin with the valid state { state = 000 ∧ items = 000 } and use the following steps for the Sink\n  pull\n  extract\n  drain\nThere will always be an item ready to extract after pull completes.\nThere are no race conditions between inject and fill, nor between extract and drain. Concurrent actions from the Sink cannot cause a BAD state between inject and fill, nor can concurrent actions from the Source introduce a BAD state between extract and drain.\nIn fact, there are no race conditions between any of the steps in the Source or the Sink. As a result, we do not need to introduce any locking mechanism to make any pairs of actions atomic. (As a reminder, all actions executed by the state machine are atomic.)"
  },
  {
    "objectID": "experimental/tiledb/common/dag/state_machine/doc/fsm2.html#additional-alternative-proof-outlines-in-various-detail",
    "href": "experimental/tiledb/common/dag/state_machine/doc/fsm2.html#additional-alternative-proof-outlines-in-various-detail",
    "title": "Analysis of Port State Machine",
    "section": "",
    "text": "In more detail, we can describe the Source behavior (including proof outline predicates). The steps of Source operation are pseudocode in normal text, while the associated state of the state machine are given in comments, with predicates in curly braces.\n  init: /* { state = 00 ∧ source_item = empty } */\n  while (not done)\n\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n     client of the source inserts an item  /* Note that although the Sink can execute and potentially change the\n                                              state here, the allowable transitions do not end up changing it */\n\n\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = full } */\n     client invokes fill event to transition from empty to full.\n\n     state machine locks mutex\n     /* { mutex = locked } */\n     state machine invokes exit action\n     if { state = 00 ∨ state = 01 } → none\n     /* { state = 00 ∨ state = 01 } ∧ { source_item = full } */\n     state machine performs transition\n     /* { state = 00 } → { state = 10 } ∧ { source_item = full } */\n     /* { state = 01 } → { state = 11 } ∧ { source_item = full } */\n     /* { state = 10 ∨ state = 11 } ∧ { source_item = full } */\n     Source notifies Sink that it is full\n     /* { state = 10 ∨ state = 11 } ∧ { source_item = full } */\n     Source returns\n     state machine unlocks mutex\n     /* { mutex = unlocked } */\n\n     /* Before the Source begins the push, the Sink may pull, drain, do both, or do nothing */\n\n     /* { state = 10 ∨ state = 11 ∨ state = 01 ∨ state = 00 } ∧ { source_item = empty ∨ source_item = full } */\n     client invokes push event\n     state machine locks the mutex\n     /* { mutex = locked */\n\n     /* { state = 10 ∨ state = 11 ∨ state = 01 ∨ state = 00 } ∧ { source_item = empty ∨ source_item = full } */\n     state machine executes push exit action, which may be one of the following, depending on the state\n     restart:\n       if { state = 00 ∨ state = 01 } → none\n       if state = 10 → execute source_swap\n       if state = 11 → execute source_wait\n         pre_source_swap: /* { state = 10 } ∧ { source_item = full } */\n            state machine swaps source_item and sink_item -- swap does not change state\n         post_source_swap: /* { state = 10 } ∧ { source_item = empty } */\n\n       if { state = 11 } → execute source_wait  \n          pre_source_wait: /* { state = 11 } */\n          /* unlock mutex and wait for Sink to become empty */\n          /* Important! When the state machine comes back from wait, it is now no longer in the state it was when it started the wait. */\n          /* We therefore restart event processing for the push event, given the state present when coming back from wait: goto restart.*/\n       /* { mutex = locked } */\n\n       /* { state = 00 ∨ state = 01 ∨ state = 10 } ∧ { source_item = empty } */      \n       make state transition according to state transition table and next_state set by most recent event\n         { state = 00 } → { state = 00 }\n         { state = 01 ∨ state = 10 } → { state = 01 }\n\n       /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n       state machine invokes entry action (none) \n\n       post_entry: /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n       state machine unlocks mutex\n       /* { mutex = unlocked } */\n\n      post_push: /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n    end_loop: /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n  post_loop: /* { state = 00 ∨ state = 01 } ∧ { source_item = empty } */\n\n\n\nThe Sink is the dual of the Source. Note that we start with pull. We can describe the Sink behavior (including proof outline predicates):\n  init: /* { state = 00 ∧ sink_item = empty } */\n  while (not done)\n     /* { state = 00 ∨ state = 01 } ∧ { sink_item = empty ∨ sink_item = full } */\n     /* Before client invokes the pull event, the source could have filled, filled and pushed, or done nothing */\n       if source filled: 00 → 10\n       if source filled and pushed 00 → 01\n       if source filled and pushed and filled 00 → 11\n       if source did nothing state does not change\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 10 } ∧ { sink_item = empty ∨ sink_item = full } */\n     client invokes pull event\n     state machine locks mutex\n     /* mutex = locked */\n\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧ { sink_item = empty ∨ sink_item = full } */\n     state machine executes pull exit action, which may be one of the following, depending on the state\n     restart:\n       { state = 01 ∨ state = 11 } → none\n       { state = 10 } → sink_swap \n       { state = 00 } → sink_wait\n         pre_sink_swap: /* { state = 10 } ∧ { sink_item = empty } */\n         post_sink_swap: /* { state = 10 } ∧ { sink_item = full } */\n\n         if { state = 00 } → execute sink_wait\n         pre_sink_wait: /* { state = 00 } */ \n           /* unlock mutex and wait for Source to become full */\n           /* Important! When the state machine comes back from wait, it is now no longer in the state it was when it started the wait. */\n           /* We therefore restart event processing for the pull event, given the state present when coming back from wait: goto restart.*/\n       /* { mutex = locked */\n\n       /* { state = 01 ∨ state = 10 ∨ state = 11 } ∧ { sink_item = full } */      \n       make state transition according to state transition table and state and next_state set by most recent event\n         { state = 01 ∨ state = 10 } → { state = 01 }\n         { state = 11 } → { state = 11 }\n\n       /* { state = 01 ∨ state = 11 } ∧ { sink_item = full } */\n       state machine invokes pull entry action (none)\n       /* post_entry: { state = 01 ∨ state = 11 } ∧ { sink_item = full } */ \n     state machine unlocks mutex\n     /* { mutex = unlocked */\n\n     /* post_pull: { state = 01 ∨ state = 11 } ∧ { sink_item = full } */\n\n     client of the sink extracts the item  /* Note that although the Source can execute and potentially change the\n                                                state here, the allowable transitions do not end up changing it */\n     /* { state = 01 ∨ state = 11 } ∧ { sink_item = empty } */\n     client invokes drain event to transition from full to empty \n     state machine locks mutex\n     /* { state = 01 ∨ state = 11 } ∧ { sink_item = empty } */\n     state machine performs exit action\n     if { state = 01 ∨ state = 11 } → none\n     { state = 01 ∨ state = 11 }\n     state machine performs transition\n     { state = 01 } → { state = 00 }\n     { state = 11 } → { state = 10 }\n     /* { state = 00 ∨ state = 10 } ∧ { sink_item = empty } */\n     state machine performs entry action\n       { state = 00 } → notify_source\n       { state = 10 } → notify_source\n     Sink returns\n     state machine unlocks mutex\n     /* end_loop: { state = 00 ∨ state = 10 } ∧ { sink_item = empty } */\n     At this point, the source could inject, fill, push\n        { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧ { sink_item = empty ∨ sink_item = full} */\n  /* post_loop: { state = 00 ∨ state = 10 } ∧ { sink_item = empty } */\n\n\n\nFrom the above analysis, we can summarize the Source proof outline below. In a manner similar to how states are represented, we indicate whether items are empty or full using 0 or 1.\n\n\n   while (not done) {\n     /* { state = 00 ∨ state = 01 } ∧ ( items = 00 ∨ items = 01 ) }   */\n     inject\n     /* { state = 00 ∨ state = 01 } ∧ ( items = 10 ∨ items = 11 ) }   */\n     fill\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧       */\n     /* { items = 00 ∨ items = 01 ∨ items = 10 ∨ state = 11 }         */\n     push\n     /* { state = 00 ∨ state = 01 } ∧ ( items = 00 ∨ items = 01 ) }   */\n   }\n\n\n\nFrom the above analysis, we can summarize the Sink proof outline as follows:\n   while (not done) {\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧       */\n     /* { items = 00 ∨ items = 01 ∨ items = 10 ∨ state = 11 }         */\n     pull\n     /* { state = 01 ∨ state = 11 } ∧ { items = 01 ∨ items = 11 }     */\n     extract\n     /* { state = 01 ∨ state = 11 } ∧ { items = 00 ∨ items = 10 }     */\n     drain\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧       */\n     /* { items = 00 ∨ items = 01 ∨ items = 10 ∨ state = 11 }         */\n   }\n\n\n\n\nIf we just consider the Source actions without considering concurrent Sink operations, we have the following proof outline:\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } */\n     fill: state[0] ← 1\n     /* { state = 10 ∧ items = 10 } */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ { items = 01 } */\n   }\nWe can make a second iteration of this proof outline, adding the condition { state = 01 ∧ items = 01 } to the beginning, since that is the final state of the previous iteration:\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ items = 01 } */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ items = 11 } */\n     fill: state[0] ← 1\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ items = 11 } */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ { items = 01 } */\n   }\nNote that if we enter push with { state = 11 }, we will not exit until that state changes, which will require action from the Sink.\n\n\n\nIf we just consider the Sink actions without considering concurrent Source operations, we have the following proof outline:\n   while (not done) {\n     /* { state = 10 ∧ items = 10 } */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ items = 01 } */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ items = 00 } */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ items = 00 } */\n   }\nA second iteration of the Sink actions would result in\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 10 ∧ items = 10 } */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ { items = 01 } */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ { items = 00 } */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ { items = 00 } */\n   }\nHere, we are stopped in the await statement, pending action by the Source.and still result in a valid proof outline.\n\n\n\nWe now work through the Source proof, allowing arbitrary Sink actions to occur between any two Source action. The allowable Sink actions at any point must have the same predicate at the point where the Sink action occurs. For example, the predicate prior to Source inject includes { state = 01 ∧ items = 01 }, which is also a predicate prior to extract. Thus, extract could also occur prior to inject. We can therefore include the predicate resulting from the extract action as part of the predicate prior to inject. Applying allowable Sink actions in this fashion to all of the Source predicates, we obtain:\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } */\n\n        /* extract: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 00 } */\n        /* drain: { state = 01 ∧ items = 00 } → { state = 00 ∧ items = 00 } */\n        /* pull: no change\n\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ ( items = 10 ∨ items = 11 ) } */\n\n        /* extract: { state = 01 ∧ items = 11 } → { state = 01 ∧ items = 10 } */\n        /* drain: { state = 01 ∧ items = 10 } → { state = 00 ∧ items = 10 } */\n        /* pull: no change\n\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ ( items = 10 ∨ items = 11 ) } */\n     fill: state[0] ← 1\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ ( items = 10 ∨ items = 11 ) } */\n\n        /* extract: { state = 11 ∧ items = 11 } → { state = 11 ∧ items = 10 } */\n        /* drain: { state = 11 ∧ items = 10 } → { state = 10 ∧ items = 10 } */\n        /* pull: { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } */\n        /* pull+extract: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 00 } */\n        /* pull+extract+drain: { state = 01 ∧ items = 00 } → { state = 00 ∧ items = 00 } */\n\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } ∨ */\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ ( items = 10 ∨ items = 11 ) } */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } */\n   }\n\n\n\nApplying the same process to the Sink proof outline as we did for the Source proof outline:\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 10 ∧ items = 10 } */\n\n       /* inject: { state = 00 ∧ items = 00 } → { state = 00 ∧ items = 10 }\n       /* inject+fill: { state = 00 ∧ items = 10 } → { state = 10 ∧ items = 10 }\n       /* inject+fill+push: { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 }\n       /* inject+fill+push+inject: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 11 }\n       /* inject+fill+push+inject+fill: { state = 01 ∧ items = 11 } → { state = 11 ∧ items = 11 }\n\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 } ∨  */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) }  */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n\n       /* inject: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 11 } */\n       /* fill: { state = 01 ∧ items = 01 } → { state = 11 ∧ items = 11 } */\n       /* push: no change \n\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 11 ∧ items = 01 ) } */\n\n       /* inject: { state = 01 ∧ items = 00 } → { state = 01 ∧ items = 10 } */\n       /* fill: { state = 01 ∧ items = 11 } → { state = 11 ∧ items = 10 } */\n       /* push: no change \n\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 11 ∧ items = 01 ) } */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 01 ) } */\n\n       /* inject: { state = 00 ∧ items = 00 } → { state = 00 ∧ items = 10 }\n       /* inject+fill: { state = 00 ∧ items = 10 } → { state = 10 ∧ items = 10 }\n       /* inject+fill+push: { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 }\n       /* inject+fill+push+inject: { state = 01 ∧ items = 01 } → { state = 01 ∧ items = 11 }\n       /* inject+fill+push+inject+fill: { state = 01 ∧ items = 11 } → { state = 11 ∧ items = 11 }\n\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 } ∨  */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) }  */\n   }\n\n\n\nSummarizing the proof outlines above (including only the relevant predicates), we obtain the proof outlines below. Note that these are compatible with the previously derived proof outlines, but the predicates exclude certain combinations of states and items. Where helpful, we include these predicates as comments in the dag source code files.\n\n\n   while (not done) {\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */\n     inject: items[0] ← 1\n     /* { state = 00 ∧ items = 10 } ∨ { state = 01 ∧ ( items = 10 ∨ items = 11 ) }   */\n     fill: state[0] ← 1\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) } ∨ */\n     /* { state = 10 ∧ items = 10 } ∨ { state = 11 ∧ ( items = 10 ∨ items = 11 ) }   */\n     push: 〈 await ¬{ state = 11 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 00 ∧ items = 00 } ∨ { state = 01 ∧ ( items = 00 ∨ items = 01 ) }   */     \n   }\n\n\n\n   while (not done) {\n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 } ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n     pull: 〈 await ¬{ state = 00 } :\n              if { state = 10 ∧ items = 10 } → { state = 01 ∧ items = 01 } ⟩\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n     extract: extract: item[1] ← 0\n     /* { state = 01 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 11 ∧ items = 01 ) } */\n     drain: state[1] ← 0  \n     /* { state = 00 ∧ ( items = 00 ∨ items = 10 ) } ∨ { state = 10 ∧ items = 10 } ∨ */\n     /* { state = 01 ∧ ( items = 01 ∨ items = 11 ) } ∨ { state = 11 ∧ items = 11 ) } */\n   }"
  },
  {
    "objectID": "experimental/tiledb/common/dag/doc/glossary.html",
    "href": "experimental/tiledb/common/dag/doc/glossary.html",
    "title": "",
    "section": "",
    "text": "A task graph is a data structure consisting of nodes and edges. Each node applies some specified computation by invoking its contained function. Edges represent dependencies between nodes. A dataflow graph (or a flow graph) passes data between nodes, with the output data of one node supplying data to the input of another node. A dependency graph specifies the dependencies (the ordering of computation) between nodes, but does not pass data between nodes.\nA basic task graph is assumed to be a directed acyclic graph (DAG).\n\n\n\n\n\n\n\n\n\nA port is a binding point between an edge and a node. A port can be either an input or an output, with respect to a given node. That is, a node receives data from its input port(s) and transmits data to its output port(s).\n\n\n\nA node is a locus for computation in the task graph. A node may have multiple input ports and multiple output ports. A node has a contained function. The expected functionality of a node is to apply its contained function to data available on the input port(s) and to return its values to the output port(s).\n\n\n\nA function node is neither a root nor leaf in the task graph. A function node has a contained function. The expected functionality of a function node is to apply its contained function to data available on its input port and to return its values to its output port. The prototype for the contained function is:\nRet fun (const Type& a);\nEquivalently, the function must satisfy the following:\n  requires std::is_invocable_r_v&lt;Ret, Fun, const Type&&gt;;\n(Note that the signature does not need to have const &).\n\n\n\nIn a task graph, a producer node is a root node in the task graph. It has only output ports(s) and zero input ports. A producer node has a contained function.\nThe expected functionality of a producer node is to apply its contained function to an std::stop_source object and to return an output item. The prototype for the contained function is:\nRet fun (std::stop_source stop);\nEquivalently, the function must satisfy the following:\n  requires std::is_invocable_r_v&lt;Ret, Fun, std::stop_source&gt;;\n\n\n\nIn a task graph, a producer node is a leaf node in the task graph. It has only input port(s) and zero output ports. A consumer node has a contained function. The expected functionality of a consumer node is to apply its contained function to an input item, returning void. The prototype for the contained function is:\nvoid fun (const Type& a);\nEquivalently, the function must satisfy the following:\n  requires std::is_invocable_r_v&lt;void, Fun, const Type&&gt;;\n\n\n\n\n\n\nAn edge connects two nodes. An edge has two endpoints, an output port of one node and an input port of another node. Edges are parametric on the number of managed data items (currently only either two or three)."
  },
  {
    "objectID": "experimental/tiledb/common/dag/doc/glossary.html#application-glossary",
    "href": "experimental/tiledb/common/dag/doc/glossary.html#application-glossary",
    "title": "",
    "section": "",
    "text": "A task graph is a data structure consisting of nodes and edges. Each node applies some specified computation by invoking its contained function. Edges represent dependencies between nodes. A dataflow graph (or a flow graph) passes data between nodes, with the output data of one node supplying data to the input of another node. A dependency graph specifies the dependencies (the ordering of computation) between nodes, but does not pass data between nodes.\nA basic task graph is assumed to be a directed acyclic graph (DAG).\n\n\n\n\n\n\n\n\n\nA port is a binding point between an edge and a node. A port can be either an input or an output, with respect to a given node. That is, a node receives data from its input port(s) and transmits data to its output port(s).\n\n\n\nA node is a locus for computation in the task graph. A node may have multiple input ports and multiple output ports. A node has a contained function. The expected functionality of a node is to apply its contained function to data available on the input port(s) and to return its values to the output port(s).\n\n\n\nA function node is neither a root nor leaf in the task graph. A function node has a contained function. The expected functionality of a function node is to apply its contained function to data available on its input port and to return its values to its output port. The prototype for the contained function is:\nRet fun (const Type& a);\nEquivalently, the function must satisfy the following:\n  requires std::is_invocable_r_v&lt;Ret, Fun, const Type&&gt;;\n(Note that the signature does not need to have const &).\n\n\n\nIn a task graph, a producer node is a root node in the task graph. It has only output ports(s) and zero input ports. A producer node has a contained function.\nThe expected functionality of a producer node is to apply its contained function to an std::stop_source object and to return an output item. The prototype for the contained function is:\nRet fun (std::stop_source stop);\nEquivalently, the function must satisfy the following:\n  requires std::is_invocable_r_v&lt;Ret, Fun, std::stop_source&gt;;\n\n\n\nIn a task graph, a producer node is a leaf node in the task graph. It has only input port(s) and zero output ports. A consumer node has a contained function. The expected functionality of a consumer node is to apply its contained function to an input item, returning void. The prototype for the contained function is:\nvoid fun (const Type& a);\nEquivalently, the function must satisfy the following:\n  requires std::is_invocable_r_v&lt;void, Fun, const Type&&gt;;\n\n\n\n\n\n\nAn edge connects two nodes. An edge has two endpoints, an output port of one node and an input port of another node. Edges are parametric on the number of managed data items (currently only either two or three)."
  },
  {
    "objectID": "experimental/tiledb/common/dag/doc/glossary.html#implementation-glossary",
    "href": "experimental/tiledb/common/dag/doc/glossary.html#implementation-glossary",
    "title": "",
    "section": "Implementation Glossary",
    "text": "Implementation Glossary\n\nData Item Type\n\n\nSource\n\n\nSink\n\n\nMover\n\n\nHandles\n\n\nTask\n\n\nScheduler\n\n\nCoroutine\n\n\nDuff’s Device\n\n\nSpecification Graph\n\n\nExecution Graph"
  },
  {
    "objectID": "doc/dev/style/CMake.html",
    "href": "doc/dev/style/CMake.html",
    "title": "CMake",
    "section": "",
    "text": "TileDB uses CMake for its configuration, build system, and package management."
  },
  {
    "objectID": "doc/dev/style/CMake.html#build-options",
    "href": "doc/dev/style/CMake.html#build-options",
    "title": "CMake",
    "section": "Build Options",
    "text": "Build Options\nThe TILEDB_* options are used primarily for enabling/disabling extra components (e.g. S3 support, test suite) and setting build or compilation settings (e.g. error on warn, superbuild). The options are defined in a single code block in the TileDB root CMakeLists.txt file.\nIf it is decided a new option is needed, the following must be updated in addition to the root CMakeLists.txt file:\n\nthe bootstrap scripts (bootstrap, bootstrap.ps1)\ncontinuous integration coverage to ensure the option is exercised in a CI build (edit github actions workflows under .github/workflows)\nthe build documentation contained in our online docs"
  },
  {
    "objectID": "doc/dev/style/CMake.html#tiledb-object-libraries",
    "href": "doc/dev/style/CMake.html#tiledb-object-libraries",
    "title": "CMake",
    "section": "TileDB Object Libraries",
    "text": "TileDB Object Libraries\nThe main TileDB library is in the process of being broken into discrete object libraries.\nTODO About the object libraries and best practices"
  },
  {
    "objectID": "doc/dev/style/CMake.html#tiledb-static-and-shared-libraries",
    "href": "doc/dev/style/CMake.html#tiledb-static-and-shared-libraries",
    "title": "CMake",
    "section": "TileDB Static and Shared Libraries",
    "text": "TileDB Static and Shared Libraries\nTODO About combining the object libraries into the main libraries created by CMake"
  },
  {
    "objectID": "doc/dev/style/CMake.html#adding-a-dependency",
    "href": "doc/dev/style/CMake.html#adding-a-dependency",
    "title": "CMake",
    "section": "Adding a dependency",
    "text": "Adding a dependency\nThere are several steps to adding a new dependency to TileDB. See TileDB wiki on GitHub for more information."
  },
  {
    "objectID": "doc/dev/style/CMake.html#using-the-common-modules",
    "href": "doc/dev/style/CMake.html#using-the-common-modules",
    "title": "CMake",
    "section": "Using the common modules",
    "text": "Using the common modules\n\n_TODO Describe (generally speaking) what is included in the cmake/common*.cmake modules_\nTODO Guidelines for adding to the modules (including best practices for CMake macros and functions)."
  },
  {
    "objectID": "doc/dev/style/ErrorHandling.html",
    "href": "doc/dev/style/ErrorHandling.html",
    "title": "Error handling",
    "section": "",
    "text": "The code began its life with a don’t-throw-exceptions practice. It’s beginning a transition into a do-throw-exceptions practice, although that hasn’t started quite yet."
  },
  {
    "objectID": "doc/dev/style/ErrorHandling.html#all-functions",
    "href": "doc/dev/style/ErrorHandling.html#all-functions",
    "title": "Error handling",
    "section": "All functions",
    "text": "All functions\n\nEvery function should signal an error only when it gives up what it’s trying to do.\n\nThis point may sound obvious, but it’s a bit more subtle than that. The magic word is only. A function that is unable to do something but knows that it can’t could return a bool value of false, for example. A function that can’t tell whether it can do something or not needs to signal an error.\nAnother way of stating this point is that signalling an error isn’t the same thing as failing an operation. If a function only ever succeeds at its operation, then it must signal an error. If a function can both succeed and fail at its operation, then it need not signal an error. Yet it’s also possible for a function to both succeed and fail and also stop working and throw an error.\nIt’s up to the function to determine when it should fail (ordinary, without error) and when it should signal an error (not ordinary). The decision about whether or not to do this is not a matter of style, but of substance.\n\nA function should signal an error only when circumstances are outside its expectations.\n\nSome code might be described as whimsical, unexpectedly not performing the duties they seem to have taken on. (It might be appropriate to refer to these as Bartelby functions, because they would prefer not to perform certain tasks.) This point is that a function should be consistent in its error behavior.\n\nEach function should document its expectations.\n\nA user of a function should have some idea about when it may rely upon the behavior of a function. If the function does not contain documentation about what its expectations are, a user cannot know with precision when it may be relied upon.\nAn expection is somewhat broader than a precondition. All preconditions are expectations, to be sure. Some expectations, though, are not. For example, an expectation about the state of a filesystem might be present. If so, however, it should be documented. There should be no implicit expectations.\n\nSignal an error by returning an error status.\n\nAs of now, this code signals error by returning an object of class Status.\nThis advice is will evolve. The first change will be “Signal an either by returning an error status or throwing”. The second will be “Signal an error by throwing”."
  },
  {
    "objectID": "doc/dev/style/ErrorHandling.html#signalling-an-error-by-returning-an-error-status",
    "href": "doc/dev/style/ErrorHandling.html#signalling-an-error-by-returning-an-error-status",
    "title": "Error handling",
    "section": "Signalling an error by returning an error status",
    "text": "Signalling an error by returning an error status\nThis section applies to functions that might not use a throw statement to cede control to their caller.\n\nEach function that can signal an error without throwing should return a Status object.\n\nThe definition of class Status is in tiledb/common/status.h. Include that header in every compilation unit that defines such functions. If only a declaration is needed, a forward declaration of class Status is acceptable.\n\nUse a tuple return value to return both Status and other values.\n\nA function may return both Status and other return values by returning a tuple. See style guide entry Inputs on the right. Outputs on the left. for more information about how to accomplish this.\n\nReturn Status::Ok() to not signal an error.\n\nThe factory function Status::Ok() returns a value that does not signal anything.\n\nUse a factory function in the vicinity of the error to make Status objects.\n\nA number of factory functions exist to create status messages local to a specific area of the code. A function should use one of these functions to construct Status objects that it returns.\n\nExample\nFor example, code that is checking for an error related to a Context might look like the following:\n#include \"tiledb/common/status.h\"\nusing namespace = tiledb::common;\nbool failure_condition();\n\nStatus foo() {\n  if (failure_condition()) {\n    return Status_ContextError(\"Cannot do requested thing\");\n  }  \n  return Status::Ok(); \n}"
  },
  {
    "objectID": "doc/dev/style/ErrorHandling.html#signalling-an-error-by-throwing",
    "href": "doc/dev/style/ErrorHandling.html#signalling-an-error-by-throwing",
    "title": "Error handling",
    "section": "Signalling an error by throwing",
    "text": "Signalling an error by throwing\nAlthough throwing exceptions is not yet permitted in library code, some of its precepts can already be stated.\n\nSignal an error by throwing an object of class tdb::exception.\n\nThe library has tdb::exception to allow greater expressiveness and precision than what’s afforded in std::exception.\n\nConvert from returning Status st by throwing tdb::exception(st)\n\ntdb::exception has a constructor with a single Status argument. This constructor is present to allow rapid conversion from returning status to throwing."
  },
  {
    "objectID": "doc/dev/style/ErrorHandling.html#style-guide-for-error-messages",
    "href": "doc/dev/style/ErrorHandling.html#style-guide-for-error-messages",
    "title": "Error handling",
    "section": "Style guide for error messages",
    "text": "Style guide for error messages\n\nMake error messages either informative or empty.\n\nWhen someone looks at an exception in practice, the source location and call stack will be available. If what you’re saying doesn’t add to this, don’t say anything else."
  },
  {
    "objectID": "doc/dev/style/index.html",
    "href": "doc/dev/style/index.html",
    "title": "Style Guide",
    "section": "",
    "text": "When in doubt and not in conflict with this style guide, follow C++ Core Guidelines"
  },
  {
    "objectID": "doc/dev/style/index.html#c-standards",
    "href": "doc/dev/style/index.html#c-standards",
    "title": "Style Guide",
    "section": "C++ Standards",
    "text": "C++ Standards\nWhere possible we should use up to C++17. Some external packages, such as PDAL are limited in what they can use. Where we control the environment we should strive for C++17 usage.\n\nTable of Contents\n\nFunctions\n\nWith emphasis: Inputs on the right. Outputs on the left.\n\nMemory handling\nStorage serialization"
  },
  {
    "objectID": "doc/dev/style/index.html#cmake",
    "href": "doc/dev/style/index.html#cmake",
    "title": "Style Guide",
    "section": "CMake",
    "text": "CMake\n\nCMake guide"
  },
  {
    "objectID": "doc/policy/api_changes.html",
    "href": "doc/policy/api_changes.html",
    "title": "Deprecations",
    "section": "",
    "text": "External C and C++ API deprecations shall be announced (via TILEDB_DEPRECATED* annotations) at least two release cycles before removal.\nTimeline: - release X.y: TILEDB_DEPRECATED_EXPORT added to function declaration. - release X.y+1: announce function removal - release X.y+2: remove function\n\n\n\nFunction removal shall be announced one release cycle before removal, following at least one release cycle of deprecation.\nAnnouncements must be included in the release notes for the release preceding the removal.\n\n\n\n2.13: All functions deprecated in TileDB &lt;=2.12 will be removed in TileDB 2.15.\n\n\n\n\n\n\n\n\ntiledb_coords\n\n\n\n\n\ntiledb_array_schema_load_with_key\ntiledb_query_set_buffer\ntiledb_query_set_buffer_var\ntiledb_query_set_buffer_nullable\ntiledb_query_set_buffer_var_nullable\ntiledb_query_get_buffer\ntiledb_query_get_buffer_var\ntiledb_query_get_buffer_nullable\ntiledb_query_get_buffer_var_nullable\ntiledb_array_open_at\ntiledb_array_open_with_key\ntiledb_array_open_at_with_key\ntiledb_array_reopen_at\ntiledb_array_get_timestamp\ntiledb_array_create_with_key\ntiledb_array_consolidate_with_key\ntiledb_fragment_info_load_with_key\n\n\n\n\n\ntiledb_query_set_subarray\ntiledb_query_add_range\ntiledb_query_add_range_by_name\ntiledb_query_add_range_var\ntiledb_query_add_range_var_by_name\ntiledb_query_get_range_num\ntiledb_query_get_range_num_from_name\ntiledb_query_get_range\ntiledb_query_get_range_from_name\ntiledb_query_get_range_var_size\ntiledb_query_get_range_var_size_from_name\ntiledb_query_get_range_var\ntiledb_query_get_range_var_from_name\n\n\n\n\n\ntiledb_group_create\n\n\n\n\n\ntiledb_array_consolidate_metadata\ntiledb_array_consolidate_metadata_with_key\n\n\n\n\n\ntiledb_query_submit_async\n\n\n\n\n\n\n\nDeprecation list was generated using the code below, from the root directory, then hand-edited and verified.\n\nimport Base.+\n+(a::VersionNumber, b::VersionNumber) = VersionNumber(a.major + b.major, a.minor + b.minor, a.patch + b.patch)\n\nversions = [\n  v\"2.2.1\",\n  v\"2.3.0\",\n  v\"2.4.0\",\n  v\"2.5.0\",\n  v\"2.6.0\",\n  v\"2.7.0\",\n  v\"2.8.0\",\n  v\"2.9.0\",\n  v\"2.10.0\",\n  v\"2.11.0\",\n  v\"2.12.0\"\n]\n\ndata = Dict()\n\nfor i in 1:length(versions)-1\n  v = versions[i]\n  v_next = versions[i+1]\n  range = \"$v..$v_next\"\n\n  data[range] = read(pipeline(`git diff $v..$v_next tiledb/sm/c_api/tiledb.h`, Cmd(`grep -A2 DEPRECATE`, ignorestatus=true)), String)\nend\n\nprint(data)\n\nopen(\"deprecations.md\", \"w\") do f\n  for i in 1:length(versions)-1\n    v = versions[i]\n    v_next = versions[i+1]\n    range = \"$v..$v_next\"\n\n    println(f, \"## $range\\n\")\n    write(f, unescape_string(string(data[range])))\n  end\nend"
  },
  {
    "objectID": "doc/policy/api_changes.html#policy",
    "href": "doc/policy/api_changes.html#policy",
    "title": "Deprecations",
    "section": "",
    "text": "External C and C++ API deprecations shall be announced (via TILEDB_DEPRECATED* annotations) at least two release cycles before removal.\nTimeline: - release X.y: TILEDB_DEPRECATED_EXPORT added to function declaration. - release X.y+1: announce function removal - release X.y+2: remove function"
  },
  {
    "objectID": "doc/policy/api_changes.html#removal-announcements",
    "href": "doc/policy/api_changes.html#removal-announcements",
    "title": "Deprecations",
    "section": "",
    "text": "Function removal shall be announced one release cycle before removal, following at least one release cycle of deprecation.\nAnnouncements must be included in the release notes for the release preceding the removal.\n\n\n\n2.13: All functions deprecated in TileDB &lt;=2.12 will be removed in TileDB 2.15."
  },
  {
    "objectID": "doc/policy/api_changes.html#deprecation-version-history",
    "href": "doc/policy/api_changes.html#deprecation-version-history",
    "title": "Deprecations",
    "section": "",
    "text": "tiledb_coords\n\n\n\n\n\ntiledb_array_schema_load_with_key\ntiledb_query_set_buffer\ntiledb_query_set_buffer_var\ntiledb_query_set_buffer_nullable\ntiledb_query_set_buffer_var_nullable\ntiledb_query_get_buffer\ntiledb_query_get_buffer_var\ntiledb_query_get_buffer_nullable\ntiledb_query_get_buffer_var_nullable\ntiledb_array_open_at\ntiledb_array_open_with_key\ntiledb_array_open_at_with_key\ntiledb_array_reopen_at\ntiledb_array_get_timestamp\ntiledb_array_create_with_key\ntiledb_array_consolidate_with_key\ntiledb_fragment_info_load_with_key\n\n\n\n\n\ntiledb_query_set_subarray\ntiledb_query_add_range\ntiledb_query_add_range_by_name\ntiledb_query_add_range_var\ntiledb_query_add_range_var_by_name\ntiledb_query_get_range_num\ntiledb_query_get_range_num_from_name\ntiledb_query_get_range\ntiledb_query_get_range_from_name\ntiledb_query_get_range_var_size\ntiledb_query_get_range_var_size_from_name\ntiledb_query_get_range_var\ntiledb_query_get_range_var_from_name\n\n\n\n\n\ntiledb_group_create\n\n\n\n\n\ntiledb_array_consolidate_metadata\ntiledb_array_consolidate_metadata_with_key\n\n\n\n\n\ntiledb_query_submit_async"
  },
  {
    "objectID": "doc/policy/api_changes.html#deprecation-history-generation",
    "href": "doc/policy/api_changes.html#deprecation-history-generation",
    "title": "Deprecations",
    "section": "",
    "text": "Deprecation list was generated using the code below, from the root directory, then hand-edited and verified.\n\nimport Base.+\n+(a::VersionNumber, b::VersionNumber) = VersionNumber(a.major + b.major, a.minor + b.minor, a.patch + b.patch)\n\nversions = [\n  v\"2.2.1\",\n  v\"2.3.0\",\n  v\"2.4.0\",\n  v\"2.5.0\",\n  v\"2.6.0\",\n  v\"2.7.0\",\n  v\"2.8.0\",\n  v\"2.9.0\",\n  v\"2.10.0\",\n  v\"2.11.0\",\n  v\"2.12.0\"\n]\n\ndata = Dict()\n\nfor i in 1:length(versions)-1\n  v = versions[i]\n  v_next = versions[i+1]\n  range = \"$v..$v_next\"\n\n  data[range] = read(pipeline(`git diff $v..$v_next tiledb/sm/c_api/tiledb.h`, Cmd(`grep -A2 DEPRECATE`, ignorestatus=true)), String)\nend\n\nprint(data)\n\nopen(\"deprecations.md\", \"w\") do f\n  for i in 1:length(versions)-1\n    v = versions[i]\n    v_next = versions[i+1]\n    range = \"$v..$v_next\"\n\n    println(f, \"## $range\\n\")\n    write(f, unescape_string(string(data[range])))\n  end\nend"
  },
  {
    "objectID": "HISTORY.html",
    "href": "HISTORY.html",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "Storage format version is now 20, bringing changes to support enumerated types. (#4051).\n\n\n\n\n\nAdd Enumeration max size configuration. #4308\n\n\n\n\n\nEnumerated data types (aka: “categoricals” in Pandas, “factors” in R). #4051\nSupport optimized creation of set membership (IN, NOT_IN) QueryConditions, replacing create+combine pattern for specifying the target set. #4164\nAdd C and C++ APIs to reflect as-built (build-time) configuration options as a JSON string. #4112, #4199\nFilter pipeline support for datatype conversions based on filtered output datatype. #4165\nAdd TileDB-REST support for vacuum and consolidation requests. #4229\nTileDB-REST support for dimension labels. #4084\n\n\n\n\n\nAdd TileDB-REST serialization for reinterpret_datatype. #4286\nUpdate VFS to call ListObjectsV2 from the AWS SDK, providing significant performance improvements for array opening and other listing-heavy operations. #4216\nWrap KV::Reader::getKey calls into std::string_view. #4186\nUnify ca_file and ca_path configuration parameters. #4087\nInline and optimize some trivial methods of the internal Array class. #4226\nMerge overlapping ranges on sparse reads. #4184\nLegacy reader: reading non written region causes segfault. #4253\nMake generic tile decompression single threaded to fix multi-query hang condition. #4256\nAdd tiledb_handle_load_enumerations_request. #4288\nAdd TileDB-REST array open v2 serialization support for enumerations. #4307\n\n\n\n\n\nDense reader: fix nullable string values when reading with QueryCondition. #4174\nFix segmentation fault in delta compressor test. #4143\nFix VFS is_empty_bucket in C++ API. #4154\nInitialize a Query at the end of deserialization to fix QueryCondition error against TileDB-REST. #4148\nSparse unordered w/ dups reader: adding ignored tiles. #4200\nAllow Subarray::get_est_result_size_nullable on remote arrays. #4202\nFix version check bug when deserializing attributes. #4189\nFix container issues in windows builds. #4177\nUnordered writes: fix deserialization for older clients. #4246\nFix serialization of var-size property for nonempty domains. #4264\nClear pending changes during group delete to avoid updating a deleted group. #4267\nUpdate filter checks for accepted datatypes. #4233\nCheck if Subarray label ranges are set in Query::only_dim_label_query. #4285\nOptimize capnp traversal size usage. #4287\nFix QueryCondition set conditions on attributes with enumerations. #4305\nBackwards compatibility: correctly handle 0 chunk variable-size tiles. #4310\n\n\n\n\n\n\n\nAdd C API attribute handle. #3982\nAdd result_buffer_elements_nullable for variable size dimension labels. #4142\nAdd new C APIs for dimension label REST support. #4072\nAdd tiledb_as_built_dump, to reflect the build-time configuration options as a JSON string. #4199\nAdd tiledb_query_condition_alloc_set_membership for optimized set membership QC creation. #4164\nAdd C API handle for domain. #4053\n\n\n\n\n\nThrow if incorrect type is passed to set_data_type. #4272\nAdd tiledb::AsBuilt::dump class method, to reflect the build-time configuration options as a JSON string. #4199\nAdd QueryCondition::create vectorized signature for set membership QC creation. #4164\n\n\n\n\n\n\nTileDB core library now requires C++20. Note that the public C++ API still only requires C++17. #4034\nFix macOS “experimental-features” build; CMake and CI QOL improvements #4297\nLibxml2 needs to be linked after Azure libraries #4281\nAdds macos experimental build to nightly CI. #4247\nFix static linking from the GitHub Releases package on Windows. #4263\nFix Azure build failures on non-Windows when vcpkg is disabled. #4258\nDo not install static libraries when TILEDB_INSTALL_STATIC_DEPS is not enabled. #4244\nUse tiledb:stdx stop_token and jthread; fix experimental/ build on macOS. #4240\nExport Azure::azure-storage-blobs on install when vcpkg is enabled. #4232\nMove Azure DevOps CI jobs to GitHub Actions. #4198\nBump catch2 to 3.3.2 in vcpkg #4201\nFix static linking to the on Windows. #4190\nFix build with Clang 16 #4206\nRemove a workaround for a no longer applicable GCC/linker script bug. #4067\nBuild curl with ZStandard support on vcpkg when serialization is enabled. #4166\nUse clipp from vcpkg if enabled. #4134\nSupport acquiring libmagic from vcpkg. #4119\nSimplify linking to Win32 libraries. #4121\ncurl version bump to resolve MyTile superbuild failure #4170\nImprove support for multi-config CMake generators. #4147\nSupport new Azure SDK in superbuild #4153\nUpdate libwebp to version 1.3.1. #4211\nFix Ninja builds on Windows #4173\nFix azure static library linking order for superbuild #4171\nFix vcpkg dependencies building only in Release mode. #4159\nFix compile error on macos 13.4.1 with c++20 enabled. #4222\n\n\n\n\n\nAdd tests for TILEDB_NOT. #4169\nFix regression test exit status; fix unhandled throw (expected). #4311\nFix as_built definitions in the C API compilation unit, and add validation test. #4289\nSmoke test: only recreate array when needed. #4262\nArray schema: C.41 changes, test support. #4004\nConditionally disable WhiteboxAllocator tests to fix GCC-13 build. #4291\nSorted reads tests: faster tests. #4278"
  },
  {
    "objectID": "HISTORY.html#disk-format",
    "href": "HISTORY.html#disk-format",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "Storage format version is now 20, bringing changes to support enumerated types. (#4051)."
  },
  {
    "objectID": "HISTORY.html#configuration-changes",
    "href": "HISTORY.html#configuration-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "Add Enumeration max size configuration. #4308"
  },
  {
    "objectID": "HISTORY.html#new-features",
    "href": "HISTORY.html#new-features",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "Enumerated data types (aka: “categoricals” in Pandas, “factors” in R). #4051\nSupport optimized creation of set membership (IN, NOT_IN) QueryConditions, replacing create+combine pattern for specifying the target set. #4164\nAdd C and C++ APIs to reflect as-built (build-time) configuration options as a JSON string. #4112, #4199\nFilter pipeline support for datatype conversions based on filtered output datatype. #4165\nAdd TileDB-REST support for vacuum and consolidation requests. #4229\nTileDB-REST support for dimension labels. #4084"
  },
  {
    "objectID": "HISTORY.html#improvements",
    "href": "HISTORY.html#improvements",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "Add TileDB-REST serialization for reinterpret_datatype. #4286\nUpdate VFS to call ListObjectsV2 from the AWS SDK, providing significant performance improvements for array opening and other listing-heavy operations. #4216\nWrap KV::Reader::getKey calls into std::string_view. #4186\nUnify ca_file and ca_path configuration parameters. #4087\nInline and optimize some trivial methods of the internal Array class. #4226\nMerge overlapping ranges on sparse reads. #4184\nLegacy reader: reading non written region causes segfault. #4253\nMake generic tile decompression single threaded to fix multi-query hang condition. #4256\nAdd tiledb_handle_load_enumerations_request. #4288\nAdd TileDB-REST array open v2 serialization support for enumerations. #4307"
  },
  {
    "objectID": "HISTORY.html#defects-removed",
    "href": "HISTORY.html#defects-removed",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "Dense reader: fix nullable string values when reading with QueryCondition. #4174\nFix segmentation fault in delta compressor test. #4143\nFix VFS is_empty_bucket in C++ API. #4154\nInitialize a Query at the end of deserialization to fix QueryCondition error against TileDB-REST. #4148\nSparse unordered w/ dups reader: adding ignored tiles. #4200\nAllow Subarray::get_est_result_size_nullable on remote arrays. #4202\nFix version check bug when deserializing attributes. #4189\nFix container issues in windows builds. #4177\nUnordered writes: fix deserialization for older clients. #4246\nFix serialization of var-size property for nonempty domains. #4264\nClear pending changes during group delete to avoid updating a deleted group. #4267\nUpdate filter checks for accepted datatypes. #4233\nCheck if Subarray label ranges are set in Query::only_dim_label_query. #4285\nOptimize capnp traversal size usage. #4287\nFix QueryCondition set conditions on attributes with enumerations. #4305\nBackwards compatibility: correctly handle 0 chunk variable-size tiles. #4310"
  },
  {
    "objectID": "HISTORY.html#api-changes",
    "href": "HISTORY.html#api-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "Add C API attribute handle. #3982\nAdd result_buffer_elements_nullable for variable size dimension labels. #4142\nAdd new C APIs for dimension label REST support. #4072\nAdd tiledb_as_built_dump, to reflect the build-time configuration options as a JSON string. #4199\nAdd tiledb_query_condition_alloc_set_membership for optimized set membership QC creation. #4164\nAdd C API handle for domain. #4053\n\n\n\n\n\nThrow if incorrect type is passed to set_data_type. #4272\nAdd tiledb::AsBuilt::dump class method, to reflect the build-time configuration options as a JSON string. #4199\nAdd QueryCondition::create vectorized signature for set membership QC creation. #4164"
  },
  {
    "objectID": "HISTORY.html#build-system-changes",
    "href": "HISTORY.html#build-system-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "TileDB core library now requires C++20. Note that the public C++ API still only requires C++17. #4034\nFix macOS “experimental-features” build; CMake and CI QOL improvements #4297\nLibxml2 needs to be linked after Azure libraries #4281\nAdds macos experimental build to nightly CI. #4247\nFix static linking from the GitHub Releases package on Windows. #4263\nFix Azure build failures on non-Windows when vcpkg is disabled. #4258\nDo not install static libraries when TILEDB_INSTALL_STATIC_DEPS is not enabled. #4244\nUse tiledb:stdx stop_token and jthread; fix experimental/ build on macOS. #4240\nExport Azure::azure-storage-blobs on install when vcpkg is enabled. #4232\nMove Azure DevOps CI jobs to GitHub Actions. #4198\nBump catch2 to 3.3.2 in vcpkg #4201\nFix static linking to the on Windows. #4190\nFix build with Clang 16 #4206\nRemove a workaround for a no longer applicable GCC/linker script bug. #4067\nBuild curl with ZStandard support on vcpkg when serialization is enabled. #4166\nUse clipp from vcpkg if enabled. #4134\nSupport acquiring libmagic from vcpkg. #4119\nSimplify linking to Win32 libraries. #4121\ncurl version bump to resolve MyTile superbuild failure #4170\nImprove support for multi-config CMake generators. #4147\nSupport new Azure SDK in superbuild #4153\nUpdate libwebp to version 1.3.1. #4211\nFix Ninja builds on Windows #4173\nFix azure static library linking order for superbuild #4171\nFix vcpkg dependencies building only in Release mode. #4159\nFix compile error on macos 13.4.1 with c++20 enabled. #4222"
  },
  {
    "objectID": "HISTORY.html#test-changes",
    "href": "HISTORY.html#test-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "",
    "text": "Add tests for TILEDB_NOT. #4169\nFix regression test exit status; fix unhandled throw (expected). #4311\nFix as_built definitions in the C API compilation unit, and add validation test. #4289\nSmoke test: only recreate array when needed. #4262\nArray schema: C.41 changes, test support. #4004\nConditionally disable WhiteboxAllocator tests to fix GCC-13 build. #4291\nSorted reads tests: faster tests. #4278"
  },
  {
    "objectID": "HISTORY.html#disk-format-1",
    "href": "HISTORY.html#disk-format-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\nBump to version 19 (.vac vacuum files now use relative filenames). #4024"
  },
  {
    "objectID": "HISTORY.html#api-changes-1",
    "href": "HISTORY.html#api-changes-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API changes",
    "text": "API changes\n\nC API\n\nC API dimension handle. #3947\nQuery Plan API. #3921\nError out no-op set_config calls, FragmentInfo. #3885\nError out no-op set_config calls, Array. #3892\nError out no-op set_config calls, Group. #3878\nError out no-op set_config calls, Query. #3884\n\n\n\nC++ API\n\nAdd a Group constructor that accepts a config in the C++ API. #4010\nAdd get_option method to the Filter class that returns the option value. #4139"
  },
  {
    "objectID": "HISTORY.html#new-features-1",
    "href": "HISTORY.html#new-features-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nSupport in place update of group members. #3928\nQuery Condition NOT support. #3844\nAdd new Delta Encoder to filters. #3969\nSupport new STSProfileWithWebIdentityCredentialsProvider S3 credential provider for use of WebIdentity S3 credentials. #4137\nEnable query condition on dimensions for dense arrays. #3887"
  },
  {
    "objectID": "HISTORY.html#configuration-changes-1",
    "href": "HISTORY.html#configuration-changes-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Configuration changes",
    "text": "Configuration changes\n\nAdd vfs.azure.max_retries, vfs.azure.retry_delay_ms and vfs.azure.max_retry_delay_ms options to control the retry policy when connecting to Azure. #4058\nRemove the vfs.azure.use_https option; connecting to Azure with HTTP will require specifying the scheme in vfs.azure.blob_endpoint instead. #4058\nRemoved sm.mem.reader.sparse_global_order.ratio_query_condition config option. #3948\n“vfs.s3.no_sign_request” to allow unsigned s3 API calls, useful for anonymous s3 bucket access. #3953"
  },
  {
    "objectID": "HISTORY.html#improvement",
    "href": "HISTORY.html#improvement",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvement",
    "text": "Improvement\n\nFilter pipeline: make chunk range path default and remove other. #3857\nPrevent combining query conditions with dimension label queries. #3901\nAdd attribute data order to attribute capnp. #3915\nDense reader: adjust memory configuration parameters. #3918\nMove array_open_for_writes from StorageManager to Array. #3898\nRefactor the POSIX VFS and improve error messages. #3849\nUpdate ArraySchema Cap’n Proto serialization to include dimension labels. #3924\nRefactor readers to use std::optional&lt;QueryCondition&gt;. #3907\nMove schema loading functions from StorageManager to ArrayDirectory. #3906\nMigrate BufferList to Handle-based API. #3911\nAvoid storing a separate vector of tiles when loading groups. #3975\nSparse unordered w/ dups: overflow fix shouldn’t include empty tile. #3985\nSparse unordered w/ dups: smaller units of work. #3948\nDense reader: read next batch of tiles as other get processed. #3965\nSparse global order reader: defer tile deletion until end of merge. #4014\nSilence warnings about unqualified calls to std::move on Apple Clang. #4023\nEnable serialization for partial attribute writes. #4021\nFix vac files to include relative file names. #4024\nUpdate Subarray Cap’n Proto serialization to include label ranges. #3961\nFix uses of noexcept in the C API code. #4040\nSparse readers: fixing null count on incomplete queries. #4037\nSparse readers: adding memory budget class. #4042\nUse the new Azure SDK for C++. #3910\n[superbuild] Set Curl CA path on Darwin. #4059\nEnable support for estimating result size on nullable, remote arrays. #4079\nFix intermittent failure in Group Metadata, delete. #4097\nAdd Subarray::add_range to core stats. #4045\nQuery.cc: Fix function ordering. #4129\nImprovements to the Azure VFS. #4058\nArray directory: parallelize large URI vector processing. #4133\nRemove all TileDB artifacts when performing Array and Group deletes. #4081"
  },
  {
    "objectID": "HISTORY.html#defects-removed-1",
    "href": "HISTORY.html#defects-removed-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nSparse global order reader: tile size error includes hilbert buffer. #3956\nSparse unordered w/ dups: fix error on double var size overflow. #3963\nDense reader: fix copies for schema evolution. #3970\nSparse global order reader: fix read progress update for duplicates. #3937\nFix check during remote unordered write finalization. #3903\nAvoid narrowing conversion warning by downcasting computed values. #3931\nFix support for empty strings for Dictionary and RLE encodings. #3938\nCheck for failed allocation in TileBase constructor and throw if so. #3955\nFix logger initialization #3962\nSparse global order reader: deletes and overflow can give wrong results. #3983\nFix buffer size check in sparse unordered with duplicates reader during partial reads. #4027\nInitialize MemFS in the VFS. #3438\nFix array/group metadata deletion when they are inserted in the same operation. #4022\nFix Query Conditions for Boolean Attributes. #4046\nAllow empty AWS credentials for reading public S3 data. #4064\nSerialize coalesce ranges for Subarray. #4043\nUse std::map::insert_or_assign to replace values in maps. #4054\nAdds serialization for webp filter options. #4085\nSubarray serialize coalesce ranges as true by default. #4090\nDon’t throw an exception from the Group destructor. #4103\nFix type and to be && in experimental group C++ API. #4106\nFix dense reader error when query conditions reference var sized fields. #4108\nRemove TILEDB_EXPORT from private C API functions. #4120\nFilter out empty s3 subfolders #4102\nFix REST Client buffer management #4135\nAdds checks to ensure subarray is within domain bounds during initialization of DenseReader. #4088\nFix C++ Filter::set_option and Filter::get_option methods to accept tiledb_datatype_t as a valid option type. #4139"
  },
  {
    "objectID": "HISTORY.html#build-system-changes-1",
    "href": "HISTORY.html#build-system-changes-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Build system changes",
    "text": "Build system changes\n\nAdd support for building dependencies with vcpkg (#3920, #3986, #3960, #3950, #4015, #4128, #4093, #4055)\nQuiet catch2 super build by only printing on failure. #3926\nAdd .rc file containing version info to windows build. #3897\nUse sccache in CI. #4016\nFix building aws-sdk-cpp on Ubuntu 22.04. #4092"
  },
  {
    "objectID": "HISTORY.html#test-changes-1",
    "href": "HISTORY.html#test-changes-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Test changes",
    "text": "Test changes\n\nUse storage-testbench as the GCS emulator. #4132\nAdd unit tests for tiledb/common/platform.h #3936\nAdjust a config buffer parameter for two tests for success in both 32bit and 64bit environments #3954\nAdd TEST_CASE blocks that check for possible regression in sparse global order reader #3933\nUse temporary directories from the OS in tests. #3935\nAdds workflow to trigger REST CI. #4123"
  },
  {
    "objectID": "HISTORY.html#c-library-changes",
    "href": "HISTORY.html#c-library-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "C++ library changes",
    "text": "C++ library changes\n\nIntroduce synchronized_optional, a multiprocessing-compatible version of std::optional #3457\nTask graph filtering experimental example #3894"
  },
  {
    "objectID": "HISTORY.html#defects-removed-2",
    "href": "HISTORY.html#defects-removed-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nFix dense reader error when query conditions reference var sized fields. #4108\nSparse readers: fixing null count on incomplete queries. #4037\nSparse unordered w/ dups: Fix incomplete queries w/ overlapping ranges. #4027"
  },
  {
    "objectID": "HISTORY.html#improvements-1",
    "href": "HISTORY.html#improvements-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nEnable support for estimating result size on nullable, remote arrays #4079"
  },
  {
    "objectID": "HISTORY.html#defects-removed-3",
    "href": "HISTORY.html#defects-removed-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nAdds serialization for webp filter options #4085\nSerialize coalesce ranges for Subarray #4043"
  },
  {
    "objectID": "HISTORY.html#defects-removed-4",
    "href": "HISTORY.html#defects-removed-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nAllow empty AWS credentials for anonymous access. #4064\nFix Query Conditions for Boolean Attributes. #4046"
  },
  {
    "objectID": "HISTORY.html#build",
    "href": "HISTORY.html#build",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Build",
    "text": "Build\n\nSet Curl CA path on Darwin #4059\nFix invalid CMake syntax due to empty variable #4026"
  },
  {
    "objectID": "HISTORY.html#new-features-2",
    "href": "HISTORY.html#new-features-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nIntroduce “vfs.s3.no_sign_request” to allow unsigned s3 API calls, useful for anonymous s3 bucket access. #3953"
  },
  {
    "objectID": "HISTORY.html#defects-removed-5",
    "href": "HISTORY.html#defects-removed-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nFix support for empty strings for Dictionary and RLE encodings #3938\nSparse global order reader: fix read progress update for duplicates. #3937\nSparse unordered w/ dups: fix error on double var size overflow. #3963\nDense reader: fix copies for schema evolution. #3970\nSparse unordered w/ dups: overflow fix shouldn’t include empty tile. #3985"
  },
  {
    "objectID": "HISTORY.html#api-changes-2",
    "href": "HISTORY.html#api-changes-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API changes",
    "text": "API changes\n\nC++ API\n\nAdd a Group constructor that accepts a Config in the C++ API. #4011"
  },
  {
    "objectID": "HISTORY.html#disk-format-2",
    "href": "HISTORY.html#disk-format-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nAdd FloatScaleFilter to format spec (documentation only; filter released in 2.11.0) #3494"
  },
  {
    "objectID": "HISTORY.html#breaking-c-api-changes",
    "href": "HISTORY.html#breaking-c-api-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking C API changes",
    "text": "Breaking C API changes\n\nRemove deprecated Query methods. #3841\nRemove deprecated API functions about buffers #3733\nRemove deprecated C API tiledb_fragment_info_load_with_key #3740\nRemove extra experimental set buffer methods #3761\nRemove C API functions tiledb_array_open_at and variations. #3755\nRemove C API functions tiledb_array_consolidate_metadata and tiledb_array_consolidate_metadata_with_key #3742\nRemove deprecated C API function tiledb_coords. #3743\nRemove bitsort filter (feature-flagged filter). #3852"
  },
  {
    "objectID": "HISTORY.html#breaking-behavior",
    "href": "HISTORY.html#breaking-behavior",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior\n\nAn error is now raised when setting a subarray on a query that is already initialized. #3668\nAn error is now raised when setting a subarray on a write to a sparse array. #3668"
  },
  {
    "objectID": "HISTORY.html#new-features-3",
    "href": "HISTORY.html#new-features-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nPartial attribute writes into a single fragment. #3714\nQuery condition support for TIME types #3784\nEnable dimension labels for the C API (provisional) #3824\nAdd dimension label C++ API (provisional) #3839"
  },
  {
    "objectID": "HISTORY.html#improvements-2",
    "href": "HISTORY.html#improvements-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdd creation of ArrayDirectory on open to stats #3769\nReturn underlying errors in Win::remove_dir #3866\nRemoves tile-aligned restriction for remote global order writes by caching tile overflow data from submissions. #3762\nImplement S3 buffering support for remote global order writes #3609\nAdd support for dimension labels on an encrypted array #3774\nRefactor relevant fragments into a separate class. #3738\nImplement duration instrument for measuring times in stats. #3746\nMove attribute order check to writer base #3748\nFragment consolidator: use average cell size for buffer allocation. #3756\nShow URL in logger trace before results are returned #3745\nRefactor tiledb_set_subarray call in test/support/src/helpers.cc #3776\nMove array open methods from StorageManager to Array #3790\nSplit Tile class into different classes for read and write. #3796\nUpdate ArrayDirectory to use ContextResources #3800\nUse EncryptionType::NO_ENCRYPTION instead of casting the C enum. #3545\nTile metadata generator: fix buffer overflow on string comparison. #3821\nOrdered writer: process next tile batch while waiting on write. #3797\nQuery v3: Reduce array open operations on Cloud #3626\nRead tiles: get rid of extra VFS allocation. #3848\nClean-up QueryBuffer and move it away from statuses. #3840\nStorage manager: remove passthrough read/write functions. #3853\nFix partial attribute write test. #3862\nQuery v2: Add array directory and fragment meta ser/deser behind config flag #3845\nOrdered dimension label reader: handle empty array. #3869\nFix comments in nullable attributes example. #3870\nRLE and dictionary filter only enabled for UTF8 since format version 17. #3868\nMove more resources into ContextResources #3807\nBetter errors for failed dimension label queries. #3872\nDense reader: process smaller units of work. #3856\nUse different config variable for enabling open v2 and query v3 #3879\nFragment consolidation: using correct buffer weights. #3877\nShow expected and actual version numbers in the error message #3855\nFix shallow test-only bug in unit_rtree.cc #3830\nFix stack buffer overflow bug in unit-curl.cc #3832\nFix stack use after free bug in unit_thread_pool #3831"
  },
  {
    "objectID": "HISTORY.html#defects-removed-6",
    "href": "HISTORY.html#defects-removed-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nFail if fragment info objects are accessed before loading them. #3846\nLet ConsolidationPlan::dump() produce valid JSON #3751\nFix exceptions thrown during array schema validation for WebP filter. #3752\nModify GlobalStats::reset() to support compensation for registered stats being effectively leaked. #3723\nAvoid pwrite bug on macOS Ventura 13.0 on Apple M1 #3799\nFix ASAN-detected UAF in sparse global order reader #3822\nImprove WebP validation, fixes SC-24766, SC-24759 #3819\nDeregister a remote array from the Consistency multimap before reopening. #3859\nFix buffer size error in deserializing query #3851\nFix segfault in unit_consistency #3880\nAvoid segfault in unit_array #3883\nVFS CAPIHandle class #3523"
  },
  {
    "objectID": "HISTORY.html#api-changes-3",
    "href": "HISTORY.html#api-changes-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API changes",
    "text": "API changes\n\nC API\n\nDeprecate tiledb_query_submit_async #3802\nDeprecate tiledb_fragment_info_get_name. #3791\nDeprecate tiledb_query_submit_async. #3802\nAdd tiledb_array_delete_fragments_list API #3798\nAdd tiledb_string_handle_t so that C API functions can output strings with independent lifespans. #3792\nAdd tiledb_dimension_label_handle_t and tiledb_dimension_label_t handles to C-API #3820\nAdd tiledb_subarray_has_label_ranges and tiledb_subarray_get_label_name. #3858\nAdd tiledb_fragment_info_get_fragment_name_v2. #3842\nAdd function to access the dimension label attribute name. #3867\nChange query set buffer methods to set label buffers in experimental builds #3761\n\n\n\nC++ API\n\nMove deprecated constructors into new array_deprecated.h file, manipulating them to use the new TemporalPolicy and EncryptionAlgorithm classes. #3854\nAdd function to access the dimension label attribute name. #3867\nAdd experimental set_data_buffer API that handles dimension labels #3882"
  },
  {
    "objectID": "HISTORY.html#build-system-changes-2",
    "href": "HISTORY.html#build-system-changes-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Build system changes",
    "text": "Build system changes\n\nSet ASAN options for TileDB #3843\nAllow building TileDB with GNU GCC on MacOS #3779"
  },
  {
    "objectID": "HISTORY.html#experimental-features",
    "href": "HISTORY.html#experimental-features",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Experimental features",
    "text": "Experimental features\n\nSet of three exploratory but fully consistent schedulers for TileDB task graph library. #3683\nVersion 0.1 of specification task graph. #3754"
  },
  {
    "objectID": "HISTORY.html#full-changelog",
    "href": "HISTORY.html#full-changelog",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Full Changelog:",
    "text": "Full Changelog:\n\nhttps://github.com/TileDB-Inc/TileDB/compare/2.14.0…2.15.0"
  },
  {
    "objectID": "HISTORY.html#announcements",
    "href": "HISTORY.html#announcements",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Announcements",
    "text": "Announcements\n\nTileDB 2.17, targeted for release in April 2023, will require C++20 support for full library compilation. C++17 compatibility will be maintained in the public C++ API for several releases beyond 2.17."
  },
  {
    "objectID": "HISTORY.html#disk-format-3",
    "href": "HISTORY.html#disk-format-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nFormat version updated to 17. #3611"
  },
  {
    "objectID": "HISTORY.html#breaking-api-changes",
    "href": "HISTORY.html#breaking-api-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking API changes",
    "text": "Breaking API changes\n\nIn TileDB 2.15, we will begin removing C and C++ API functions deprecated through TileDB 2.12. See list of deprecated functions."
  },
  {
    "objectID": "HISTORY.html#breaking-behavior-1",
    "href": "HISTORY.html#breaking-behavior-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior\n\nFor global order reads, an error is now raised if multiple ranges are set when the query is submitted. Previously, an error was raised immediately upon adding another range during subarray creation. #3664\nFor dense writes, an error is now raised if multiple ranges are set when the query is submitted. Previously, an error was raised immediately upon adding another range during subarray creation. #3664\nFor sparse writes, an error is now raised if the subarray is not default when the query is submitted. Previously, an error was raised when adding ranges to a subarray for a sparse write. #3664\nTileDB will not longer override the AWS_DEFAULT_REGION or AWS_REGION environment variables, if set in the process environment. Previously, the library would unconditionally default to setting the region to “us-east-1” unless the vfs.s3.region config variable was specified; now the library will check for the mentioned environment variables, and avoid setting a default if present. The vfs.s3.region config variable will still take precedence if provided #3788"
  },
  {
    "objectID": "HISTORY.html#new-features-4",
    "href": "HISTORY.html#new-features-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdd UTF-8 attribute binary comparison support for QueryConditions. #3766\nAdd Boolean Support For Sparse Query Conditions. #3651\nConsolidation plan API. #3647"
  },
  {
    "objectID": "HISTORY.html#improvements-3",
    "href": "HISTORY.html#improvements-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nDefects Removed\n\nSparse global order reader: fix tile cleanup when ending an iteration. #3674\nDense array: Tile var size metadata not loaded on read. #3645\nFix parsing of multiple enable= arguments to bootstrap script. #3650\nFix bad write by label error. #3664\nFix build issue with consolidation plan. #3704\nFix macOS versions before 10.15 do not support &lt;filesystem&gt;. #3729\nSegfault reading array with query condition after schema evolution. #3732\nQuery condition: fix when attribute condition is not in user buffers. #3713\nFragment consolidator: stop consolidation if no progress can be made. #3671\nThrow error when memory budget leads to splitting to unary range. #3715\nDo not resubmit http requests for curl errors for REST requests #3712\n\n\n\nInternal\n\nValidation for scale and offsets input to the float scale filter. #3726\nAdding attribute ordering checks to ordered dimension label reader. #3643\nSparse globalorder reader: prevent unused tiles from being loaded again. #3710\nArray directory serialization. #3543\nGlobal order writes serialization tests: fixing incorrect section usage. #3670\nUpdate dimension labels to use an array instead of group. #3667\nConsolidation plan: implementation for creating a plan using MBRs. #3696\nSparse unordered w/dups reader: allow partial tile offsets loading. #3716\nImplement static thread pool scheduler using “throw-catch” state transitions, as well as segmented nodes. #3638\nRemove open_arrays_ from StorageManager. #3649\nReplaces domain_str with range_str. #3640\nReplace use of Buffer with Serializer/Deserializer FragmentMetadata footer methods. #3551\nRemove non owning tile constructor. #3661\nAdd CMake modules for compactly specifying object libraries. #3548\nCMake environment unit_test. #3703\nAdd Attribute data order constructor and checks. #3662\nAdding size computation for tile offsets to sparse readers. #3669\nExtract CA certificate discovery from GlobalState. #3717\nExtract libcurl initialization from GlobalState. #3718\nRemove StorageManager::array_close_for_$type methods. #3658\nFix build on upcoming gcc-13. #3722\nAdds nightly C++20 builds alongside C++17 nightlies. #3688\nAdding average cell size API to array. #3700"
  },
  {
    "objectID": "HISTORY.html#api-changes-4",
    "href": "HISTORY.html#api-changes-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API Changes",
    "text": "API Changes\n\nC API\n\nDeprecate tiledb_array_delete_array add tiledb_array_delete. #3744\nAdded tiledb_group_delete_group API. #3560\nDocument tiledb_mime_type_t. #3625\nFix missing argument in set dimension label tile C-API. #3739"
  },
  {
    "objectID": "HISTORY.html#full-changelog-1",
    "href": "HISTORY.html#full-changelog-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Full Changelog:",
    "text": "Full Changelog:\n\nhttps://github.com/TileDB-Inc/TileDB/compare/2.13.0…2.14.0"
  },
  {
    "objectID": "HISTORY.html#bug-fixes",
    "href": "HISTORY.html#bug-fixes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nEnsures coordinate tiles are initialized for multipart remote queries #3795"
  },
  {
    "objectID": "HISTORY.html#improvements-4",
    "href": "HISTORY.html#improvements-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nSparse globalorder reader: prevent unused tiles from being loaded again. #3710\nDo not resubmit http requests for curl errors for REST requests #3712\nEnables WebP by default #3724"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-1",
    "href": "HISTORY.html#bug-fixes-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nQuery condition: fix when attribute condition is not in user buffers. #3713"
  },
  {
    "objectID": "HISTORY.html#disk-format-4",
    "href": "HISTORY.html#disk-format-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nDocumentation\n\nDocument dictionary encoding format #3566\nImprove documentation of consolidated commits files and ignore files #3606"
  },
  {
    "objectID": "HISTORY.html#new-features-5",
    "href": "HISTORY.html#new-features-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nBitsort filter, to write attribute and associated coordinates in the attribute’s data order, and re-sort into global order on read #3483, #3570\nSupport for lossless and lossy RGB(A) and BGR(A) image compression using WebP #3549"
  },
  {
    "objectID": "HISTORY.html#api-changes-5",
    "href": "HISTORY.html#api-changes-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API Changes",
    "text": "API Changes\n\nC API\n\nAdded tiledb_status_code #3580\nAdded tiledb_group_get_is_relative_uri_by_name #3550\nAdded tiledb_array_delete_array API #3539\n\n\n\nDocumentation\n\nDocument missing fragment info C APIs. #3577\nDocument that the callback of tiledb_query_submit_async is executed in an internal thread pool thread. #3558"
  },
  {
    "objectID": "HISTORY.html#improvements-5",
    "href": "HISTORY.html#improvements-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nDefects removed\n\nFix for dense arrays: var size metadata not loaded on read [#3645]https://github.com/TileDB-Inc/TileDB/pull/3645)\nDense consolidation: set correct non-empty domain #3635\nSparse global order reader: fixing incomplete reason for rest queries #3620\nFixes dim label reader range order to always take valid ranges #3561\nGlobal order writes should send relative uris over serialization #3557\nFix unit-cppapi-update-queries test failure when throwing UpdateValue exception. #3578\nFix duplicate logger instantiations in global state #3591\nAdd missing query_type in array_open capnp #3616\nFix use-after-free on a capnp::FlatArrayMessageReader #3631\nBitsort filter: preallocate filtered buffer for dim tiles. #3632\nAdd Boolean Support For Sparse Query Conditions #3651\n\n\n\nInternal\n\nRead tiles: refactor tile creation code. #3492\nUse snprintf to avoid deprecation warnings as errors #3608\nRemoves non-C.41 range setting (part 1) #3598\nAdd [[nodiscard]] attribute to Status class and fix failures #3559\nRemove WhiteboxTile. #3612\nGlobal order writer: allow splitting fragments. #3605\nMove unit uuid to unit tests. #3614\nConsolidation: add fragment max size. #3607\nMove unit-bytevecvalue, unit-TileDomain and unit-domain to unit test. #3615\nAdding rtree object library. #3613\nGlobal order writes serialization: no tests when serialization disabled. #3603\nFragment info serialization support #3530\nArray Metadata related refactoring, use Serializer/Deserializer instead of Buffer #3544\nClass VFS C.41 compliance, Part 1 #3477\nUpdates: adding strategy. #3513\nRemove stale declarations from query #3565\nImprovements in C API implementation #3524\nSparse readers: adding relevant cells stats. #3593\nFix find_heap_api_violations.py #3634\n\n\n\nBuild system changes\n\nUpdate to catch2 ver3 #3504\nLinux release artifacts now target manylinux2014 #3656"
  },
  {
    "objectID": "HISTORY.html#full-changelog-2",
    "href": "HISTORY.html#full-changelog-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Full Changelog:",
    "text": "Full Changelog:\n\nhttps://github.com/TileDB-Inc/TileDB/compare/2.12.0…2.13.0"
  },
  {
    "objectID": "HISTORY.html#disk-format-5",
    "href": "HISTORY.html#disk-format-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nAdded Delete commit file to format specification.\nAdded XOR filter type #3383."
  },
  {
    "objectID": "HISTORY.html#new-features-6",
    "href": "HISTORY.html#new-features-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nSupport for DELETE query type, providing the capability to non-destructively (until consolidation with purge) delete data from array from query timestamp forward\n\n\nDELETE feature pull-requests\n\n\nDeletes: legacy reader process deletes. #3387\nDeletes: implement delete strategy. #3337\nDeletes: refactored readers process deletes. #3374\nDeletes: adding support for commits consolidation. #3378\nDense reader: adding num tiles to stats. #3434\nOpt-in core-to-REST-server instrumentation #3432\nDeletes: implement consolidation. #3402\nDeletes: adding examples. #3437\nDeletes: implement serialization. #3450\nDeletes consolidation: switch from marker hashes to condition indexes. #3451\nDeletes: adding purge option for consolidation. #3458\nDeletes: disallow in middle of consolidated fragment with no timestamps. #3470\n\n\nImplement delete_fragments API for removing all fragments within a specified time range #3400\nImplement XOR Filter #3383\nFragment info serialization support for tiledb:// URIs #3530\nAdd support for global order writes to tiledb:// URIs #3393"
  },
  {
    "objectID": "HISTORY.html#api-changes-6",
    "href": "HISTORY.html#api-changes-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API Changes",
    "text": "API Changes\n\nConfig parameters\n\nAdd new config rest.curl.buffersize for setting CURLOPT_BUFFERSIZE. #3440\n\n\n\nC API\n\nAdd tiledb_group_get_is_relative_uri_by_name #3550\nAdding experimental API for getting relevant fragments, tiledb_query_get_relevant_fragment_num. #3413\nAdd tiledb_query_get_relevant_fragment_num for experimental API to get relevant fragments. #3413\nAdd include policy for non-TileDB headers in the C API #3414"
  },
  {
    "objectID": "HISTORY.html#improvements-6",
    "href": "HISTORY.html#improvements-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nPerformance\n\nSparse global order reader: merge algorithm optimization. #3331\nAzure: parallelize remove_dir. #3357\nSparse refactored readers, mark empty fragments as fully loaded early. #3394\nReduce the number of requests in dir_size #3382\nVFS: Adding option to disable batching for read_tiles. #3421\nAvoid duplicate string_view creation in compute_results_count_sparse_string_range #3491\nMemory tracker: using the correct type for setting default budget. #3509\nSparse global order reader: compute hilbert vals before filtering tiles. #3497\nAvoid string copy in Dictionary Encoding decompression. #3490\n\n\n\nDefects removed\n\nAdd check that Dict/RLE for strings is the first filter in the pipeline; allow use w/ other filters #3510\nRemove stale declarations from query #3565\nFix SC-19287: segfault due to deref nonexistent filestore key #3359\nDemonstrating mingw handle leakage (tiledb_unit extract) #3362\nFix and regression test for SC-19240 #3360\nDon’t try to consolidate empty array; fixes SC-19516 #3389\nFixes check for experimental schema features to be current version #3404\nPrevent possible compiler dependent errors serializing groups #3399\nuse stoul() to correctly parse (32bit unsigned values) experimental version numbers cross-platform #3410\nFix deserialize to set array_schema_all_ into array object #3363\n#3430 #3431\nRemoves unneeded fabs causing warning on clang #3484\nDictionary encoding should handle zero length strings #3493\nFix empty metadata after array open/query submit #3495\nHandle filter_from_capnp FilterType::NONE case #3516\nSparse global order reader: incomplete reads when hitting memory limits. #3518\nFixes silent failure for mismatched layout and bad layout/array combos on query #3521\nCorrect defect in source of keying material #3529\nRework delete_fragments API #3505\nFix segfault after schema evolution when reading using TILEDB_UNORDERED #3528\nFix SC-21741, array evolve via REST #3532\nDon’t fetch Array data in Controller until the Array is fully open #3538\nDo not allow creation of sparse array with zero capacity. #3546\nTile metadata: fixing for ordered writes. #3527\n\n\n\nInternal\n\nArray consistency controller #3130\nExperimental build format versioning #3364\nImplementation of DataBlock, DataBlock allocator, join view, and updates to Source and Sink. #3366\nImplemented basic platform library #3420\nAdds Edge and simple Node classes to the TileDB task graph library. #3453\nAdds attribute ranges to Subarray for internal usage #3520\nAdd list of point ranges to Subarray #3502\nAdd support for new array open REST call #3339\nAdded Config::must_find marker for use with new Config::get signature. Throws Status_ConfigError if value cannot be found. #3482\n\n\n\nBuild system changes\n\nAdd abseil/absl to build via ExternalProject_Add #3454\nAdd Crc32c to tiledb build via ExternalProject_Add #3455\nEnable superbuild libcurl to support zstd #3469\nAdjust example dockerfile so layers can be cached better #3488"
  },
  {
    "objectID": "HISTORY.html#full-changelog-3",
    "href": "HISTORY.html#full-changelog-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Full Changelog:",
    "text": "Full Changelog:\n\nhttps://github.com/TileDB-Inc/TileDB/compare/2.11.0…2.12.0"
  },
  {
    "objectID": "HISTORY.html#improvements-7",
    "href": "HISTORY.html#improvements-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nBackport changes from #3326 to add experimental docs to stable #3526"
  },
  {
    "objectID": "HISTORY.html#defects-removed-10",
    "href": "HISTORY.html#defects-removed-10",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nMemory tracker: using the correct type for setting default budget. #3509\nSparse global order reader: incomplete reads when hitting memory limits. #3518\nFix segfault after schema evolution when reading using TILEDB_UNORDERED #3528\nSparse GO reader: issue when no tile progress because user buffers full. #3531"
  },
  {
    "objectID": "HISTORY.html#improvements-8",
    "href": "HISTORY.html#improvements-8",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nBuild\n\nAdjust example dockerfile so layers can be cached better #3488"
  },
  {
    "objectID": "HISTORY.html#defects-removed-11",
    "href": "HISTORY.html#defects-removed-11",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nDictionary encoding should handle zero length strings #3493\nFix empty metadata after array open/query submit #3495\nSparse global order reader: compute hilbert vals before filtering tiles. #3497"
  },
  {
    "objectID": "HISTORY.html#improvements-9",
    "href": "HISTORY.html#improvements-9",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nPerformance\n\nAdd support for new array open REST call #3339\nFix reader serialization for old clients. #3446\nSparse global order reader: merge algorithm optimization. #3331\n\n\n\nInternal\n\nDense reader: adding num tiles to stats. #3434\nOpt-in core-to-REST-server instrumentation #3432\nSerialization: improvements around query serialization/deserialization. #3379"
  },
  {
    "objectID": "HISTORY.html#defects-removed-12",
    "href": "HISTORY.html#defects-removed-12",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\n#3430 #3431\nFix deserialize to set array_schema_all_ into array object #3363"
  },
  {
    "objectID": "HISTORY.html#disk-format-6",
    "href": "HISTORY.html#disk-format-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nBump format version and remove config option for consolidation with timestamps #3267"
  },
  {
    "objectID": "HISTORY.html#breaking-c-api-changes-1",
    "href": "HISTORY.html#breaking-c-api-changes-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking C API changes",
    "text": "Breaking C API changes\n\ntiledb_filter_alloc no longer returns TILEDB_OK when passed a nullptr as the third argument. #3222"
  },
  {
    "objectID": "HISTORY.html#breaking-behavior-2",
    "href": "HISTORY.html#breaking-behavior-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior\n\nEnforce version upper bound for reads, #3248. TileDB will no longer attempt to read an array written with a newer library version. This has never been officially supported, and in practice it usually leads to confusing errors due to format changes. Backward read-compatibility is unaffected, and is tested for each release."
  },
  {
    "objectID": "HISTORY.html#potential-change",
    "href": "HISTORY.html#potential-change",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Potential change",
    "text": "Potential change\n\nIf you have a use-case with tiles larger than 64 MB (in-memory), please get in touch. In order to facilitate improved memory usage and i/o processing performance, we are considering a default upper-bound on tile size in a future version."
  },
  {
    "objectID": "HISTORY.html#new-features-7",
    "href": "HISTORY.html#new-features-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nFloating point scaling filter (#3243, #3083)\n\nAPI Note: input parameters and compatibility with input data are currently unvalidated. This filter requires care in the selection of scale and offset factors compatible with the input data range.\n\nSupport for overlapping ranges in QueryCondition. #3264\nEnable query condition on dimensions for sparse arrays. #3302"
  },
  {
    "objectID": "HISTORY.html#improvements-10",
    "href": "HISTORY.html#improvements-10",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nPerformance\n\nAWS/GCS: parallelize remove_dir. #3338\nOptimize compute_results_count_sparse_string. #3263\nUse sparse global ordered reader for unordered queries with no dups. #3207\ncompute_results_count_sparse_string: using cached ranges properly. #3314\nGCS/AWS: remove unnecessary classA operations. #3323\nReduce the number of requests in dir_size #3382\n\n\n\nInternal\n\nAdd DataBlocks, port finite state machine, and other DAG infrastructure #3328\nStorage manager: exposing methods to load/store generic tile. #3325\nReplace unnecessary uses of std::unique_lock and std::scope_lock with std::lock_guard. #3340\nDeletes: implement negate for query condition. #3299\nDeletes: adding configuration parameter for purging deleted cells. #3334"
  },
  {
    "objectID": "HISTORY.html#defects-removed-13",
    "href": "HISTORY.html#defects-removed-13",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Defects removed",
    "text": "Defects removed\n\nSparse refactored readers, mark empty fragments as fully loaded early. #3394\nFix printing of TILEDB_BOOL attributes in Attribute::Dump. #3251\nStore compression filter’s version as uint32. #3341\nSparse global order reader: consider qc results after deduplication. #3350\nSerialization: using same functions to choose strategy as in query. #3352\nFix error reporting in tiledb_subarray_alloc. #3220\nFix printing of TILEDB_BLOB attributes in Attribute::Dump. #3250\nAdd missing filters to switch case for Filter serialization. #3256\nFix timestamp_now_ms() on 32-bit Windows. #3292\nAvoid incorrect use of deflateEnd on init errors. #3007\nDatatype for domain must be serialized for backwards client compatibility. #3343\nFix issue with sparse unordered without duplicates query deserialization to use Indexed Reader. #3347\nBug Fix: Wrong results when using OR condition with nullable attributes. #3308\nFix SC-19287: segfault due to deref nonexistent filestore key. #3359"
  },
  {
    "objectID": "HISTORY.html#api-additions",
    "href": "HISTORY.html#api-additions",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nNew (experimental): tiledb_fragment_info_get_total_cell_num #3234\nNew (experimental): tiledb_query_get_relevant_fragment_num #3413\nRemove incorrect noexcept annotations from C API implementations in filestore API. #3273"
  },
  {
    "objectID": "HISTORY.html#test-only-changes",
    "href": "HISTORY.html#test-only-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Test only changes",
    "text": "Test only changes\n\nAdding tests to count VFS calls on array open. #3358\nPrint seed in unit_thread_pool on failure. #3355"
  },
  {
    "objectID": "HISTORY.html#build-system-changes-5",
    "href": "HISTORY.html#build-system-changes-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Build system changes",
    "text": "Build system changes\n\nProduce a TileDBConfigVersion.cmake file. #3240\nAdd nlohmann/json.hpp to superbuild. #3279\nUpdate pkg-config private requirements on Windows only. #3330"
  },
  {
    "objectID": "HISTORY.html#full-changelog-4",
    "href": "HISTORY.html#full-changelog-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Full Changelog:",
    "text": "Full Changelog:\n\nhttps://github.com/TileDB-Inc/TileDB/compare/2.10.0…2.11.0"
  },
  {
    "objectID": "HISTORY.html#disk-format-7",
    "href": "HISTORY.html#disk-format-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nConsolidation with timestamps: add includes timestamps to fragment footer. #3138"
  },
  {
    "objectID": "HISTORY.html#new-features-8",
    "href": "HISTORY.html#new-features-8",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nOR clause support in Query Conditions (#3041 #3083 #3112 #3264)\nNew examples for QueryCondition usage with the C++ API (#3225) and C API (#3242)\nTILEDB_BOOL Datatype #3164\nSupport for group metadata consolidation and vacuuming #3175"
  },
  {
    "objectID": "HISTORY.html#breaking-behavior-3",
    "href": "HISTORY.html#breaking-behavior-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior\n\nRemove timestamp-range vacuuming (experimental) #3214 ## Improvements\nSparse global order reader: refactor merge algorithm. #3173\nDense reader: add better stats for attribute copy. #3199\nDense reader: adding ability to fully disable tile cache. #3227\nOptimize compute_results_count_sparse_string. #3263"
  },
  {
    "objectID": "HISTORY.html#deprecations",
    "href": "HISTORY.html#deprecations",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\nDeprecate sm.num_tbb_threads config option #3177"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-2",
    "href": "HISTORY.html#bug-fixes-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nDense reader: fixing query conditions with overlapping domains. #3244\nConsolidation w timestamps: cell slab length computations fix. #3230\nUnordered writer: fixing segfault for empty writes. #3161\nSparse global order reader: Check the right incomplete reason is returned in case of too small user buffer #3170\nGlobal writes: check global order on write continuation. #3109\nFixing Dimension::splitting_value to prevent overflows. #3116\nParse minor and patch version with more than one digit #3098\nSparse unordered w/ dups reader: fix incomplete reason for cloud reads. #3104\nRearrange context member initialization so logger is initialized prior to getting thread counts (which may log) #3128\nadjust assert for var Range usage in legacy global order reader #3122\nFix SC-17415: segfault due to underflow in for loop #3143\nFix fragment_consolidation.cc example #3145\nChange test_assert path used to locate try_assert #3158\nexample writing_sparse_global_&lt;c,cpp&gt;, change illegal write to be legal #3159\navoid unit_range warning as error build failures #3171\nchange to avoid warning causing build error with msvc #3162\nSparse unordered w/ dups reader: fixing overflow on int value. #3181\nSparse index readers: fixing queries with overlapping ranges. #3208\nFix bad_optional_access exceptions when running consolidation with timestamps tests #3213\nFix SC-18250: segfault due to empty default-constructed FilterPipeline #3233\nFixed regression test for SC12024, Incorrect selected type in Dimension::oob #3219\nFixed bug in documentation for cpp_api query condition examples. #3239\nFix File API failure when importing into TileDB Cloud array #3246\nFix undefined behavior in filestore whilst detecting compression #3291\nFix printing of TILEDB_BLOB attributes in Attribute::Dump #3250\nAdd missing filters to switch case for Filter serialization #3256\nFix a typo in the byteshuffle constructor for capnp serialization #3284\nRemove incorrect noexcept annotations from C API implementations in filestore API #3273\nUpdate ensure_datatype_is_valid to fix deserialization issues #3303"
  },
  {
    "objectID": "HISTORY.html#api-changes-7",
    "href": "HISTORY.html#api-changes-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API Changes",
    "text": "API Changes\n\nC++ API\n\nApply TILEDB_NO_API_DEPRECATION_WARNINGS to C++ API #3236"
  },
  {
    "objectID": "HISTORY.html#build-system",
    "href": "HISTORY.html#build-system",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Build System",
    "text": "Build System\n\nAdd tiledb_regression test target #3143\nProduce a TileDBConfigVersion.cmake file #3240\nIntegrate build of webp into tiledb superbuild (note: build-only) #3113"
  },
  {
    "objectID": "HISTORY.html#new-contributors",
    "href": "HISTORY.html#new-contributors",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New Contributors",
    "text": "New Contributors\n\n@-OgreTransporter made their first contribution in https://github.com/TileDB-Inc/TileDB/pull/3118\n@-Biswa96 made their first contribution in https://github.com/TileDB-Inc/TileDB/pull/3124"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-3",
    "href": "HISTORY.html#bug-fixes-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix a typo in the byteshuffle constructor for capnp serialization #3284"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-4",
    "href": "HISTORY.html#bug-fixes-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix File API failure when importing into TileDB Cloud array #3246\nFix printing of TILEDB_BLOB attributes in Attribute::Dump #3250\nAdd missing filters to switch case for Filter serialization #3256\nFix filterpipeline segfault on release-2.9 #3261"
  },
  {
    "objectID": "HISTORY.html#improvements-11",
    "href": "HISTORY.html#improvements-11",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nMake filestore api get configurable buffer sizes #3223\nSpecial case tiledb cloud uris to use row_major writes 3232]([https://github.com/TileDB-Inc/TileDB/pull/3232)\n\n\nC++ API\n\nApply TILEDB_NO_API_DEPRECATION_WARNINGS to C++ API #3236"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-5",
    "href": "HISTORY.html#bug-fixes-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nUpdate Zlib Download URL #3200"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-6",
    "href": "HISTORY.html#bug-fixes-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n[Backport release-2.9] Sparse global order reader: refactor merge algorithm. (#3173) by @KiterLuc in https://github.com/TileDB-Inc/TileDB/pull/3182\n[Backport release-2.9] Sparse unordered w/ dups reader: fixing overflow on int value. by @github-actions in https://github.com/TileDB-Inc/TileDB/pull/3184\n[Backport release-2.9] Sparse global order reader: Check correct incomplete reason in case of too small user buffer by @github-actions in https://github.com/TileDB-Inc/TileDB/pull/3186\n[Backport release-2.9] relocate magic_mgc_gzipped.bin for build by @github-actions in https://github.com/TileDB-Inc/TileDB/pull/3185"
  },
  {
    "objectID": "HISTORY.html#disk-format-8",
    "href": "HISTORY.html#disk-format-8",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nUpdate on-disk format because of the new available compressor for Dictionary-encoding of strings #3042"
  },
  {
    "objectID": "HISTORY.html#new-features-9",
    "href": "HISTORY.html#new-features-9",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdd virtual filesystem ls_with_sizes function #2971\nAdd new CMake build option for TILEDB_EXPERIMENTAL_FEATURES to compile time protect experimental features. #2748\nForwardport Group API #3058\nSupport Dictionary-encoding filter for string dimensions and attributes #3077\nUse legacy sparse global order reader for 2.9 #3096\nAdd libmagic to build process. #3088\nNew file storage APIs (tiledb_filestore_…) #3121"
  },
  {
    "objectID": "HISTORY.html#improvements-12",
    "href": "HISTORY.html#improvements-12",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nConvert FilterPipeline deserialize function to static factory function #2799\nConvert array metadata deserialize function to factory function #2784\nA new thread pool with modern C++ compatible API and exception-safe behavior. #2944\nSmart pointer conversion: ArraySchema Domain #2948\nDeclare all C API functions noexcept. Put existing C API functions inside exception safety wrappers to meet the declaration change. #2961\nAdd support for compile-time assertion configuration #2962\nRefactored tiledb::sm::serialization::attribute_from_capnp to be C41 compliant #2937\nSmart pointer conversion: ArraySchema Dimension #2926\nRefactored tiledb::sm::serialization::filter_pipeline_from_capnp to be C41 compliant #2943\nEnable sparse global order reader by default. #2997\nAdd API for FragmentInfo::get_fragment_name #2977\nadd validity file format specification #2998\nConvert Domain class deserialize function to factory function #2800\nDense reader: fix user buffer offset computation for multi-index queries. #3002\nSparse readers: using zipped coords buffers for fragment version &lt; 5. #3016\nExtra UTs on string RLEs #3024\nBump Catch2 version to 2.13.8 #3027\nSplit consolidator in multiple classes. #3004\nHTML-render the existing format-spec Markdown docs. #3043\nAdd more detailed doc for schema evolution timestamp range functions. #3029\nRun doc-render job on doc-only PRs, and not on non-doc PRs #3045\nSupport curl POSTing &gt;2GB data to REST #3048\nDense reader: do not sort input ranges. #3036\nSupport consolidating non-contiguous fragments. #3037\nIntroduce dictionary-encoding as an enum option for filters #3042\nMove Range to new tiledb::type namespace #3059\nConvert tdb shared to shared #2965\nAdd StatusException, an exception class to be thrown instead of returning Status #3050\nCherry-pick #3061 #3064\nTypo fix in group.cc #3078\nRename tiledb time.h/math.h to avoid possible conflicts with standard header files. #3087\nConvert ArraySchema’s deserialize to a factory function #3012\nvarying_size_datum_at: fixing comparison error. #3127\nGlobal writes: check global order on write continuation. #3109"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-7",
    "href": "HISTORY.html#bug-fixes-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n[bug] Fix SC-17415: segfault due to underflow in for loop #3143\nSparse global order reader: prevent dims from being unfiltered twice. #3150\ncompare nullptr, avoid catch2 comparison warning failure #2970\nCheck that array is open before getting non_empty_domain #2980\nFix assertion failure in GCS, debug build #3001\nFix missing stats on cloud queries. #3009\nSparse unordered w/ dups reader: coord tiles management fix. #3023\nIncorrect validity result count in REST query #3015\nuse different API approach to avoid possible file sharing violation #3056\navoid some potentially invalid vector references #2932\nSparse Global Order Reader Fix: Decrement Total Cells #3046"
  },
  {
    "objectID": "HISTORY.html#api-additions-1",
    "href": "HISTORY.html#api-additions-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC++ API\n\nAdd function to check if Config contains a parameter #3082"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-8",
    "href": "HISTORY.html#bug-fixes-8",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nAll ranges tile overlap: skip computation for default dimensions. #3080\nFilter pipeline: fixing empty pipeline, multi chunk, refactored queries. #3149\nUnordered writer: fixing segfault for empty writes. #3161"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-9",
    "href": "HISTORY.html#bug-fixes-9",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nSparse unordered w/ dups reader: fix incomplete reason for cloud reads. #3104"
  },
  {
    "objectID": "HISTORY.html#improvements-13",
    "href": "HISTORY.html#improvements-13",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdd golang annotation to capnp spec file #3089\nUpdate group metadata REST request to standardize cap’n proto class usage #3095"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-10",
    "href": "HISTORY.html#bug-fixes-10",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nSparse Index Reader Fix: Check For Empty Buffer #3051\nReset group metadata only based on end timestamp to ensure its always reset to now #3091"
  },
  {
    "objectID": "HISTORY.html#disk-format-9",
    "href": "HISTORY.html#disk-format-9",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nAdd Metadata to groups #2966\nAdd Group on disk structure for members #2966"
  },
  {
    "objectID": "HISTORY.html#new-features-10",
    "href": "HISTORY.html#new-features-10",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nSupport gs:// as an alias for gcs:// #2864\nEliminate LOG_FATAL use from codebase #2897\nCollect missing docs #2922\nSupport tiledb:// objects in the Object API #2954\nRLE compression support for var-length string dimensions #2938\nAdd Metadata to groups #2966\nAdd robust API to groups for adding and removing members of a group #2966"
  },
  {
    "objectID": "HISTORY.html#improvements-14",
    "href": "HISTORY.html#improvements-14",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nSupport top-level cap’n proto array object #2844\nNicer error message for tiledb fragment listing #2872\nRemoving Buffer from Tile. #2852\nSplitting Writer class into 3 separate classes. #2884\nAdding a compressor algorithm for RLE encoding of strings #2857\nReader: treating empty string range as expected. #2883\nAdd a compression algorithm for dictionary encoding of strings #2880\nAdds an ArrayDirectory class to manage all URIs within the array directory. #2909\nRemove accidental addition of writer.cc. #2917\nTile metadata generator: code cleanup. #2919\nListing improvements: new directory structure for array. #2918\nArraySchema’s Attribute smart pointer conversion #2887\nAdd option for tile level filtering #2906\nSwitch to smart pointers and const references for ArraySchema, and avoid fetching the latest array schema twice. #2923\nMove vfs_helpers.cc and helper.cc into separate library with target that can be referenced elsewhere. #2929\nAvoid calling generate_uri in ArraySchema accessors #2928\nGlobal order writer: initialize last_var_offsets_. #2930\nWrap some C API functions with exception handlers. #2650\nFragment metadata: add min/max/sun/null count. #2934\nFilter pipeline: incorrect stopping point during chunk parallellization. #2955\nAdding support to consolidate ok/wrt files. #2933\nTile medatada: treating TILEDB_CHAR as TILEDB_STRING_ASCII. #2953\nFragment metadata: treating TILEDB_CHAR as TILEDB_STRING_ASCII. #2958\nGlobal order writer: fixing multi writes for var size attributes. #2963\nVFS: adding configuration for vfs.max_batch_size. #2960\nFixing build errors using MacOSX12.3.sdk. #2981\nSupport reading all consolidated fragment metadata files. #2973\nFixing compute_results_count_sparse_string for multiple range threads. #2983\nGlobal writes: fixing OOM on write continuation. #2993\nDo not store offsets when RLE is used on string dimensions #2969\nDynamically infer bytesizes for run length and strings for strings RLE compression #2984\nAdd ability to store (optional) name with group member #3068"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-11",
    "href": "HISTORY.html#bug-fixes-11",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nAvoid thread starvation by removing std::future usage in S3 multipart upload #2851\nwindows_sanity fix #2870\nAdds missing pthreads link to dynamic memory unit test #2888\nRemove common.h from arrow_io_impl.h #2915\nRange::set_start and set_end should throw instead of empty returning #2913\nGlobal writer: fixing write continuation for fixed sized attributes. #3062\ntiledb_serialize_array_metadata should load metadata if its not loaded before serializing #3065\ntiledb_serialize_group_metadata should load group metadata if its not loaded. #3070"
  },
  {
    "objectID": "HISTORY.html#api-additions-2",
    "href": "HISTORY.html#api-additions-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nIntroduce experimental tiledb_ctx_alloc_with_error to return error when context alloc fails #2905\nAdd tiledb_group_* APIs for robust group support #2966\n\n\n\nC++ API\n\nAdd missing cstddef include to fix compile w/ GCC 7 #2885\nAdd Group::* APIs for robust group support #2966"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-12",
    "href": "HISTORY.html#bug-fixes-12",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nSparse unordered w/ dups reader: fixing memory management for tiles. #2924"
  },
  {
    "objectID": "HISTORY.html#disk-format-10",
    "href": "HISTORY.html#disk-format-10",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nRemoved file __lock.tdb from the array folder and updated the format spec. Also removed config vfs.file.enable_filelocks. #2692"
  },
  {
    "objectID": "HISTORY.html#new-features-11",
    "href": "HISTORY.html#new-features-11",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nPublish Subarray access/functionality for use outside of core. #2214\nAdd TILEDB_BLOB datatype #2721\nExpose array schema libtiledb version information #2863"
  },
  {
    "objectID": "HISTORY.html#improvements-15",
    "href": "HISTORY.html#improvements-15",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nConvert loose files thread_pool.* into a unit #2520\nBump cmake_minimum_required to 3.21 #2532\ndynamic_memory unit, including allocator #2542\nimplement windows CI functionality with github actions #2498\nimplement windows crash dump file processing for windows #2657\nAdded object libraries: baseline, buffer, thread_pool #2629\nubuntu core dump processing (GA) CI #2642\nAvoid copy and set of config in tiledb_query_add_range as a performance optimization from the new subarray APIs #2740\ncore file stack backtracing and artifact uploading for mac CI builds #2632\nMove Status::*Error to Status_*Error #2610\nZStd Compressor: allocate one context per thread and re-use. #2701\nInclude offset index in oversize error message #2757\nPrint resource pool contained type on error #2757\nUsing RETURN_NOT_OK_TUPLE in attribute.cc. #2787\nAdd timestamp range to schema evolution to avoid race conditions based on schema timestamp #2776\nConvert dimension deserialize to factory #2763\nImprove object type detection performance #2792\nQuery condition: differentiate between nullptr and empty string. #2802\npatch git+git: to git+https: to avoid GH access failure #2805\nRemove examples writing sparse fragments to dense arrays. #2804\nChange the ZStd filter compression level range and add defined compression level default values #2623\nChange the ZStd filter compression level range and add defined compression level default values #2811\nchanges to augment dbg output on build failures #2801\nSparse unordered w/ dups reader: process queries until full user buffers #2816\nSupport var-length CHAR QueryConditions #2814\nSparse global order reader: merge smaller cell slabs. #2818\nFixing build errors in query condition. #2820\nEnabling refactored dense reader by default. #2808\nSwitching TUPLE macros to be variadic. #2822\nRead tiles: no buffer pre-allocation for no-opt filter. #2819\nRefactored dense reader: resetting the unsplittable flag on completion. #2832\nRefactored dense reader: code cleanup. #2833\nFixing Subarray::crop_to_tile to crop default dimensions. #2831\nDense reader: fixing src cell offset for different cell order. #2835\nSupport storing integral cells in a chunk. #2821\nImprove error handling for Array non empty domain calls. #2845\nRemove OpenArray and refactor StorageManager and FragmentInfo #2839\nAdding min/max/sum/null count to fragment info. #2830\nFine-tune unfiltering parallelization #2842\nRemoving ch9473 comments in unit-cppapi-string-dims.cc. #2837\nGlobal writes: writting bad validity values when coordinates are dups. #2848\nFixing issues in VS2019. #2853\nWriter: processing var tiles before offset tiles. #2854\nWriter: moving unordered_map::emplace outside of parallel_for. #2860\nDense reader: removing unnecessary loop around read_attributes. #2859\nConvert deserialize function of Attribute to factory function #2566\nConvert Filter class deserialize and create to factory functions #2655\nTile metadata test: reducing amount of spew in verbose mode. #2882\ncompute_results_count_sparse_string: fixing incorect memcmp. #2892\nSparse rindex readers: fixing query resume on TileDB cloud. #2900"
  },
  {
    "objectID": "HISTORY.html#deprecations-1",
    "href": "HISTORY.html#deprecations-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\nDeprecate TILEDB_CHAR in favor of users using TILEDB_BLOB or TILEDB_STRING_ASCII for attribute datatypes. #2742 #2797\nDeprecate TILEDB_ANY datatype #2807\nDeprecate TILEDB_STRING_USC2 and TILEDB_STRING_USC4 datatypes. #2812\nDrop support for Visual Studio 2017 compiler #2847"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-13",
    "href": "HISTORY.html#bug-fixes-13",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nBetter windows relative path support with ‘/’ #2607\nFixed dangling links in README. #2712\nHandle multiple core files for stack traces and archiving #2723\nrestore lost cmake code necessary for clean build on windows with -EnableAzure #2656\nReturn error for nonempty_domain access regardless of local/remote status #2766\nFix segfault in new sparse null QueryCondition code #2794\nReaderBase needs to load var sizes #2809\nOnly initialize REST query strategies once #2836\nLogger json format output is not valid #2850\nClosing a non-opened array should be a no-op instead of error. #2889\npatch (unsupported) 3rd party azure cpp lite sdk used by TileDB to avoid memory faults #2881\nFree allocated latest array schema when on error of loading all array schemas #2907"
  },
  {
    "objectID": "HISTORY.html#api-additions-3",
    "href": "HISTORY.html#api-additions-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdd tiledb_array_schema_evolution_set_timestamp_range to avoid race conditions based on schema timestamp #2776\nDeprecate TILEDB_CHAR in favor of users using TILEDB_BLOB or TILEDB_STRING_ASCII for attribute datatypes. #2742 #2797\nAdd {set,get}_validity_filter_list #2798\nDeprecate TILEDB_ANY datatype #2807\nDeprecate TILEDB_STRING_USC2 and TILEDB_STRING_USC4 datatypes. #2812\nAdd tiledb_array_schema_get_version for fetching array schema version #2863\nIntroduce experimental tiledb_ctx_alloc_with_error to return error when context alloc fails #2905\n\n\n\nC++ API\n\nAdd ArraySchemaEvolution::set_timestamp_range to avoid race conditions based on schema timestamp #2776\nDeprecate TILEDB_CHAR in favor of users using TILEDB_BLOB or TILEDB_STRING_ASCII for attribute datatypes. #2742 #2797\nAdd validity_filter_list set/get and missing get tests #2798\nDeprecate TILEDB_ANY datatype #2807\nDeprecate TILEDB_STRING_USC2 and TILEDB_STRING_USC4 datatypes. #2812\nAdd ArraySchema::version() for fetching array schema version #2863\nAdd missing cstddef include to fix compile w/ GCC 7 #2885"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-14",
    "href": "HISTORY.html#bug-fixes-14",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nSparse unordered w/ dups reader: all empty string attribute fix. #2874\nUpdate Location of Zlib Download URL #2945"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-15",
    "href": "HISTORY.html#bug-fixes-15",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\ncompute_results_count_sparse_string: fixing incorect memcmp. #2892\nSparse rindex readers: fixing query resume on TileDB cloud. #2900"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-16",
    "href": "HISTORY.html#bug-fixes-16",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nOnly initialize REST query strategies once #2836\nSparse unordered w dups reader: fixing max pos calculation in tile copy. #2840"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-17",
    "href": "HISTORY.html#bug-fixes-17",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nSparse unordered w/ dups reader: off by one error in query continuation. #2815\nSparse unordered w dups reader: fixing query continuation with subarray. #2824"
  },
  {
    "objectID": "HISTORY.html#api-additions-4",
    "href": "HISTORY.html#api-additions-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC++ API\n\ntiledb::Array destructor no longer calls ::close for non-owned C ptr #2823"
  },
  {
    "objectID": "HISTORY.html#improvements-16",
    "href": "HISTORY.html#improvements-16",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nSparse unordered with dups reader: removing result cell slabs. #2606\nUse as-installed path for TileDBConfig CMake static library imports #2669\nCheck error message variable for nullptr before further use #2634\nFixing str_coord_intersects to use std::basic_string_view. #2654\nSparse unordered with duplicates reader: cell num fix. #2636\nSparse unordered with dups reader: fixing initial bound calculation. #2638\nRead_tiles parallelization improvements. #2644\nSparse global order reader: memory management unit tests. #2645\nReduce scope of open_array_for_reads_mtx_ locks #2681\nSparse refactored readers: better parallelization for tile bitmaps. #2643\nZStd compressor: allocate one context per thread and re-use. #2691\nSparse refactored readers: disable filtered buffer tile cache. #2651\nMoving coord_string from returning a std::string to std::basic_string_view. #2704\nSparse unordered w/ dups reader: tracking cell progress. #2668\nSparse unordered w/ dups reader: fixing var size overflow adjustment. #2713\nEnable memfs tests that were disabled by mistake #2648\nAdd helpful details to memory limit error strings. #2729\nSparse refactored readers: Better vectorization for tile bitmaps calculations. #2711\nSort ranges for unordered with duplicate reader and exit comparisons early #2736\nSparse refactored readers: better vectorization for query condition. #2737\nUse correct frag index in tiles creation for compute_result_space_tiles. #2741\nUse a single uint64 for cell counts #2749\nArray Schema name should be included with cap’n proto serialization #2696\nAdd and use blocking resource pool #2735\nMaking the allocation part of read_tiles single threaded. #2753\nSparse unordered w/ dups reader: remove invalid assert. #2778\nRead tiles: fixing preallocation size for var and validity buffers. #2781\nSparse unordered w/ dups: var buffer overflow on tile continuation fix. #2777\nDetermine non overlapping ranges automatically. #2780"
  },
  {
    "objectID": "HISTORY.html#deprecations-2",
    "href": "HISTORY.html#deprecations-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\neliminate usage of std::iterator due to c++17 deprecation #2675"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-18",
    "href": "HISTORY.html#bug-fixes-18",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nupgrade to blosc 1.21.0 from 1.14.x #2422\nGuard ZStd resource pool to fix initialization race #2699\nC API Add missing save_error calls in vfs_ls #2714\nUse fragment array schema for applying query condition to account for schema evolution #2698\nDon’t try to read config from uninitialize storage manager #2771"
  },
  {
    "objectID": "HISTORY.html#api-additions-5",
    "href": "HISTORY.html#api-additions-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdd bulk point-range setter tiledb_query_add_point_ranges #2765\nAdd experimental query status details API #2770\n\n\n\nC++ API\n\nBackport Query::ctx and Query::array getters from 2.7 #2754"
  },
  {
    "objectID": "HISTORY.html#improvements-17",
    "href": "HISTORY.html#improvements-17",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nRemoving unnecessary openssl callback function. #2705\nopenssl3 md5 deprecation mitigation #2716\nSparse refactored reader: change all_tiles_loaded_ to vector of uint8_t. #2724"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-19",
    "href": "HISTORY.html#bug-fixes-19",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nProperly check and use legacy readers instead of refactored in serialized query. #2667\nSet array URI in cap’n proto object for compatibility with repeated opened array usage in TileDB 2.4 and older. #2676\nAdd the compute_mbr_var_func_pointer assignment in Dimension constructor #2730"
  },
  {
    "objectID": "HISTORY.html#improvements-18",
    "href": "HISTORY.html#improvements-18",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nProvide non-AVX2 build artifact on Linux #2649\nError out when setting multiple ranges for global layout #2658"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-20",
    "href": "HISTORY.html#bug-fixes-20",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nPatch AWS sdk for cmake 3.22 support #2639\nRemove assert on memory_used_result_tile_ranges_ in SparseUnorderedWithDupsReader #2652\nRemove tiles that are empty through being filtered with a query condition #2659\nAlways load array schemas during array open to find any new array schemas created from array schema evolution #2613"
  },
  {
    "objectID": "HISTORY.html#new-features-12",
    "href": "HISTORY.html#new-features-12",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nDisable AVX2 for MSys2 builds used by CRAN #2614"
  },
  {
    "objectID": "HISTORY.html#improvements-19",
    "href": "HISTORY.html#improvements-19",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nClarify error messages in check_buffers_correctness() #2580"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-21",
    "href": "HISTORY.html#bug-fixes-21",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix schema evolution calls on all pre-TileDB 2.4 arrays #2611\nUnordered reads should be allowed for dense arrays #2608\nFix logger creation on context to be threadsafe #2625"
  },
  {
    "objectID": "HISTORY.html#api-additions-6",
    "href": "HISTORY.html#api-additions-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC++ API\n\nAdd C++ API for Context::last_error() #2609"
  },
  {
    "objectID": "HISTORY.html#breaking-c-api-changes-2",
    "href": "HISTORY.html#breaking-c-api-changes-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking C API changes",
    "text": "Breaking C API changes\n\nRemove deprecated c-api tiledb_array_max_buffer_size and tiledb_array_max_buffer_size_var #2579\nRemove deprecated cpp-api Array::max_buffer_elements #2579"
  },
  {
    "objectID": "HISTORY.html#new-features-13",
    "href": "HISTORY.html#new-features-13",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nSupport upgrading an older version array to the latest version #2513\nAdd improved logging support to classes #2565"
  },
  {
    "objectID": "HISTORY.html#improvements-20",
    "href": "HISTORY.html#improvements-20",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nReplace Buffer key_ with char key_[32] per shortcut story id 9561 #2502\nRemove support for sparse writes in dense arrays. #2504\nInitial dense refactor. #2503\nMore concise cmake output during build #2512\nSparse refactored readers: fixing looping behavior on large arrays. #2530\nUse sparse global order reader for unordered without duplicates queries. #2526\nAdd CMakeUserPresets.json to .gitignore #2534\nSparse unordered with duplicates reader: support multiple ranges. #2537\nRefactored sparse readers: tile overlap refactor. #2547\nRefactored dense reader: fixing output buffer offsets with multi-ranges. #2553\nRefactored sparse readers: serialization fixes. #2558\nRefactored sparse readers: proper lifetime for tile bitmaps. #2563\nREST scratch buffer is now owned by the query to allow reuse #2555\nRemove default constructor from Dimension #2561\nResource pool: fixing off by one error. #2567\nSplitting config for refactored readers. #2569\nSparse refactored readers: memory management unit tests. #2568\nRemoved all aspects of posix_code from Status #2571\nFixing pre-loading for tile offsets in various readers. #2570\nUse the new logger in Subarray, SubarrayPartitioner and Consolidator classes. #2574\nAdd tiledb_fragment_info_get_schema_name #2581\nEnable CMake AVX2 check #2591\nAdding logging for sparse refactored readers. #2575\nuse ROW_MAJOR read paths for unordered reads of Hilbert layout array #2551"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-22",
    "href": "HISTORY.html#bug-fixes-22",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix the memory leak in store_array_schema in the StorageManager class. #2480\nFix curl/REST query scratch size to reset after each query is processed. #2535\nSparse refactored readers: segfault with dimension only reads. #2539\nREST array metadata writes should post with timestamps #2545\nFix bug in Arrow schema construction #2554\nReplaced auto& path with auto path #2560"
  },
  {
    "objectID": "HISTORY.html#api-additions-7",
    "href": "HISTORY.html#api-additions-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nExpose MBR in Fragment Info API #2222\nAdd tiledb_fragment_info_get_array_schema_name for fetching array name used by fragment #2581\n\n\n\nC++ API\n\nExpose MBR in Fragment Info API #2222\nAdd FragmentInfo::array_schema_name for fetching array name used by fragment #2581"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-23",
    "href": "HISTORY.html#bug-fixes-23",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix segfault in result ResultTile::coord_string and ResultTile::compute_results_sparse&lt;char&gt; due empty chunk buffer #2531\nFix memory corruption with empty result set in extra_element mode #2540\nREST array metadata writes should post with timestamps #2545\nBackport fixes for new Sparse Unordered with Duplicate readers from #2530 and #2538"
  },
  {
    "objectID": "HISTORY.html#api-additions-8",
    "href": "HISTORY.html#api-additions-8",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions"
  },
  {
    "objectID": "HISTORY.html#new-features-14",
    "href": "HISTORY.html#new-features-14",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdd support for empty string as query condition value. #2507"
  },
  {
    "objectID": "HISTORY.html#improvements-21",
    "href": "HISTORY.html#improvements-21",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nSupport writing empty strings for dimensions #2501\nRefactored readers can segfault when multiple contexts are used. #2525"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-24",
    "href": "HISTORY.html#bug-fixes-24",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix ch10191: check cell_val_num for varlen status instead of result count #2505\nDo not access variables after moving them #2522\nAdd try/catch to tiledb_ctx_alloc for exception safety #2527"
  },
  {
    "objectID": "HISTORY.html#disk-format-11",
    "href": "HISTORY.html#disk-format-11",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nStore array schemas under __schema directory #2258"
  },
  {
    "objectID": "HISTORY.html#new-features-15",
    "href": "HISTORY.html#new-features-15",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nPerform early audit for acceptable aws sdk windows path length #2260\nSupport setting via config s3 BucketCannedACL and ObjectCannedACL via SetACL() methods #2383\nUpdate spdlog dependency to 1.9.0 fixing c++17 compatibility and general improvements #1973\nAdded Azure SAS token config support and new config option #2420\nLoad all array schemas in storage manager and pass the appropriate schema pointer to each fragment #2415\nFirst revision of the Interval class #2417\nAdd tiledb_schema_evolution_t and new apis for schema evolution #2426\nAdd ArraySchemaEvolution to cpp_api and its unit tests are also added. #2462\nAdd c and cpp api functions for getting the array schema of a fragment #2468\nAdd capnp serialization and rest support for array schema evolution objects #2467"
  },
  {
    "objectID": "HISTORY.html#improvements-22",
    "href": "HISTORY.html#improvements-22",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nencryption_key and encryption_type parameters have been added to the config; internal APIs now use these parameters to set the key. #2245\nInitial read refactor #2374\nCreate class ByteVecValue from typedef #2368\nEncapsulate spdlog.h #2396\nUpdate OSX target to 10.14 for release artifacts #2401\nAdd nullable (and unordered, nullable) support to the smoke test. #2405\nInitial sparse global order reader #2395\nRemove sm.sub_partitioner_memory_budget #2402\nUpdate the markdown documents for our new version of array schemas #2416\nSparse global order reader: no more result cell slab copy. #2411\nSparse global order reader: initial memory budget improvements. #2413\nOptimization of result cell slabs generation for sparse global order reader. #2414\nRemove selective unfiltering. #2410\nUpdated Azure Storage Lite SDK to 0.3.0 #2419\nRespect memory budget for sparse global order reader. #2425\nUse newer Azure patch for all platforms to solve missing header error #2433\nincreased diag output for differences reported by tiledb_unit (some of which may be reasonable) #2437\nAdjustments to schema evolution new attribute reads #2484\nChange Quickstart link in readthedocs/doxygen index.rst #2448\nInitial sparse unordered with duplicates reader. #2441\nAdd calls to malloc_trim on context and query destruction linux to potentially reduce idle memory usage #2443\nAdd logger internals for std::string and std::stringstream for developer convenience #2454\nAllow empty attribute writes. #2461\nRefactored readers: serialization. #2458\nAllow null data pointers for writes. #2481\nUpdate backwards compatibility arrays for 2.3.0 #2487"
  },
  {
    "objectID": "HISTORY.html#deprecations-3",
    "href": "HISTORY.html#deprecations-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\nDeprecate all *_with_key APIs. #2245 #2308 #2412"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-25",
    "href": "HISTORY.html#bug-fixes-25",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix to correctly apply capnproto create_symlink avoidance patch #2264\nThe bug for calculating max_size_validity for var_size attribute caused incomplete query #2266\nAlways run ASAN with matching compiler versions #2277\nFix some loop bounds that reference non-existent elements #2282\nTreating std::vector like an array; accessing an element that is not present to get its address. #2276\nFix buffer arguments in unit-curl.cc #2287\nStop loop iterations within limits of vector being initialized. #2320\nModify FindCurl_EP.cmake to work for WIN32 -EnableDebug builds #2319\nFixing test failure because of an uninitialized buffer. #2386\nChange a condition that assumed MSVC was the only compiler for WIN32 #2388\nFix defects in buffer classes: read, set_offset, advance_offset #2342\nUse CHECK_SAFE() to avoid multi-threaded conflict #2394\nUse tiledb _SAFE() items when overlapping threads may invoke code #2418\nChanges to address issues with default string dimension ranges in query #2436\nOnly set cmake policy CMP0076 if cmake version in use knows about it #2463\nFix handling curl REST request having all data in single call back #2485\nWrite queries should post start/end timestamps for REST arrays #2492"
  },
  {
    "objectID": "HISTORY.html#api-additions-9",
    "href": "HISTORY.html#api-additions-9",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nIntroduce new tiledb_experimental.h c-api header for new feature that don’t have a stabilized api yet #2453\nIntroduce new tiledb_experimental cpp-api header for new feature that don’t have a stabilized api yet #2453\n\n\nC API\n\nRefactoring [get/set]_buffer APIs #2315\nAdd tiledb_fragment_info_get_array_schema functions for getting the array schema of a fragment #2468\nAdd tiledb_schema_evolution_t and new apis for schema evolution #2426\n\n\n\nC++ API\n\nRefactoring [get/set]_buffer APIs #2399\nAdd FragmentInfo::array_schema functions for getting the array schema of a fragment #2468\nAdd ArraySchemaEvolution to cpp_api and its unit tests are also added. #2462"
  },
  {
    "objectID": "HISTORY.html#improvements-23",
    "href": "HISTORY.html#improvements-23",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nQuery::set_layout: setting the layout on the subarray. #2451\nAllow empty attribute writes. #2461"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-26",
    "href": "HISTORY.html#bug-fixes-26",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix deserialization of buffers in write queries with nullable var-length attributes #2442"
  },
  {
    "objectID": "HISTORY.html#improvements-24",
    "href": "HISTORY.html#improvements-24",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nIncrease REST (TileDB Cloud) retry count from 3 to 25 to be inline with S3/GCS retry times #2421\nAvoid unnecessary est_result_size computation in must_split #2431\nUse newer Azure patch for all platforms to solve missing header error #2433"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-27",
    "href": "HISTORY.html#bug-fixes-27",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix c-api error paths always resetting any alloced pointers to nullptr in-addition to deleting #2427"
  },
  {
    "objectID": "HISTORY.html#improvements-25",
    "href": "HISTORY.html#improvements-25",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nSupport more env selectable options in both azure-windows.yml and azure-windows-release.yml #2384\nEnable Azure/Serialization for windows CI artifacts #2400"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-28",
    "href": "HISTORY.html#bug-fixes-28",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nCorrect check for last offset position so that undefined memory is not accessed. #2390\nFix ch8416, failure to read array written with tiledb 2.2 via REST #2404\nFix ch7582: use the correct buffer for validity deserialization #2407"
  },
  {
    "objectID": "HISTORY.html#improvements-26",
    "href": "HISTORY.html#improvements-26",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nUpdate bzip2 in windows build to 1.0.8 #2332\nFixing S3 build for OSX11 #2339\nFixing possible overflow in Dimension::tile_num #2265\nFixing tile extent calculations for signed integer domains #2303\nAdd support for cross compilation on OSX in superbuild #2354\nRemove curl link args for cross compilation #2359\nEnable MacOS arm64 release artifacts #2360\nAdd more stats for `compute_result_coords` path #2366\nSupport credentials refresh for AWS #2376"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-29",
    "href": "HISTORY.html#bug-fixes-29",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixing intermittent metadata test failure #2338\nFix query condition validation check for nullable attributes with null conditions #2344\nMulti-range single dimension query fix #2347\nRewrite Dimension::overlap_ratio #2304\nFollow up fixes to floating point calculations for tile extents #2341\nFix for set_null_tile_extent_to_range #2361\nSubarray partitioner, unordered should be unordered, even for Hilbert. #2377"
  },
  {
    "objectID": "HISTORY.html#disk-format-12",
    "href": "HISTORY.html#disk-format-12",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nFormat version incremented to 9. #2108"
  },
  {
    "objectID": "HISTORY.html#breaking-behavior-4",
    "href": "HISTORY.html#breaking-behavior-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior\n\nThe setting of `sm.read_range_oob` now defaults to `warn`, allowing queries to run with bounded ranges that errored before. #2176\nRemoves TBB as an optional dependency #2181"
  },
  {
    "objectID": "HISTORY.html#new-features-16",
    "href": "HISTORY.html#new-features-16",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nSupport TILEDB_DATETIME_{SEC,MS,US,NS} in arrow_io_impl.h #2228\nAdds support for filtering query results on attribute values #2141\nAdding support for time datatype dimension and attribute #2140\nAdd support for serialization of config objects #2164\nAdd C and C++ examples to the examples/ directory for the tiledb_fragment_info_t APIs. #2160\nsupporting serialization (using capnproto) build on windows #2100\nConfig option “vfs.s3.sse” for S3 server-side encryption support #2130\nName attribute/dimension files by index. This is fragment-specific and updates the format version to version 9. #2107\nSmoke Test, remove nullable structs from global namespace. #2078"
  },
  {
    "objectID": "HISTORY.html#improvements-27",
    "href": "HISTORY.html#improvements-27",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nreplace ReadFromOffset with ReadRange in GCS::read() to avoid excess gcs egress traffic #2307\nHilbert partitioning fixes #2269\nStats refactor #2267\nImprove Cap’n Proto cmake setup for system installations #2263\nRuntime check for minimum validity buffer size #2261\nEnable partial vacuuming when vacuuming with timestamps #2251\nConsolidation: de-dupe FragmentInfo #2250\nConsolidation: consider non empty domain before start timestamp #2248\nAdd size details to s3 read error #2249\nConsolidation: do not re-open array for each fragment #2243\nSupport back compat writes #2230\nSerialization support for query conditions #2240\nMake SubarrayPartitioner’s member functions to return Status after calling Subarray::get_range_num. #2235\nUpdate bzip2 super build version to 1.0.8 to address CVE-2019-12900 in libbzip2 #2233\nTimestamp start and end for vacuuming and consolidation #2227\nFix memory leaks reported on ASAN when running with leak-detection. #2223\nUse relative paths in consolidated fragment metadata #2215\nOptimize Subarray::compute_relevant_fragments #2216\nAWS S3: improve is_dir #2209\nAdd nullable string to nullable attribute example #2212\nAWS S3: adding option to skip Aws::InitAPI #2204\nAdded additional stats for subarrays and subarray partitioners #2200\nIntroduces config parameter “sm.skip_est_size_partitioning” #2203\nAdd config to query serialization. #2177\nConsolidation support for nullable attributes #2196\nAdjust unit tests to reduce memory leaks inside the tests. #2179\nReduces memory usage in multi-range range reads #2165\nAdd config option `sm.read_range_oob` to toggle bounding read ranges to domain or erroring #2162\nWindows msys2 build artifacts are no longer uploaded #2159\nAdd internal log functions to log at different log levels #2161\nParallelize Writer::filter_tiles #2156\nAdded config option “vfs.gcs.request_timeout_ms” #2148\nImprove fragment info loading by parallelizing fragment_size requests #2143\nAllow open array stats to be printed without read query #2131\nCleanup the GHA CI scripts - put common code into external shell scripts. #2124\nReduced memory consumption in the read path for multi-range reads. #2118\nThe latest version of dev was leaving behind a test/empty_string3/. This ensures that the directory is removed when make check is run. #2113\nMigrating AZP CI to GA #2111\nCache non_empty_domain for REST arrays like all other arrays #2105\nAdd additional stats printing to breakdown read state initialization timings #2095\nPlaces the in-memory filesystem under unit test #1961\nAdds a Github Action to automate the HISTORY.md #2075\nChange printfs in C++ examples to cout, edit C print statements to fix format warnings #2226"
  },
  {
    "objectID": "HISTORY.html#deprecations-4",
    "href": "HISTORY.html#deprecations-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\nThe following APIs have been deprecated: tiledb_array_open_at, tiledb_array_open_at_with_key, tiledb_array_reopen_at. #2142"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-30",
    "href": "HISTORY.html#bug-fixes-30",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix a segfault on VFS::ls for the in-memory filesystem #2255\nFix rare read corruption in S3 #2253\nUpdate some union initializers to use strict syntax #2242\nFix race within S3::init_client #2247\nExpand accepted windows URIs. #2237\nWrite fix for unordered writes on nullable, fixed attributes. #2241\nFix tile extent to be reported as domain extent for sparse arrays with Hilbert ordering #2231\nDo not consider option sm.read_range_oob for set_subarray() on Write queries #2211\nChange avoiding generation of multiple, concatenated, subarray flattened data. #2190\nChange mutex from basic to recursive #2180\nFixes a memory leak in the S3 read path #2189\nFixes a potential memory leak in the filter pipeline #2185\nFixes misc memory leaks in the unit tests #2183\nFix memory leak of `tiledb_config_t` in error path of `tiledb_config_alloc`. #2178\nFix check for null pointer in query deserialization #2163\nFixes a potential crash when retrying incomplete reads #2137\nFixes a potential crash when opening an array with consolidated fragment metadata #2135\nCorrected a bug where sparse cells may be incorrectly returned using string dimensions. #2125\nFix segfault in serialized queries when partition is unsplittable #2120\nAlways use original buffer size in serialized read queries serverside. #2115\nFix an edge-case where a read query may hang on array with string dimensions #2089"
  },
  {
    "objectID": "HISTORY.html#api-additions-10",
    "href": "HISTORY.html#api-additions-10",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdded tiledb_array_set_open_timestamp_start and tiledb_array_get_open_timestamp_start #2285\nAdded tiledb_array_set_open_timestamp_end and tiledb_array_get_open_timestamp_end #2285\nAddition of tiledb_array_set_config to directly assign a config to an array. #2142\ntiledb_query_get_array now returns a deep-copy #2184\nAdded `tiledb_serialize_config` and `tiledb_deserialize_config` #2164\nAdd new api, tiledb_query_get_config to get a query’s config. #2167\nRemoves non-default parameter in “tiledb_config_unset”. #2099\n\n\n\nC++ API\n\nAdded Array::set_open_timestamp_start and Array::open_timestamp_start #2285\nAdded Array::set_open_timestamp_end and Array::open_timestamp_end #2285\nadd Query::result_buffer_elements_nullable support for dims #2238\nAddition of tiledb_array_set_config to directly assign a config to an array. #2142\nAdd new api, Query.config() to get a query’s config. #2167\nRemoves non-default parameter in “Config::unset”. #2099\nAdd support for a string-typed, variable-sized, nullable attribute in the C++ API. #2090"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-31",
    "href": "HISTORY.html#bug-fixes-31",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix rare read corruption in S3 #2254\nWrite fix for unordered writes on nullable, fixed attributes #2241"
  },
  {
    "objectID": "HISTORY.html#disk-format-13",
    "href": "HISTORY.html#disk-format-13",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format"
  },
  {
    "objectID": "HISTORY.html#breaking-c-api-changes-3",
    "href": "HISTORY.html#breaking-c-api-changes-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking C API changes",
    "text": "Breaking C API changes"
  },
  {
    "objectID": "HISTORY.html#breaking-behavior-5",
    "href": "HISTORY.html#breaking-behavior-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior"
  },
  {
    "objectID": "HISTORY.html#new-features-17",
    "href": "HISTORY.html#new-features-17",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nSupport TILEDB_DATETIME_{SEC,MS,US,NS} in arrow_io_impl.h #2229\nAdd support for serialization of config objects #2164\nAdd support for serialization of query config #2177"
  },
  {
    "objectID": "HISTORY.html#improvements-28",
    "href": "HISTORY.html#improvements-28",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nOptimize Subarray::compute_relevant_fragments #2218\nReduces memory usage in multi-range range reads #2165\nAdd config option sm.read_range_oob to toggle bounding read ranges to domain or erroring #2162\nUpdates bzip2 to v1.0.8 on Linux/OSX #2233"
  },
  {
    "objectID": "HISTORY.html#deprecations-5",
    "href": "HISTORY.html#deprecations-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-32",
    "href": "HISTORY.html#bug-fixes-32",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixes a potential memory leak in the filter pipeline #2185\nFixes misc memory leaks in the unit tests #2183\nFix memory leak of tiledb_config_t in error path of tiledb_config_alloc. #2178"
  },
  {
    "objectID": "HISTORY.html#api-additions-11",
    "href": "HISTORY.html#api-additions-11",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\ntiledb_query_get_array now returns a deep-copy #2188\nAdd new api,tiledb_query_get_config to get a query’s config. #2167\nAdded tiledb_serialize_config and tiledb_deserialize_config #2164\n\n\n\nC++ API\n\nAdd new api, Query.config() to get a query’s config. #2167"
  },
  {
    "objectID": "HISTORY.html#improvements-29",
    "href": "HISTORY.html#improvements-29",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdded config option vfs.gcs.request_timeout_ms #2148\nImprove fragment info loading by parallelizing fragment_size requests #2143\nApply ‘var_offsets.extra_element’ mode to string dimension offsets too #2145"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-33",
    "href": "HISTORY.html#bug-fixes-33",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixes a potential crash when retrying incomplete reads #2137"
  },
  {
    "objectID": "HISTORY.html#new-features-18",
    "href": "HISTORY.html#new-features-18",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nConfig option vfs.s3.sse for S3 server-side encryption support #2130"
  },
  {
    "objectID": "HISTORY.html#improvements-30",
    "href": "HISTORY.html#improvements-30",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nReduced memory consumption in the read path for multi-range reads. #2118\nCache non_empty_domain for REST arrays like all other arrays #2105\nAdd additional timer statistics for openning array for reads #2027\nAllow open array stats to be printed without read query #2131"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-34",
    "href": "HISTORY.html#bug-fixes-34",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixes a potential crash when opening an array with consolidated fragment metadata #2135\nCorrected a bug where sparse cells may be incorrectly returned using string dimensions. #2125\nAlways use original buffer size in serialized read queries serverside. #2115\nFix segfault in serialized queries when partition is unsplittable #2120"
  },
  {
    "objectID": "HISTORY.html#improvements-31",
    "href": "HISTORY.html#improvements-31",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdd additional stats printing to breakdown read state initialization timings #2095\nImprove GCS multipart locking #2087"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-35",
    "href": "HISTORY.html#bug-fixes-35",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix an edge-case where a read query may hang on array with string dimensions #2089\nFix mutex locking bugs on Windows due to unlocking on different thread and missing task join #2077\n\n\nC++ API\n\nAdd support for a string-typed, variable-sized, nullable attribute in the C++ API. #2090"
  },
  {
    "objectID": "HISTORY.html#new-features-19",
    "href": "HISTORY.html#new-features-19",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdd support for retrying REST requests that fail with certain http status code such as 503 #2060"
  },
  {
    "objectID": "HISTORY.html#improvements-32",
    "href": "HISTORY.html#improvements-32",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nParallelize across attributes when closing a write #2048\nSupport for dimension/attribute names that contain commonly reserved filesystem characters #2047\nRemove unnecessary is_dir in FragmentMetadata::store, this can increase performance for s3 writes #2050\nImprove S3 multipart locking #2055\nParallize loading fragments and array schema #2061"
  },
  {
    "objectID": "HISTORY.html#new-features-20",
    "href": "HISTORY.html#new-features-20",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nREST client support for caching redirects #1919"
  },
  {
    "objectID": "HISTORY.html#improvements-33",
    "href": "HISTORY.html#improvements-33",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdd rest.creation_access_credentials_name configuration parameter #2025"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-36",
    "href": "HISTORY.html#bug-fixes-36",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed ArrowAdapter export of string arrays with 64-bit offsets #2037\nFixed ArrowAdapter export of TILEDB_CHAR arrays with 64-bit offsets #2039"
  },
  {
    "objectID": "HISTORY.html#api-additions-12",
    "href": "HISTORY.html#api-additions-12",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdd tiledb_query_set_config to apply a tiledb_config_t to query-level parameters #2030\n\n\n\nC++ API\n\nAdded Query::set_config to apply a tiledb::Config to query-level parameters #2030"
  },
  {
    "objectID": "HISTORY.html#breaking-behavior-6",
    "href": "HISTORY.html#breaking-behavior-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior\n\nThe tile extent can now be set to null, in which case internally TileDB sets the extent to the dimension domain range. #1880\nThe C++ API std::pair&lt;uint64_t, uint64_t&gt; Query::est_result_size_var has been changed to 1) a return type of std::array&lt;uint64_t, 2&gt; and 2) returns the offsets as a size in bytes rather than elements. #1946"
  },
  {
    "objectID": "HISTORY.html#new-features-21",
    "href": "HISTORY.html#new-features-21",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nSupport for nullable attributes. #1895 #1938 #1948 #1945\nSupport for Hilbert order sorting for sparse arrays. #1880\nSupport for AWS S3 “AssumeRole” temporary credentials #1882\nSupport for zero-copy import/export with the Apache Arrow adapter #2001\nExperimental support for an in-memory backend used with bootstrap option “–enable-memfs” #1873\nSupport for element offsets when reading var-sized attributes. [#1897] (https://github.com/TileDB-Inc/TileDB/pull/1897)\nSupport for an extra offset indicating the size of the returned data when reading var-sized attributes. [#1932] (https://github.com/TileDB-Inc/TileDB/pull/1932)\nSupport for 32-bit offsets when reading var-sized attributes. [#1950] (https://github.com/TileDB-Inc/TileDB/pull/1950)"
  },
  {
    "objectID": "HISTORY.html#improvements-34",
    "href": "HISTORY.html#improvements-34",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nOptimized string dimension performance.\nAdded functionality to get fragment information from an array. #1900\nPrevented unnecessary sorting when (1) there is a single fragment and (i) either the query layout is global order, or (ii) the number of dimensions is 1, and (2) when there is a single range for which the result coordinates have already been sorted. #1880\nAdded extra stats for consolidation. #1880\nDisabled checking if cells are written in global order when consolidating, as it was redundant (the cells are already being read in global order during consolidation). #1880\nOptimize consolidated fragment metadata loading #1975"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-37",
    "href": "HISTORY.html#bug-fixes-37",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix tiledb_dimension_alloc returning a non-null pointer after error [#1959]((https://github.com/TileDB-Inc/TileDB/pull/1859)\nFixed issue with string dimensions and non-set subarray (which implies spanning the whole domain). There was an assertion being triggered. Now it works properly.\nFixed bug when checking the dimension domain for infinity or NaN values. #1880\nFixed bug with string dimension partitioning. #1880"
  },
  {
    "objectID": "HISTORY.html#api-additions-13",
    "href": "HISTORY.html#api-additions-13",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdded functions for getting fragment information. #1900\nAdded APIs for getting and setting ranges of queries using a dimension name. #1920\n\n\n\nC++ API\n\nAdded class FragmentInfo and functions for getting fragment information. #1900\nAdded function Dimension::create that allows not setting a space tile extent. #1880\nAdded APIs for getting and setting ranges of queries using a dimension name. #1920\nChanged std::pair&lt;uint64_t, uint64_t&gt; Query::est_result_size_var to std::array&lt;uint64_t, 2&gt; Query::est_result_size_var. Additionally, the size estimate for the offsets have been changed from elements to bytes. #1946"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-38",
    "href": "HISTORY.html#bug-fixes-38",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix deadlock in ThreadPool::wait_or_work #1994\nFix “[TileDB::ChunkedBuffer] Error: Cannot init chunk buffers; Total size must be non-zero.” in read path #1992"
  },
  {
    "objectID": "HISTORY.html#improvements-35",
    "href": "HISTORY.html#improvements-35",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nOptimize consolidated fragment metadata loading #1975\nOptimize Reader::load_tile_offsets for loading only relevant fragments #1976 #1983\nOptimize locking in FragmentMetadata::load_tile_offsets and FragmentMetadata::load_tile_var_offsets #1979\nExit early in Reader::copy_coordinates/Reader::copy_attribute_values when no results #1984"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-39",
    "href": "HISTORY.html#bug-fixes-39",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix segfault in optimized compute_results_sparse&lt;char&gt; #1969\nFix GCS “Error:: Read object failed”#1966\nFix segfault in ResultTile::str_coords_intersects #1981"
  },
  {
    "objectID": "HISTORY.html#improvements-36",
    "href": "HISTORY.html#improvements-36",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nOptimize ResultTile::compute_results_sparse&lt;char&gt; resulting in significant performance increases in certain cases with string dimensions #1963"
  },
  {
    "objectID": "HISTORY.html#improvements-37",
    "href": "HISTORY.html#improvements-37",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nOptimized string dimension performance. #1922"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-40",
    "href": "HISTORY.html#bug-fixes-40",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nUpdated the AWS SDK to v1.8.84 to fix an uncaught exception when using S3 #1899TileDB-Py #409\nFixed bug where a read on a sparse array may return duplicate values. #1905\nFixed bug where an array could not be opened if created with an array schema from an older version #1889\nFix compilation of TileDB Tools #1926"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-41",
    "href": "HISTORY.html#bug-fixes-41",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix ArraySchema not write protecting fill values for only schema version 6 or newer #1868\nFix segfault that may occur in the VFS read-ahead cache #1871"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-42",
    "href": "HISTORY.html#bug-fixes-42",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nThe result size estimatation routines will no longer return non-zero sizes that can not contain a single value. #1849\nFix serialization of dense writes that use ranges #1860\nFixed a crash from an unhandled SIGPIPE signal that may raise when using S3 #1856"
  },
  {
    "objectID": "HISTORY.html#breaking-behavior-7",
    "href": "HISTORY.html#breaking-behavior-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior\n\nEmpty dense arrays now return cells with fill values. Also the result estimator is adjusted to work properly with this new behavior."
  },
  {
    "objectID": "HISTORY.html#new-features-22",
    "href": "HISTORY.html#new-features-22",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdded configuration option “sm.compute_concurrency_level” #1766\nAdded configuration option “sm.io_concurrency_level” #1766\nAdded configuration option “sm.sub_partitioner_memory_budget” #1729\nAdded configuration option “vfs.read_ahead_size” #1785\nAdded configuration option “vfs.read_ahead_cache_size” #1785\nAdded support for getting/setting Apache Arrow buffers from a Query #1816"
  },
  {
    "objectID": "HISTORY.html#improvements-38",
    "href": "HISTORY.html#improvements-38",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nSource built curl only need HTTP support #1712\nAWS SDK version bumped to 1.8.6 #1718\nSplit posix permissions into files and folers permissions #1719\nSupport seeking for CURL to allow redirects for posting to REST #1728\nChanged default setting for vfs.s3.proxy_scheme from https to http to match common usage needs #1759\nEnabled parallelization with native system threads when TBB is disabled #1760\nSubarray ranges will be automatically coalesced as they are added #1755\nUpdate GCS SDK to v1.16.0 to fixes multiple bugs reported #1768\nRead-ahead cache for cloud-storage backends #1785\nAllow multiple empty values at the end of a variable-length write #1805\nBuild system will raise overridable error if important paths contain regex character #1808\nLazily create AWS ClientConfiguration to avoid slow context creations for non S3 usage after the AWS SDK version bump #1821\nMoved Status, ThreadPool, and Logger classes from folder tiledb/sm to tiledb/common #1843"
  },
  {
    "objectID": "HISTORY.html#deprecations-6",
    "href": "HISTORY.html#deprecations-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\nDeprecated config option “sm.num_async_threads” #1766\nDeprecated config option “sm.num_reader_threads” #1766\nDeprecated config option “sm.num_writer_threads” #1766\nDeprecated config option “vfs.num_threads” #1766\nSupport for MacOS older than 10.13 is being dropped when using the AWS SDK. Prebuilt Binaries now target 10.13 #1753\nUse of Intel’s Thread Building Blocks (TBB) will be discontinued in the future. It is now disabled by default #1762\nNo longer building release artifacts with Intel’s Thread Building Blocks (TBB) enabled #1825"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-43",
    "href": "HISTORY.html#bug-fixes-43",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed bug in setting a fill value for var-sized attributes.\nFixed a bug where the cpp headers would always produce compile-time warnings about using the deprecated c-api “tiledb_coords()” #1765\nOnly serialize the Array URI in the array schema client side. #1806\nFix C++ api consolidate_metadata function uses incorrect config #1841 #1844"
  },
  {
    "objectID": "HISTORY.html#api-additions-14",
    "href": "HISTORY.html#api-additions-14",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdded functions tiledb_attribute_{set,get}_fill_value to get/set default fill values\n\n\n\nC++ API\n\nAdded functions Attribute::{set,get}_fill_value to get/set default fill values"
  },
  {
    "objectID": "HISTORY.html#improvements-39",
    "href": "HISTORY.html#improvements-39",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nLazy initialization for GCS backend #1752\nAdd additional release artifacts which include disabling TBB #1753"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-44",
    "href": "HISTORY.html#bug-fixes-44",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix crash during GCS backend initialization due to upstream bug. #1752"
  },
  {
    "objectID": "HISTORY.html#improvements-40",
    "href": "HISTORY.html#improvements-40",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nVarious performance optimizations in the read path. #1689 #1692 #1693 #1694 #1695\nGoogle Cloud SDK bumped to 1.14. #1687, #1742"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-45",
    "href": "HISTORY.html#bug-fixes-45",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed error “Error: Out of bounds read to internal chunk buffer of size 65536” that may occur when writing var-sized attributes. #1732\nFixed error “Error: Chunk read error; chunk unallocated error” that may occur when reading arrays with more than one dimension. #1736\nFix Catch2 detection of system install #1733\nUse libtiledb-detected certificate path for Google Cloud Storage, for better linux binary/wheel portability. #1741\nFixed a small memory leak when opening arrays. #1690\nFixed an overflow in the partioning path that may result in a hang or poor read performance. #1725#1707\nFix compilation on gcc 10.1 for blosc #1740\nFixed a rare hang in the usage of load_tile_var_sizes. #1748"
  },
  {
    "objectID": "HISTORY.html#improvements-41",
    "href": "HISTORY.html#improvements-41",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdd new config option vfs.file.posix_permissions. #1710"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-46",
    "href": "HISTORY.html#bug-fixes-46",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nReturn possible env config variables in config iter #1714"
  },
  {
    "objectID": "HISTORY.html#improvements-42",
    "href": "HISTORY.html#improvements-42",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nDon’t include curl’s linking to libz, avoids build issue with double libz linkage #1682"
  },
  {
    "objectID": "HISTORY.html#improvements-43",
    "href": "HISTORY.html#improvements-43",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nFix typo in GCS cmake file for superbuild #1665\nDon’t error on GCS client init failure #1667\nDon’t include curl’s linking to ssl, avoids build issue on fresh macos 10.14/10.15 installs #1671\nHandle ubuntu’s cap’n proto package not providing cmake targets #1659"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-47",
    "href": "HISTORY.html#bug-fixes-47",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nThe C++ Attribute::create API now correctly builds from an STL array #1670\nAllow opening arrays with read-only permission on posix filesystems #1676\nFixed build issue caused by passing std::string to an Aws method #1678"
  },
  {
    "objectID": "HISTORY.html#improvements-44",
    "href": "HISTORY.html#improvements-44",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdd robust retries for S3 SLOW_DOWN errors #1651\nImprove GCS build process #1655\nAdd generation of pkg-config file #1656\nS3 should use HEADObject for file size #1657\nImprovements to stats #1652\nAdd artifacts to releases from CI #1663"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-48",
    "href": "HISTORY.html#bug-fixes-48",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nRemove to unneeded semicolons noticed by the -pedantic flag #1653\nFix cases were TILEDB_FORCE_ALL_DEPS picked up system builds #1654\nAllow errors to be show in cmake super build #1658\nProperly check vacuum files and limit fragment loading #1661\nFix edge case where consolidated but unvacuumed array can have coordinates report twice #1662"
  },
  {
    "objectID": "HISTORY.html#api-additions-15",
    "href": "HISTORY.html#api-additions-15",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nAdd c-api tiledb_stats_raw_dump[_str] function for raw stats dump #1660\nAdd c++-api Stats::raw_dump function for raw stats dump #1660"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-49",
    "href": "HISTORY.html#bug-fixes-49",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix hang on open array v1.6 #1645"
  },
  {
    "objectID": "HISTORY.html#improvements-45",
    "href": "HISTORY.html#improvements-45",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAllow empty values for variable length attributes #1646"
  },
  {
    "objectID": "HISTORY.html#improvements-46",
    "href": "HISTORY.html#improvements-46",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nRemove deprecated max buffer size APIs from unit tests #1625\nRemove deprecated max buffer API from examples #1626\nRemove zipped coords from examples #1632\nAllow AWSSDK_ROOT_DIR override #1637"
  },
  {
    "objectID": "HISTORY.html#deprecations-7",
    "href": "HISTORY.html#deprecations-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-50",
    "href": "HISTORY.html#bug-fixes-50",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nAllow setting zipped coords multiple times #1627\nFix overflow in check_tile_extent #1635\nFix C++ Dimension API {tile_extent,domain}_to_str. #1638\nRemove xlock in FragmentMetadata::store #1639"
  },
  {
    "objectID": "HISTORY.html#api-additions-16",
    "href": "HISTORY.html#api-additions-16",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions"
  },
  {
    "objectID": "HISTORY.html#disk-format-14",
    "href": "HISTORY.html#disk-format-14",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Disk Format",
    "text": "Disk Format\n\nRemoved file __coords.tdb that stored the zipped coordinates in sparse fragments\nNow storing the coordinate tiles on each dimension in separate files\nChanged fragment name format from __t1_t2_uuid to __t1_t2_uuid_&lt;format_version&gt;. That was necessary for backwards compatibility"
  },
  {
    "objectID": "HISTORY.html#breaking-c-api-changes-4",
    "href": "HISTORY.html#breaking-c-api-changes-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking C API changes",
    "text": "Breaking C API changes\n\nChanged domain input of tiledb_dimension_get_domain to const void** (from void**).\nChanged tile_extent input of tiledb_dimension_get_tile_extent to const void** (from void**).\nAnonymous attribute and dimensions (i.e., empty strings for attribute/dimension names) is no longer supported. This is because now the user can set separate dimension buffers to the query and, therefore, supporting anonymous attributes and dimensions creates ambiguity in the current API."
  },
  {
    "objectID": "HISTORY.html#breaking-behavior-8",
    "href": "HISTORY.html#breaking-behavior-8",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking behavior",
    "text": "Breaking behavior\n\nNow the TileDB consolidation process does not clean up the fragments or array metadata it consolidates. This is (i) to avoid exclusively locking at any point during consolidation, and (ii) to enable fine-grained time traveling even in the presence of consolidated fragments or array metadata. Instead, we added a special vacuuming API which explicitly cleans up consolidated fragments or array metadata (with appropriate configuration parameters). The vacuuming functions briefly exclusively lock the array."
  },
  {
    "objectID": "HISTORY.html#new-features-23",
    "href": "HISTORY.html#new-features-23",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdded string dimension support (currently only TILEDB_STRING_ASCII).\nThe user can now set separate coordinate buffers to the query. Also any subset of the dimensions is supported.\nThe user can set separate filter lists per dimension, as well as the number of values per coordinate."
  },
  {
    "objectID": "HISTORY.html#improvements-47",
    "href": "HISTORY.html#improvements-47",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdded support for AWS Security Token Service session tokens via configuration option vfs.s3.session_token. #1472\nAdded support for indicating zero-value metadata by returning value_num == 1 from the _get_metadatata and Array::get_metadata APIs #1438 (this is a non-breaking change, as the documented return of value == nullptr to indicate missing keys does not change)`\nUser can set coordinate buffers separately for write queries.\nAdded option to enable duplicate coordinates for sparse arrays #1504\nAdded support for writing at a timestamp by allowing opening an array at a timestamp (previously disabled).\nAdded special files with the same name as a fragment directory and an added suffix “.ok”, to indicate a committed fragment. This improved the performance of opening an array on object stores significantly, as it avoids an extra REST request per fragment.\nAdded functionality to consolidation, which allows consolidating the fragment metadata footers in a single file by toggling a new config parameter. This leads to a huge performance boost when opening an array, as it avoids fetching a separate footer per fragment from storage.\nVarious reader parallelizations that boosted read performance significantly.\nConfiguration parameters can now be read from environmental variables. vfs.s3.session_token -&gt; TILEDB_VFS_S3_SESSION_TOKEN. The prefix of TILEDB_ is configurable via config.env_var_prefix. #1600"
  },
  {
    "objectID": "HISTORY.html#deprecations-8",
    "href": "HISTORY.html#deprecations-8",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\nThe TileDB tiledb_array_consolidate_metadata and tiledb_array_consolidate_metadata_with_key C-API routines have been deprecated and will be removed entirely in a future release. The tiledb_array_consolidate and tiledb_array_consolidate_with_key routines achieve the same behavior when the “sm.consolidation.mode” parameter of the configuration argument is equivalent to “array_meta”.\nThe TileDB Array::consolidate_metadata CPP-API routine has been deprecated and will be removed entirely in a future release. The Array::consolidate routine achieves the same behavior when the “sm.consolidation.mode” parameter of the configuration argument is equivalent to “array_meta”."
  },
  {
    "objectID": "HISTORY.html#bug-fixes-51",
    "href": "HISTORY.html#bug-fixes-51",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed bug in dense consolidation when the array domain is not divisible by the tile extents."
  },
  {
    "objectID": "HISTORY.html#api-additions-17",
    "href": "HISTORY.html#api-additions-17",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nAdded C API function tiledb_array_has_metadata_key and C++ API function Array::has_metadata_key #1439\nAdded C API functions tiledb_array_schema_{set,get}_allows_dups and C++ API functions Array::set_allows_dups and Array::allows_dups\nAdded C API functions tiledb_dimension_{set,get}_filter_list and tiledb_dimension_{set,get}_cell_val_num\nAdded C API functions tiledb_array_get_non_empty_domain_from_{index,name}\nAdded C API function tiledb_array_vacuum\nAdded C API functions tiledb_array_get_non_empty_domain_var_size_from_{index,name}\nAdded C API functions tiledb_array_get_non_empty_domain_var_from_{index,name}\nAdded C API function tiledb_array_add_range_var\nAdded C API function tiledb_array_get_range_var_size\nAdded C API function tiledb_array_get_range_var\nAdded C++ API functions Dimension::set_cell_val_num and Dimension::cell_val_num.\nAdded C++ API functions Dimension::set_filter_list and Dimension::filter_list.\nAdded C++ API functions Array::non_empty_domain(unsigned idx) and Array::non_empty_domain(const std::string& name).\nAdded C++ API functions Domain::dimension(unsigned idx) and Domain::dimension(const std::string& name).\nAdded C++ API function Array::load_schema(ctx, uri) and Array::load_schema(ctx, uri, key_type, key, key_len).\nAdded C++ API function Array::vacuum.\nAdded C++ API functions Array::non_empty_domain_var (from index and name).\nAdded C++ API function add_range with string inputs.\nAdded C++ API function range with string outputs.\nAdded C++ API functions Array and Context constructors which take a c_api object to wrap. #1623"
  },
  {
    "objectID": "HISTORY.html#api-removals",
    "href": "HISTORY.html#api-removals",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API removals",
    "text": "API removals"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-52",
    "href": "HISTORY.html#bug-fixes-52",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix expanded domain consolidation #1572"
  },
  {
    "objectID": "HISTORY.html#new-features-24",
    "href": "HISTORY.html#new-features-24",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdd MD5 and SHA256 checksum filters #1515"
  },
  {
    "objectID": "HISTORY.html#improvements-48",
    "href": "HISTORY.html#improvements-48",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdded support for AWS Security Token Service session tokens via configuration option vfs.s3.session_token. #1472"
  },
  {
    "objectID": "HISTORY.html#deprecations-9",
    "href": "HISTORY.html#deprecations-9",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-53",
    "href": "HISTORY.html#bug-fixes-53",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix new SHA1 for intel TBB in superbuild due to change in repository name #1551"
  },
  {
    "objectID": "HISTORY.html#api-additions-18",
    "href": "HISTORY.html#api-additions-18",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions"
  },
  {
    "objectID": "HISTORY.html#api-removals-1",
    "href": "HISTORY.html#api-removals-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API removals",
    "text": "API removals"
  },
  {
    "objectID": "HISTORY.html#new-features-25",
    "href": "HISTORY.html#new-features-25",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features"
  },
  {
    "objectID": "HISTORY.html#improvements-49",
    "href": "HISTORY.html#improvements-49",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAvoid useless serialization of Array Metadata on close #1485\nUpdate CONTRIBUTING and Code of Conduct #1487"
  },
  {
    "objectID": "HISTORY.html#deprecations-10",
    "href": "HISTORY.html#deprecations-10",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-54",
    "href": "HISTORY.html#bug-fixes-54",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix deadlock in writes of TileDB Cloud Arrays #1486"
  },
  {
    "objectID": "HISTORY.html#api-additions-19",
    "href": "HISTORY.html#api-additions-19",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions"
  },
  {
    "objectID": "HISTORY.html#api-removals-2",
    "href": "HISTORY.html#api-removals-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API removals",
    "text": "API removals"
  },
  {
    "objectID": "HISTORY.html#new-features-26",
    "href": "HISTORY.html#new-features-26",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features"
  },
  {
    "objectID": "HISTORY.html#improvements-50",
    "href": "HISTORY.html#improvements-50",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nREST requests now will use http compression if available #1479"
  },
  {
    "objectID": "HISTORY.html#deprecations-11",
    "href": "HISTORY.html#deprecations-11",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-55",
    "href": "HISTORY.html#bug-fixes-55",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes"
  },
  {
    "objectID": "HISTORY.html#api-additions-20",
    "href": "HISTORY.html#api-additions-20",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions"
  },
  {
    "objectID": "HISTORY.html#api-removals-3",
    "href": "HISTORY.html#api-removals-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API removals",
    "text": "API removals"
  },
  {
    "objectID": "HISTORY.html#new-features-27",
    "href": "HISTORY.html#new-features-27",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features"
  },
  {
    "objectID": "HISTORY.html#improvements-51",
    "href": "HISTORY.html#improvements-51",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nArray metadata fetching is now lazy (fetch on use) to improve array open performance #1466\nlibtiledb on Linux will no longer re-export symbols from statically linked dependencies #1461"
  },
  {
    "objectID": "HISTORY.html#deprecations-12",
    "href": "HISTORY.html#deprecations-12",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-56",
    "href": "HISTORY.html#bug-fixes-56",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes"
  },
  {
    "objectID": "HISTORY.html#api-additions-21",
    "href": "HISTORY.html#api-additions-21",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions"
  },
  {
    "objectID": "HISTORY.html#api-removals-4",
    "href": "HISTORY.html#api-removals-4",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API removals",
    "text": "API removals"
  },
  {
    "objectID": "HISTORY.html#new-features-28",
    "href": "HISTORY.html#new-features-28",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features"
  },
  {
    "objectID": "HISTORY.html#improvements-52",
    "href": "HISTORY.html#improvements-52",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdded support for getting/setting array metadata via REST. #1449"
  },
  {
    "objectID": "HISTORY.html#deprecations-13",
    "href": "HISTORY.html#deprecations-13",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-57",
    "href": "HISTORY.html#bug-fixes-57",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed several REST query and deserialization bugs. #1433, #1437, #1440, #1444\nFixed bug in setting certificate path on Linux for the REST client. #1452"
  },
  {
    "objectID": "HISTORY.html#api-additions-22",
    "href": "HISTORY.html#api-additions-22",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions"
  },
  {
    "objectID": "HISTORY.html#api-removals-5",
    "href": "HISTORY.html#api-removals-5",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API removals",
    "text": "API removals"
  },
  {
    "objectID": "HISTORY.html#new-features-29",
    "href": "HISTORY.html#new-features-29",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features"
  },
  {
    "objectID": "HISTORY.html#improvements-53",
    "href": "HISTORY.html#improvements-53",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements"
  },
  {
    "objectID": "HISTORY.html#deprecations-14",
    "href": "HISTORY.html#deprecations-14",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-58",
    "href": "HISTORY.html#bug-fixes-58",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed bug in dense consolidation when the array domain is not divisible by the tile extents. #1442"
  },
  {
    "objectID": "HISTORY.html#api-additions-23",
    "href": "HISTORY.html#api-additions-23",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nAdded C API function tiledb_array_has_metadata_key and C++ API function Array::has_metadata_key #1439\nAdded support for indicating zero-value metadata by returning value_num == 1 from the _get_metadatata and Array::get_metadata APIs #1438 (this is a non-breaking change, as the documented return of value == nullptr to indicate missing keys does not change)`"
  },
  {
    "objectID": "HISTORY.html#api-removals-6",
    "href": "HISTORY.html#api-removals-6",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API removals",
    "text": "API removals"
  },
  {
    "objectID": "HISTORY.html#new-features-30",
    "href": "HISTORY.html#new-features-30",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdded array metadata. #1377"
  },
  {
    "objectID": "HISTORY.html#improvements-54",
    "href": "HISTORY.html#improvements-54",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAllow writes to older-versioned arrays. #1417\nAdded overseen optimization to check the fragment non-empty domain before loading the fragment R-Tree. #1395\nUse major.minor for SOVERSION instead of full major.minor.rev. #1398"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-59",
    "href": "HISTORY.html#bug-fixes-59",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nNumerous query serialization bugfixes and performance improvements.\nNumerous tweaks to build strategy for libcurl dependency.\nFix crash in StorageManager destructor when GlobalState init fails. #1393\nFix Windows destructor crash due to missing unlock (mutex/refcount). #1400\nNormalize attribute names in multi-range size estimation C API. #1408"
  },
  {
    "objectID": "HISTORY.html#api-additions-24",
    "href": "HISTORY.html#api-additions-24",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nAdded C API functions tiledb_query_get_{fragment_num,fragment_uri,fragment_timestamp_range}. #1396\nAdded C++ API functions Query::{fragment_num,fragment_uri,fragment_timestamp_range}. #1396\nAdded C API function tiledb_ctx_set_tag and C++ API Context::set_tag(). #1406\nAdd config support for S3 ca_path, ca_file, and verify_ssl options. #1376"
  },
  {
    "objectID": "HISTORY.html#api-removals-7",
    "href": "HISTORY.html#api-removals-7",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API removals",
    "text": "API removals\n\nRemoved key-value functionality, tiledb_kv_* functions from the C API and Map and MapSchema from the C++ API. #1415"
  },
  {
    "objectID": "HISTORY.html#additions",
    "href": "HISTORY.html#additions",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Additions",
    "text": "Additions\n\nAdded config param vfs.s3.logging_level. #1236"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-60",
    "href": "HISTORY.html#bug-fixes-60",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed FP slice point-query with small (eps) gap coordinates. #1384\nFixed several unused variable warnings in unit tests. #1385\nFixed missing include in subarray.h. #1374\nFixed missing virtual destructor in C++ API schema.h. #1391\nFixed C++ API build error with clang regarding deleted default constructors. #1394"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-61",
    "href": "HISTORY.html#bug-fixes-61",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix incorrect version number listed in tiledb_version.h header file and doc page.\nFix issue with release notes from 1.6.0 release. #1359"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-62",
    "href": "HISTORY.html#bug-fixes-62",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nBug fix in incomplete query behavior. #1358"
  },
  {
    "objectID": "HISTORY.html#new-features-31",
    "href": "HISTORY.html#new-features-31",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdded support for multi-range reads (non-continuous range slicing) for dense and sparse arrays.\nAdded support for datetime domains and attributes."
  },
  {
    "objectID": "HISTORY.html#improvements-55",
    "href": "HISTORY.html#improvements-55",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nRemoved fragment metadata caching. #1197\nRemoved array schema caching. #1197\nThe tile MBR in the in-memory fragment metadata are organized into an R-Tree, speeding up tile overlap operations during subarray reads. #1197\nImproved encryption key validation process when opening already open arrays. Fixes issue with indefinite growing of the URI to encryption key mapping in StorageManager (the mapping is no longer needed). #1197\nImproved dense write performance in some benchmarks. #1229\nSupport for direct writes without using the S3 multi-part API. Allows writing to Google Cloud Storage S3 compatibility mode. #1219\nRemoved 256-character length limit from URIs. #1288\nDense reads and writes now always require a subarray to be set, to avoid confusion. #1320\nAdded query and array schema serialization API. #1262"
  },
  {
    "objectID": "HISTORY.html#deprecations-15",
    "href": "HISTORY.html#deprecations-15",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\nThe TileDB KV API has been deprecated and will be removed entirely in a future release. The KV mechanism will be removed when full support for string-valued dimensions has been added."
  },
  {
    "objectID": "HISTORY.html#bug-fixes-63",
    "href": "HISTORY.html#bug-fixes-63",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nBug fix with amplification factor in consolidation. #1275\nFixed thread safety issue in opening arrays. #1252\nFixed floating point exception when writing fixed-length attributes with a large cell value number. #1289\nFixed off-by-one limitation with floating point dimension tile extents. #1314"
  },
  {
    "objectID": "HISTORY.html#api-additions-25",
    "href": "HISTORY.html#api-additions-25",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdded functions tiledb_query_{get_est_result_size, get_est_result_size_var, add_range, get_range_num, get_range}.\nAdded function tiledb_query_get_layout\nAdded datatype tiledb_buffer_t and functions tiledb_buffer_{alloc,free,get_type,set_type,get_data,set_data}.\nAdded datatype tiledb_buffer_list_t and functions tiledb_buffer_list_{alloc,free,get_num_buffers,get_total_size,get_buffer,flatten}.\nAdded string conversion functions tiledb_*_to_str() and tiledb_*_from_str() for all public enum types.\nAdded config param vfs.file.enable_filelocks\nAdded datatypes TILEDB_DATETIME_*\nAdded function tiledb_query_get_array\n\n\n\nC++ API\n\nAdded functions Query::{query_layout, add_range, range, range_num, array}. ## Breaking changes\n\n\n\nC API\n\nRemoved ability to set null tile extents on dimensions. All dimensions must now have an explicit tile extent.\n\n\n\nC++ API\n\nRemoved cast operators of C++ API objects to their underlying C API objects. This helps prevent inadvertent memory issues such as double-frees.\nRemoved ability to set null tile extents on dimensions. All dimensions must now have an explicit tile extent.\nChanged argument config in Array::consolidate() from a const-ref to a pointer.\nRemoved default includes of Map and MapSchema. To use the deprecated KV API temporarily, include &lt;tiledb/map.h&gt; explicitly."
  },
  {
    "objectID": "HISTORY.html#improvements-56",
    "href": "HISTORY.html#improvements-56",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nBetter handling of {C,CXX}FLAGS during the build. #1209\nUpdate libcurl dependency to v7.64.1 for S3 builds. #1240"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-64",
    "href": "HISTORY.html#bug-fixes-64",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nS3 SDK build error fix. #1201\nFixed thread safety issue with ZStd compressor. #1208\nFixed crash in consolidation due to accessing invalid entry #1213\nFixed memory leak in C++ KV API. #1247\nFixed minor bug when writing in global order with empty buffers. #1248"
  },
  {
    "objectID": "HISTORY.html#new-features-32",
    "href": "HISTORY.html#new-features-32",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAdded an advanced, tunable consolidation algorithm. #1101"
  },
  {
    "objectID": "HISTORY.html#improvements-57",
    "href": "HISTORY.html#improvements-57",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nSmall tiles are now batched for larger VFS read operations, improving read performance in some cases. #1093\nPOSIX error messages are included in log messages. #1076\nAdded tiledb command-line tool with several supported actions. #1081\nAdded build flag to disable internal statistics. #1111\nImproved memory overhead slightly by lazily allocating memory before checking the tile cache. #1141\nImproved tile cache utilization by removing erroneous use of cache for metadata. #1151\nS3 multi-part uploads are aborted on error. #1166"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-65",
    "href": "HISTORY.html#bug-fixes-65",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nBug fix when reading from a sparse array with real domain. Also added some checks on NaN and INF. #1100\nFixed C++ API functions group_by_cell and ungroup_var_buffer to treat offsets in units of bytes. #1047\nSeveral HDFS test build errors fixed. #1092\nFixed incorrect indexing in parallel_for. #1105\nFixed incorrect filter statistics counters. #1112\nPreserve anonymous attributes in ArraySchema copy constructor. #1144\nFix non-virtual destructors in C++ API. #1153\nAdded zlib dependency to AWS SDK EP. #1165\nFixed a hang in the ‘S3::ls()’. #1183\nMany other small and miscellaneous bug fixes."
  },
  {
    "objectID": "HISTORY.html#api-additions-26",
    "href": "HISTORY.html#api-additions-26",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdded function tiledb_vfs_dir_size.\nAdded function tiledb_vfs_ls.\nAdded config params vfs.max_batch_read_size and vfs.max_batch_read_amplification.\nAdded functions tiledb_{array,kv}_encryption_type.\nAdded functions tiledb_stats_{dump,free}_str.\nAdded function tiledb_{array,kv}_schema_has_attribute.\nAdded function tiledb_domain_has_dimension.\n\n\n\nC++ API\n\n{Array,Map}::consolidate{_with_key} now takes a Config as an optional argument.\nAdded function VFS::dir_size.\nAdded function VFS::ls.\nAdded {Array,Map}::encryption_type().\nAdded {ArraySchema,MapSchema}::has_attribute()\nAdded Domain::has_dimension()\nAdded constructor overloads for Array and Map to take a std::string encryption key.\nAdded overloads for {Array,Map}::{open,create,consolidate} to take a std::string encryption key.\nAdded untyped overloads for Query::set_buffer()."
  },
  {
    "objectID": "HISTORY.html#breaking-changes",
    "href": "HISTORY.html#breaking-changes",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking changes",
    "text": "Breaking changes\n\nC API\n\nDeprecated tiledb_compressor_t APIs from v1.3.x have been removed, replaced by the tiledb_filter_list API. #1128\ntiledb_{array,kv}_consolidate{_with_key} now takes a tiledb_config_t* as argument.\n\n\n\nC++ API\n\nDeprecated tiledb::Compressor APIs from v1.3.x have been removed, replaced by the FilterList API. #1128"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-66",
    "href": "HISTORY.html#bug-fixes-66",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed support for config parameter values sm.num_reader_threads and `sm.num_writer_threads. User-specified values had been ignored for these parameters. #1096\nFixed GCC 7 linker errors. #1091\nBug fix in the case of dense reads in the presence of both dense and sparse fragments. #1079\nFixed double-delta decompression bug on reads for uncompressible chunks. #1074\nFixed unnecessary linking of shared zlib when using TileDB superbuild. #1125"
  },
  {
    "objectID": "HISTORY.html#improvements-58",
    "href": "HISTORY.html#improvements-58",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdded lazy creation of S3 client instance on first request. #1084\nAdded config params vfs.s3.aws_access_key_id and vfs.s3.aws_secret_access_key for configure s3 access at runtime. #1036\nAdded missing check if coordinates obey the global order in global order sparse writes. #1039"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-67",
    "href": "HISTORY.html#bug-fixes-67",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed bug in incomplete queries, which should always return partial results. An incomplete status with 0 returned results must always mean that the buffers cannot even fit a single cell value. #1056\nFixed performance bug during global order write finalization. #1065\nFixed error in linking against static TileDB on Windows. #1058\nFixed build error when building without TBB. #1051"
  },
  {
    "objectID": "HISTORY.html#improvements-59",
    "href": "HISTORY.html#improvements-59",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nSet LZ4, Zlib and Zstd compressors to build in release mode. #1034\nChanged coordinates to always be split before filtering. #1054\nAdded type-safe filter option methods to C++ API. #1062"
  },
  {
    "objectID": "HISTORY.html#new-features-33",
    "href": "HISTORY.html#new-features-33",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nAll array data can now be encrypted at rest using AES-256-GCM symmetric encryption. #968\nNegative and real-valued domain types are now fully supported. #885\nNew filter API for transforming attribute data with an ordered list of filters. #912\nCurrent filters include: previous compressors, bit width reduction, bitshuffle, byteshuffle, and positive-delta encoding.\n\nThe bitshuffle filter uses an implementation by Kiyoshi Masui.\nThe byteshuffle filter uses an implementation by Francesc Alted (from the Blosc project).\n\nArrays can now be opened at specific timestamps. #984"
  },
  {
    "objectID": "HISTORY.html#deprecations-16",
    "href": "HISTORY.html#deprecations-16",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Deprecations",
    "text": "Deprecations\n\nThe C and C++ APIs for compression have been deprecated. The corresponding filter API should be used instead. The compression API will be removed in a future TileDB version. #1008\nRemoved Blosc compressors (obviated by byteshuffle -&gt; compressor filter list)."
  },
  {
    "objectID": "HISTORY.html#bug-fixes-68",
    "href": "HISTORY.html#bug-fixes-68",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix issue where performing a read query with empty result could cause future reads to return empty #882\nFix TBB initialization bug with multiple contexts #898\nFix bug in max buffer sizes estimation #903\nFix Buffer allocation size being incorrectly set on realloc #911"
  },
  {
    "objectID": "HISTORY.html#improvements-60",
    "href": "HISTORY.html#improvements-60",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdded check if the coordinates fall out-of-bounds (i.e., outside the array domain) during sparse writes, and added config param sm.check_coord_oob to enable/disable the check (enabled by default). #996\nAdd config params sm.num_reader_threads and sm.num_writer_threads for separately controlling I/O parallelism from compression parallelism.\nAdded contribution guidelines #899\nEnable building TileDB in Cygwin environment on Windows #890\nAdded a simple benchmarking script and several benchmark programs #889\nChanged C API and disk format integer types to have explicit bit widths. #981"
  },
  {
    "objectID": "HISTORY.html#api-additions-27",
    "href": "HISTORY.html#api-additions-27",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdded tiledb_{array,kv}_open_at, tiledb_{array,kv}_open_at_with_key and tiledb_{array,kv}_reopen_at.\nAdded tiledb_{array,kv}_get_timestamp().\nAdded tiledb_kv_is_open\nAdded tiledb_filter_t tiledb_filter_type_t, tiledb_filter_option_t, and tiledb_filter_list_t types\nAdded tiledb_filter_* and tiledb_filter_list_* functions.\nAdded tiledb_attribute_{set,get}_filter_list, tiledb_array_schema_{set,get}_coords_filter_list, tiledb_array_schema_{set,get}_offsets_filter_list functions.\nAdded tiledb_query_get_buffer and tiledb_query_get_buffer_var.\nAdded tiledb_array_get_uri\nAdded tiledb_encryption_type_t\nAdded tiledb_array_create_with_key, tiledb_array_open_with_key, tiledb_array_schema_load_with_key, tiledb_array_consolidate_with_key\nAdded tiledb_kv_create_with_key, tiledb_kv_open_with_key, tiledb_kv_schema_load_with_key, tiledb_kv_consolidate_with_key\n\n\n\nC++ API\n\nAdded encryption overloads for Array(), Array::open(), Array::create(), ArraySchema(), Map(), Map::open(), Map::create() and MapSchema().\nAdded Array::timestamp() and Array::reopen_at() methods.\nAdded Filter and FilterList classes\nAdded Attribute::filter_list(), Attribute::set_filter_list(), ArraySchema::coords_filter_list(), ArraySchema::set_coords_filter_list(), ArraySchema::offsets_filter_list(), ArraySchema::set_offsets_filter_list() functions.\nAdded overloads for Array(), Array::open(), Map(), Map::open() for handling timestamps."
  },
  {
    "objectID": "HISTORY.html#breaking-changes-1",
    "href": "HISTORY.html#breaking-changes-1",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking changes",
    "text": "Breaking changes\n\nC API\n\nRemoved Blosc compressors.\nRemoved tiledb_kv_set_max_buffered_items.\nModified tiledb_kv_open to not take an attribute subselection, but instead take as input the query type (similar to arrays). This makes the key-value store behave similarly to arrays, which means that the key-value store does not support interleaved reads/writes any more (an opened key-value store is used either for reads or writes, but not both).\ntiledb_kv_close does not flush the written items. Instead, tiledb_kv_flush must be invoked explicitly.\n\n\n\nC++ API\n\nRemoved Blosc compressors.\nRemoved Map::set_max_buffered_items.\nModified Map::Map and Map::open to not take an attribute subselection, but instead take as input the query type (similar to arrays). This makes the key-value store behave similarly to arrays, which means that the key-value store does not support interleaved reads/writes any more (an opened key-value store is used either for reads or writes, but not both).\nMap::close does not flush the written items. Instead, Map::flush must be invoked explicitly."
  },
  {
    "objectID": "HISTORY.html#bug-fixes-69",
    "href": "HISTORY.html#bug-fixes-69",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix read query bug from multiple fragments when query layout differs from array layout #869\nFix error when consolidating empty arrays #861\nFix bzip2 external project URL #875\nInvalidate cached buffer sizes when query subarray changes #882"
  },
  {
    "objectID": "HISTORY.html#improvements-61",
    "href": "HISTORY.html#improvements-61",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdd check to ensure tile extent greater than zero #866\nAdd TILEDB_INSTALL_LIBDIR CMake option #858\nRemove TILEDB_USE_STATIC_* CMake variables from build #871\nAllow HDFS init to succeed even if libhdfs is not found #873"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-70",
    "href": "HISTORY.html#bug-fixes-70",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nAdd missing checks when setting subarray for sparse writes #843\nFix dl linking build issue for C/C++ examples on Linux #844\nAdd missing type checks for C++ api Query object #845\nAdd missing check that coordinates are provided for sparse writes #846"
  },
  {
    "objectID": "HISTORY.html#improvements-62",
    "href": "HISTORY.html#improvements-62",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nFixes to compile on llvm v3.5 #831\nAdd option disable building unittests #836"
  },
  {
    "objectID": "HISTORY.html#new-features-34",
    "href": "HISTORY.html#new-features-34",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nNew guided tutorial series added to documentation.\nQuery times improved dramatically with internal parallelization using TBB (multiple PRs)\nOptional deduplication pass on writes can be enabled #636\nNew internal statistics reporting system to aid in performance optimization #736\nAdded string type support: ASCII, UTF-8, UTF-16, UTF-32, UCS-2, UCS-4 #415\nAdded TILEDB_ANY datatype #446\nAdded parallelized VFS read operations, enabled by default #499\nSIGINT signals will cancel in-progress queries #578"
  },
  {
    "objectID": "HISTORY.html#improvements-63",
    "href": "HISTORY.html#improvements-63",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nArrays must now be open and closed before issuing queries, which clarifies the TileDB consistency model.\nImproved handling of incomplete queries and variable-length attribute data.\nAdded parallel S3, POSIX, and Win32 reads and writes, enabled by default.\nQuery performance improvements with parallelism (using TBB as a dependency).\nGot rid of special S3 “directory objects.”\nRefactored sparse reads, making them simpler and more amenable to parallelization.\nRefactored dense reads, making them simpler and more amenable to parallelization.\nRefactored dense ordered writes, making them simpler and more amenable to parallelization.\nRefactored unordered writes, making them simpler and more amenable to parallelization.\nRefactored global writes, making them simpler and more amenable to parallelization.\nAdded ability to cancel pending background/async tasks. SIGINT signals now cancel pending tasks.\nAsync queries now use a configurable number of background threads (default number of threads is 1).\nAdded checks for duplicate coordinates and option for coordinate deduplication.\nMap usage via the C++ API operator[] is faster, similar to the MapItem path."
  },
  {
    "objectID": "HISTORY.html#bug-fixes-71",
    "href": "HISTORY.html#bug-fixes-71",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nFixed bugs with reads/writes of variable-sized attributes.\nFixed file locking issue with simultaneous queries.\nFixed S3 issues with simultaneous queries within the same context."
  },
  {
    "objectID": "HISTORY.html#api-additions-28",
    "href": "HISTORY.html#api-additions-28",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "API additions",
    "text": "API additions\n\nC API\n\nAdded tiledb_array_alloc\nAdded tiledb_array_{open, close, free}\nAdded tiledb_array_reopen\nAdded tiledb_array_is_open\nAdded tiledb_array_get_query_type\nAdded tiledb_array_get_schema\nAdded tiledb_array_max_buffer_size and tiledb_array_max_buffer_size_var\nAdded tiledb_query_finalize function.\nAdded tiledb_ctx_cancel_tasks function.\nAdded tiledb_query_set_buffer and tiledb_query_set_buffer_var which sets a single attribute buffer\nAdded tiledb_query_get_type\nAdded tiledb_query_has_results\nAdded tiledb_vfs_get_config function.\nAdded tiledb_stats_{enable,disable,reset,dump} functions.\nAdded tiledb_kv_alloc\nAdded tiledb_kv_reopen\nAdded tiledb_kv_has_key to check if a key exists in the key-value store.\nAdded tiledb_kv_free.\nAdded tiledb_kv_iter_alloc which takes as input a kv object\nAdded tiledb_kv_schema_{set,get}_capacity.\nAdded tiledb_kv_is_dirty\nAdded tiledb_kv_iter_reset\nAdded sm.num_async_threads, sm.num_tbb_threads, and sm.enable_signal_handlers config parameters.\nAdded sm.check_dedup_coords and sm.dedup_coords config parameters.\nAdded vfs.num_threads and vfs.min_parallel_size config parameters.\nAdded vfs.{s3,file}.max_parallel_ops config parameters.\nAdded vfs.s3.multipart_part_size config parameter.\nAdded vfs.s3.proxy_{scheme,host,port,username,password} config parameters.\n\n\n\nC++ API\n\nAdded Array::{open, close}\nAdded Array::reopen\nAdded Array::is_open\nAdded Array::query_type\nAdded Context::cancel_tasks() function.\nAdded Query::finalize() function.\nAdded Query::query_type\nAdded Query::has_results\nChanged the return type of the Query setters to return the object reference.\nAdded an extra Query constructor that omits the query type (this is inherited from the input array).\nAdded Map::{open, close}\nAdded Map::reopen\nAdded Map::is_dirty\nAdded Map::has_key to check for key presence.\nA tiledb::Map defined with only one attribute will allow implicit usage, e.x. map[key] = val instead of map[key][attr] = val.\nAdded optional attributes argument in Map::Map and Map::open\nMapIter can be used to create iterators for a map.\nAdded MapIter::reset\nAdded MapSchema::set_capacity and MapSchema::capacity\nSupport for trivially copyable objects, such as a custom data struct, was added. They will be backed by an sizeof(T) sized char attribute.\nAttribute::create&lt;T&gt; can now be used with compound T, such as std::string and std::vector&lt;T&gt;, and other objects such as a simple data struct.\nAdded a Dimension::create factory function that does not take tile extent, which sets the tile extent to NULL.\ntiledb::Attribute can now be constructed with an enumerated type (e.x. TILEDB_CHAR).\nAdded Stats class (wraps C API tiledb_stats_* functions)\nAdded Config::save_to_file"
  },
  {
    "objectID": "HISTORY.html#breaking-changes-2",
    "href": "HISTORY.html#breaking-changes-2",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking changes",
    "text": "Breaking changes\n\nC API\n\ntiledb_query_finalize must always be called before tiledb_query_free after global-order writes.\nRemoved tiledb_vfs_move and added tiledb_vfs_move_file and tiledb_vfs_move_dir instead.\nRemoved force argument from tiledb_vfs_move_* and tiledb_object_move.\nRemoved vfs.s3.file_buffer_size config parameter.\nRemoved tiledb_query_get_attribute_status.\nAll tiledb_*_free functions now return void and do not take ctx as input (except for tiledb_ctx_free).\nChanged signature of tiledb_kv_close to take a tiledb_kv_t* argument instead of tiledb_kv_t**.\nRenamed tiledb_domain_get_rank to tiledb_domain_get_ndim to avoid confusion with matrix def of rank.\nChanged signature of tiledb_array_get_non_empty_domain.\nRemoved tiledb_array_compute_max_read_buffer_sizes.\nChanged signature of tiledb_{array,kv}_open.\nRemoved tiledb_kv_iter_create\nRenamed all C API functions that create TileDB objects from tiledb_*_create to tiledb_*_alloc.\nRemoved tiledb_query_set_buffers\nRemoved tiledb_query_reset_buffers\nAdded query type argument to tiledb_array_open\nChanged argument order in tiledb_config_iter_alloc, tiledb_ctx_alloc, tiledb_attribute_alloc, tiledb_dimension_alloc, tiledb_array_schema_alloc, tiledb_kv_schema_load, tiledb_kv_get_item, tiledb_vfs_alloc\n\n\n\nC++ API\n\nFixes with Array::max_buffer_elements and Query::result_buffer_elements to comply with the API docs. pair.first is the number of elements of the offsets buffer. If pair.first is 0, it is a fixed-sized attribute or coordinates.\nstd::array&lt;T, N&gt; is backed by a char tiledb attribute since the size is not guaranteed.\nHeaders have the tiledb_cpp_api_ prefix removed. For example, the include is now #include &lt;tiledb/attribute.h&gt;\nRemoved VFS::move and added VFS::move_file and VFS::move_dir instead.\nRemoved force argument from VFS::move_* and Object::move.\nRemoved vfs.s3.file_buffer_size config parameter.\nQuery::finalize must always be called before going out of scope after global-order writes.\nRemoved Query::attribute_status.\nThe API was made header only to improve cross-platform compatibility. config_iter.h, filebuf.h, map_item.h, map_iter.h, and map_proxy.h are no longer available, but grouped into the headers of the objects they support.\nPreviously a tiledb::Map could be created from a std::map, an anonymous attribute name was defined. This must now be explicitly defined: tiledb::Map::create(tiledb::Context, std::string uri, std::map, std::string attr_name)\nRemoved tiledb::Query::reset_buffers. Any previous usages can safely be removed.\nMap::begin refers to the same iterator object. For multiple concurrent iterators, a MapIter should be manually constructed instead of using Map::begin() more than once.\nRenamed Domain::rank to Domain::ndim to avoid confusion with matrix def of rank.\nAdded query type argument to Array constructor\nRemoved iterator functionality from Map.\nRemoved Array::parition_subarray."
  },
  {
    "objectID": "HISTORY.html#bug-fixes-72",
    "href": "HISTORY.html#bug-fixes-72",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFix I/O bug on POSIX systems with large reads/writes (#467)\nMemory overflow error handling (moved from constructors to init functions) (#472)\nMemory leaks with realloc in case of error (#472)\nHandle non-existent config param in C++ API (#475)\nRead query overflow handling (#485)"
  },
  {
    "objectID": "HISTORY.html#improvements-64",
    "href": "HISTORY.html#improvements-64",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nChanged S3 default config so that AWS S3 just works (#455)\nMinor S3 optimizations and error message fixes (#462)\nDocumentation additions including S3 usage (#456, #458, #459)\nVarious CI improvements (#449)"
  },
  {
    "objectID": "HISTORY.html#bug-fixes-73",
    "href": "HISTORY.html#bug-fixes-73",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nFixed TileDB header includes for all examples (#409)\nFixed TileDB library dynamic linking problem for C++ API (#412)\nFixed VS2015 build errors (#424)\nBug fix in the sparse case (#434)\nBug fix for 1D vector query layout (#438)"
  },
  {
    "objectID": "HISTORY.html#improvements-65",
    "href": "HISTORY.html#improvements-65",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nAdded documentation to API and examples (#410, #414)\nMigrated docs to Readthedocs (#418, #420, #422, #423, #425)\nAdded dimension domain/tile extent checks (#429)"
  },
  {
    "objectID": "HISTORY.html#new-features-35",
    "href": "HISTORY.html#new-features-35",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "New features",
    "text": "New features\n\nWindows support. TileDB is now fully supported on Windows systems (64-bit Windows 7 and newer).\nPython API. We are very excited to announce the initial release of a Python API for TileDB. The Python API makes TileDB accessible to a much broader audience, allowing seamless integration with existing Python libraries such as NumPy, Pandas and the scientific Python ecosystem.\nC++ API. We’ve included a C++ API, which allows TileDB to integrate into modern C++ applications without having to write code towards the C API. The C++ API is more concise and provides additional compile time type safety.\nS3 object store support. You can now easily store, query, and manipulate your TileDB arrays on S3 API compatibile object stores, including Amazon’s AWS S3 service.\nVirtual filesystem interface. The TileDB API now exposes a virtual filesystem (or VFS) interface, allowing you to perform tasks such as file creation, deletion, reads, and appends without worrying about whether your files are stored on S3, HDFS, a POSIX or Windows filesystem, etc.\nKey-value store. TileDB now provides a key-value (meta) data storage abstraction. Its implementation is built upon TileDB’s sparse arrays and inherits all the properties of TileDB sparse arrays."
  },
  {
    "objectID": "HISTORY.html#improvements-66",
    "href": "HISTORY.html#improvements-66",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Improvements",
    "text": "Improvements\n\nHomebrew formula added for easier installation on macOS. Homebrew is now the perferred method for installing TileDB and its dependencies on macOS.\nDocker images updated to include stable/unstable/dev options, and easy configuration of additional components (e.g. S3 support).\nTile cache implemented, which will greatly speed up repeated queries on overlapping regions of the same array.\nAbility to pass runtime configuration arguments to TileDB/VFS backends.\nUnnamed (or “anonymous”) dimensions are now supported; having a single anonymous attribute is also supported.\nConcurrency bugfixes for several compressors.\nCorrectness issue fixed in double-delta compressor for some datatypes.\nBetter build behavior on systems with older GCC or CMake versions.\nSeveral memory leaks and overruns fixed with help of sanitizers.\nMany improved error condition checks and messages for easier debugging.\nMany other small bugs and API inconsistencies fixed."
  },
  {
    "objectID": "HISTORY.html#c-api-additions",
    "href": "HISTORY.html#c-api-additions",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "C API additions",
    "text": "C API additions\n\ntiledb_config_*: Types and functions related to the new configuration object and functionality.\ntiledb_config_iter_*: Iteration functionality for retieving parameters/values from the new configuration object.\ntiledb_ctx_get_config(): Function to get a configuration from a context.\ntiledb_filesystem_t: Filesystem type enum.\ntiledb_ctx_is_supported_fs(): Function to check for support for a given filesystem backend.\ntiledb_error_t, tiledb_error_message() and tiledb_error_free(): Type and functions for TileDB error messages.\ntiledb_ctx_get_last_error(): Function to get last error from context.\ntiledb_domain_get_rank(): Function to retrieve number of dimensions in a domain.\ntiledb_domain_get_dimension_from_index() and tiledb_domain_get_dimension_from_name(): Replaces dimension iterators.\ntiledb_dimension_{create,free,get_name,get_type,get_domain,get_tile_extent}(): Functions related to creation and manipulation of tiledb_dimension_t objects.\ntiledb_array_schema_set_coords_compressor(): Function to set the coordinates compressor.\ntiledb_array_schema_set_offsets_compressor(): Function to set the offsets compressor.\ntiledb_array_schema_get_attribute_{num,from_index,from_name}(): Replaces attribute iterators.\ntiledb_query_create(): Replaced many arguments with new tiledb_query_set_*() setter functions.\ntiledb_array_get_non_empty_domain(): Function to retrieve the non-empty domain from an array.\ntiledb_array_compute_max_read_buffer_sizes(): Function to compute an upper bound on the buffer sizes required for a read query.\ntiledb_object_ls(): Function to visit the children of a path.\ntiledb_uri_to_path(): Function to convert a file:// URI to a platform-native path.\nTILEDB_MAX_PATH and tiledb_max_path(): The maximum length for tiledb resource paths.\ntiledb_kv_*: Types and functions related to the new key-value store functionality.\ntiledb_vfs_*: Types and functions related to the new virtual filesystem (VFS) functionality."
  },
  {
    "objectID": "HISTORY.html#breaking-changes-3",
    "href": "HISTORY.html#breaking-changes-3",
    "title": "TileDB v2.17.0 Release Notes",
    "section": "Breaking changes",
    "text": "Breaking changes\n\nC API\n\nRename tiledb_array_metadata_t -&gt; tiledb_array_schema_t, and associated tiledb_array_metadata_* functions to tiledb_array_schema_*.\nRemove tiledb_attribute_iter_t.\nRemove tiledb_dimension_iter_t.\nRename tiledb_delete(), tiledb_move(), tiledb_walk() to tiledb_object_{delete,move,walk}().\ntiledb_ctx_create: Config argument added.\ntiledb_domain_create: Datatype argument removed.\ntiledb_domain_add_dimension: Name, domain and tile extent arguments replaced with single tiledb_dimension_t argument.\ntiledb_query_create(): Replaced many arguments with new tiledb_query_set_*() setter functions.\ntiledb_array_create(): Added array URI argument.\ntiledb_*_free(): All free functions now take a pointer to the object pointer instead of simply object pointer.\nThe include files are now installed into a tiledb folder. The correct path is now #include &lt;tiledb/tiledb.h&gt; (or #include &lt;tiledb/tiledb&gt; for the C++ API).\n\n\n\nResource Management\n\nSupport for moving resources across previous VFS backends (local fs &lt;-&gt; HDFS) has been removed. A more generic implementation for this functionality with improved performance is planned for the next version of TileDB."
  },
  {
    "objectID": "ports/openssl/license.html",
    "href": "ports/openssl/license.html",
    "title": "",
    "section": "",
    "text": "CC0, from https://github.com/luncliff/vcpkg-registry/tree/main/ports/openssl1"
  },
  {
    "objectID": "scripts/ci/posix/DIRECTORY.html",
    "href": "scripts/ci/posix/DIRECTORY.html",
    "title": "",
    "section": "",
    "text": "This directory holds common auxiliary scripts used by Github actions workflows for macOS and Linux (see &lt;root&gt;/.github/workflows/ci-linux_mac.yml).\n\nprelim.sh: install required system packages; configure system for core dumps\nbuild-services-start.sh: start emulators (eg S3, GCS) for backend-specific testing\nbuild-services-stop.sh: stop emulators (this has been necessary to avoid job failure due to unexited processes)\ndump-core-stacks.sh: dump the stack traces from saved core dumps (if applicable)"
  },
  {
    "objectID": "doc/dev/style/Memory.html",
    "href": "doc/dev/style/Memory.html",
    "title": "Memory Guidelines",
    "section": "",
    "text": "The use of new or malloc is forbidden unless justifcation and a team agreement on an exception is granted. Inplace a smart pointer should be used.\nThat is a unique_ptr, shared_ptr or weak_ptr should be used in all cases. With weak_ptr being rarely used.\nIn the core TileDB library we have our own tdb_unique_ptr and tdb_shared_ptr that must be used inplace of the stl versions.\nmake_unique_ptr or make_shared_ptr is the preferred, exception safe, method of creating the smart pointers.\nExtreme care must be given to all allocations. Even when using STL containers like std::vector thoughtful consideration must be given for the dynamic allocations that will occur.\nIf we are only allocating one megabyte or less strongly consider fixed allocations instead. Fixed allocation help to avoid fragmentation of the heap."
  },
  {
    "objectID": "doc/dev/style/Memory.html#pointers-and-allocations",
    "href": "doc/dev/style/Memory.html#pointers-and-allocations",
    "title": "Memory Guidelines",
    "section": "",
    "text": "The use of new or malloc is forbidden unless justifcation and a team agreement on an exception is granted. Inplace a smart pointer should be used.\nThat is a unique_ptr, shared_ptr or weak_ptr should be used in all cases. With weak_ptr being rarely used.\nIn the core TileDB library we have our own tdb_unique_ptr and tdb_shared_ptr that must be used inplace of the stl versions.\nmake_unique_ptr or make_shared_ptr is the preferred, exception safe, method of creating the smart pointers.\nExtreme care must be given to all allocations. Even when using STL containers like std::vector thoughtful consideration must be given for the dynamic allocations that will occur.\nIf we are only allocating one megabyte or less strongly consider fixed allocations instead. Fixed allocation help to avoid fragmentation of the heap."
  },
  {
    "objectID": "doc/dev/style/Functions.html",
    "href": "doc/dev/style/Functions.html",
    "title": "Functions",
    "section": "",
    "text": "The legacy of C is that functions have only a single return and only return by value. When a function should naturally return multiple return values, it was expedient to pass “out-arguments” to the function. In C this was a pointer; in C++ it could also be a reference. This convention commingled in- and out-arguments in a call list. C++17 can do better.\n\nUsestd::tuple to return multiple values.\nStrictly speaking, returning multiple values was possible before. The overhead required was burdensome. A function could return a struct value, packaging multiple values as a single one. std::tuple allows something like an in-place struct declaration; the elements are typed but anonymous. Such a function is still returning a single value, but of a type that’s designed to hold multiples.\nUse std::optional to return a value that might not be initialized.\nA near-universal pattern is to return a value when a function succeeds but not to return anything when it fails. This is the pattern when a function throws on failure. If it should not throw on failure, though, it has to return both a success/failure status and sometimes a value. std::optional is the standard library’s way of representing “sometimes a value”. The “not this time” value is std::nullopt.\nThere’s an alternative return value that this style guide considers an antipattern: a default-constructed instance. Of all the problems it has, the most pernicious is that is discourages adherence to “C.41: A constructor should create a fully initialized object”. It’s less efficient, too, since it requires an extraneous construction and assignment. The present style guide enables direct-initialization with no run-time overhead.\nUse std::optional even for bool-convertible classes like shared_ptr.\nYes, it’s certainly possible to use an a empty shared_ptr to signify a “pointer that’s not present”. A consistent return convention makes it quicker to read code. Please don’t put your colleagues in the position of needing to figure out that you’re doing something different to accomplish something that’s not different.\nRemember the std::in_place_t form of the std::optional constructor.\nAs a rule, it’s more efficient to construct an object immediately before you need it. When returning an optional, that’s the return statement. The in_place argument to optional&lt;T&gt; allows its constructor to also construct the T instance, removing any need for a temporary or extraneous variable.\nPrefer a structured binding declaration to initialize variables.\nStructured binding, introduced in C++17, allows multiple assignment or multiple initialization from tuples (or from other types that can act like tuples). While it’s the preferred way of retrieving return values, it’s not the only way.\nThe easiest alternative is auto x{f(...)};. This statement initializes x as a tuple. The disadvantage is that elements have to be retrieved by position. A structured binding declaration allows elements to be retrieved by name. A tuple variable is perfectly good, though, when the value is being forwarded on.\nAnother alternative is to use std::tie to assign to existing variables. This can be better in circumstances where it’s beneficial to reuse an existing variable.\nPrefer structured direct-initialization over structured assignment.\nOne of the forms of structured binding looks like an assignment; it performs copy-initialization. The other forms perform direct-initialization. The direct form is more general and more efficient that the copy form. (The copy form might be needed in unusual circumstances.) In particular, direct-initialization works with non-copyable classes.\nAlso, use the curly-brace syntax for direct-initialization. There’s a parenthesis syntax available, but its easily confused at a glance with the lambda syntax.\nBy default, use the && ref-qualifier in a structured binding.\nRoughly speaking, the && ref-qualifier yields perfect forwarding, as if the structured variables had been directly initialized in the return statement of the function.\nIf the structured variables should all be constant, then const auto& may be appropriate, and the rules for temporary materialization will apply. The structured binding syntax does not allow mixed declarations with both const and non-const variables, as the cv-declaration is about the temporary value (the right side, roughly) rather than to the variables.\nCheck that a returned class has a move constructor.\nThis point is about efficiency. If a move constructor is not present, the compiler will use the copy constructor. The default move constructor will typically suffice.\n\n\n\n\nDeclare as tuple&lt;boolean_like, optional&lt;T&gt;&gt; f(...)\nInitialize as auto && [b, t]{ f(...) };\nMove constructor T(T&&) = default;\n\n\n\n\n#include &lt;optional&gt;\n\nclass NC {\n  int x;\n  int y;\n public:\n  NC() = delete;\n  explicit NC(int x, int y) : x(x), y(y) {};\n  NC(const NC&) = delete; // No copy constructor\n  NC(NC&&) = default;  // ... but there is a move constructor\n  int operator()() const { return x; }\n};\n\nstd::tuple&lt;bool, std::optional&lt;NC&gt;&gt; foo() {\n  return {true,std::optional&lt;NC&gt;( std::in_place, 1, 3 )};\n}\n\nstd::tuple&lt;bool, std::optional&lt;NC&gt;&gt; bar() {\n  return {false, std::nullopt};\n}\n\nTEST_CASE(\"foo\")\n{\n  auto && [b, nc]{ foo() };\n  CHECK(b);\n  REQUIRE(nc.has_value());\n  CHECK((*nc)() == 1);\n}\n\nTEST_CASE(\"bar\")\n{\n  auto && [b, nc]{ bar() };\n  CHECK_FALSE(b);\n  CHECK_FALSE(nc.has_value());\n}"
  },
  {
    "objectID": "doc/dev/style/Functions.html#inputs-on-the-right.-outputs-on-the-left.",
    "href": "doc/dev/style/Functions.html#inputs-on-the-right.-outputs-on-the-left.",
    "title": "Functions",
    "section": "",
    "text": "The legacy of C is that functions have only a single return and only return by value. When a function should naturally return multiple return values, it was expedient to pass “out-arguments” to the function. In C this was a pointer; in C++ it could also be a reference. This convention commingled in- and out-arguments in a call list. C++17 can do better.\n\nUsestd::tuple to return multiple values.\nStrictly speaking, returning multiple values was possible before. The overhead required was burdensome. A function could return a struct value, packaging multiple values as a single one. std::tuple allows something like an in-place struct declaration; the elements are typed but anonymous. Such a function is still returning a single value, but of a type that’s designed to hold multiples.\nUse std::optional to return a value that might not be initialized.\nA near-universal pattern is to return a value when a function succeeds but not to return anything when it fails. This is the pattern when a function throws on failure. If it should not throw on failure, though, it has to return both a success/failure status and sometimes a value. std::optional is the standard library’s way of representing “sometimes a value”. The “not this time” value is std::nullopt.\nThere’s an alternative return value that this style guide considers an antipattern: a default-constructed instance. Of all the problems it has, the most pernicious is that is discourages adherence to “C.41: A constructor should create a fully initialized object”. It’s less efficient, too, since it requires an extraneous construction and assignment. The present style guide enables direct-initialization with no run-time overhead.\nUse std::optional even for bool-convertible classes like shared_ptr.\nYes, it’s certainly possible to use an a empty shared_ptr to signify a “pointer that’s not present”. A consistent return convention makes it quicker to read code. Please don’t put your colleagues in the position of needing to figure out that you’re doing something different to accomplish something that’s not different.\nRemember the std::in_place_t form of the std::optional constructor.\nAs a rule, it’s more efficient to construct an object immediately before you need it. When returning an optional, that’s the return statement. The in_place argument to optional&lt;T&gt; allows its constructor to also construct the T instance, removing any need for a temporary or extraneous variable.\nPrefer a structured binding declaration to initialize variables.\nStructured binding, introduced in C++17, allows multiple assignment or multiple initialization from tuples (or from other types that can act like tuples). While it’s the preferred way of retrieving return values, it’s not the only way.\nThe easiest alternative is auto x{f(...)};. This statement initializes x as a tuple. The disadvantage is that elements have to be retrieved by position. A structured binding declaration allows elements to be retrieved by name. A tuple variable is perfectly good, though, when the value is being forwarded on.\nAnother alternative is to use std::tie to assign to existing variables. This can be better in circumstances where it’s beneficial to reuse an existing variable.\nPrefer structured direct-initialization over structured assignment.\nOne of the forms of structured binding looks like an assignment; it performs copy-initialization. The other forms perform direct-initialization. The direct form is more general and more efficient that the copy form. (The copy form might be needed in unusual circumstances.) In particular, direct-initialization works with non-copyable classes.\nAlso, use the curly-brace syntax for direct-initialization. There’s a parenthesis syntax available, but its easily confused at a glance with the lambda syntax.\nBy default, use the && ref-qualifier in a structured binding.\nRoughly speaking, the && ref-qualifier yields perfect forwarding, as if the structured variables had been directly initialized in the return statement of the function.\nIf the structured variables should all be constant, then const auto& may be appropriate, and the rules for temporary materialization will apply. The structured binding syntax does not allow mixed declarations with both const and non-const variables, as the cv-declaration is about the temporary value (the right side, roughly) rather than to the variables.\nCheck that a returned class has a move constructor.\nThis point is about efficiency. If a move constructor is not present, the compiler will use the copy constructor. The default move constructor will typically suffice.\n\n\n\n\nDeclare as tuple&lt;boolean_like, optional&lt;T&gt;&gt; f(...)\nInitialize as auto && [b, t]{ f(...) };\nMove constructor T(T&&) = default;\n\n\n\n\n#include &lt;optional&gt;\n\nclass NC {\n  int x;\n  int y;\n public:\n  NC() = delete;\n  explicit NC(int x, int y) : x(x), y(y) {};\n  NC(const NC&) = delete; // No copy constructor\n  NC(NC&&) = default;  // ... but there is a move constructor\n  int operator()() const { return x; }\n};\n\nstd::tuple&lt;bool, std::optional&lt;NC&gt;&gt; foo() {\n  return {true,std::optional&lt;NC&gt;( std::in_place, 1, 3 )};\n}\n\nstd::tuple&lt;bool, std::optional&lt;NC&gt;&gt; bar() {\n  return {false, std::nullopt};\n}\n\nTEST_CASE(\"foo\")\n{\n  auto && [b, nc]{ foo() };\n  CHECK(b);\n  REQUIRE(nc.has_value());\n  CHECK((*nc)() == 1);\n}\n\nTEST_CASE(\"bar\")\n{\n  auto && [b, nc]{ bar() };\n  CHECK_FALSE(b);\n  CHECK_FALSE(nc.has_value());\n}"
  },
  {
    "objectID": "doc/dev/style/Functions.html#references",
    "href": "doc/dev/style/Functions.html#references",
    "title": "Functions",
    "section": "References",
    "text": "References\n\nhttps://en.cppreference.com/w/cpp/utility/tuple\nhttps://en.cppreference.com/w/cpp/utility/optional\nhttps://en.cppreference.com/w/cpp/language/structured_binding"
  },
  {
    "objectID": "doc/dev/style/Variables.html",
    "href": "doc/dev/style/Variables.html",
    "title": "Initialization",
    "section": "",
    "text": "The title says it all. Here are some examples.\n// No. This is initialization with an assignment expression.\nint x = 1;\n\n// No. This uses a parenthesized expression list.\nint x(1);\n\n// No. This has a brace initialization list, but it uses an assignment expression,\n// not direct initialization\nstd::vector&lt;int&gt; y = {1,2,3};\n\n// Yes\nint x{1};\nstd::vector&lt;int&gt; y{1,2,3};"
  },
  {
    "objectID": "doc/dev/style/Variables.html#prefer-direct-initialization-with-a-braced-initialization-list",
    "href": "doc/dev/style/Variables.html#prefer-direct-initialization-with-a-braced-initialization-list",
    "title": "Initialization",
    "section": "",
    "text": "The title says it all. Here are some examples.\n// No. This is initialization with an assignment expression.\nint x = 1;\n\n// No. This uses a parenthesized expression list.\nint x(1);\n\n// No. This has a brace initialization list, but it uses an assignment expression,\n// not direct initialization\nstd::vector&lt;int&gt; y = {1,2,3};\n\n// Yes\nint x{1};\nstd::vector&lt;int&gt; y{1,2,3};"
  },
  {
    "objectID": "doc/dev/style/Variables.html#references",
    "href": "doc/dev/style/Variables.html#references",
    "title": "Initialization",
    "section": "References",
    "text": "References\n\nThe FAQ for C++11 calls this uniform initialization. The term “uniform initialization” isn’t the official name for the syntax, however. Rather, it describes the goal to allow a single syntax to be used for all initializations.\nSection “11.6 Initializers [dcl.init]” in the C++17 standard, ISO/IEC 14882:2017."
  },
  {
    "objectID": "doc/dev/style/Serialization.html",
    "href": "doc/dev/style/Serialization.html",
    "title": "On-disk Serialization and Deserialization",
    "section": "",
    "text": "The status of on-disk serialization is currently in flux. At time of writing, the current guidelines for writing serialize functions for TileDB classes is as follows:\n\nOnly use sizeof on fixed-sized types (not variables or types with compiler-dependent size like bool).\nReturn a TileDB shared pointer to the object in deserialize methods. Use exceptions, not Status objects, for errors.\nValidate all types either at time of deserialization or in the constructor used for generating the object.\nAdd unit tests that check the validations throws the expected errors.\n\n\n\nFor validating enums, consider creating a function that converts from the underlying variable type to the enum with validation. The serialization/deserialization should be to an fixed-size integer of the under-lying type. Consider writing a function with validation to get the actual enum from the integer. For example,\ninline ArrayType array_type_from_int(uint8_t array_type_enum) {\n    auto type = ArrayType(array_type_enum);\n    if (type != ArrayType::DENSE && type != ArrayType::SPARSE) {\n        throw std::runtime_error(\n            \"Invalid ArrayType (\" + std::to_string(array_type_enum) + \")\");\n    }\n    return type;\n}\nIf the enum has many types, use a switch statement to validate the types."
  },
  {
    "objectID": "doc/dev/style/Serialization.html#serialization-and-deserialization-guidelines",
    "href": "doc/dev/style/Serialization.html#serialization-and-deserialization-guidelines",
    "title": "On-disk Serialization and Deserialization",
    "section": "",
    "text": "The status of on-disk serialization is currently in flux. At time of writing, the current guidelines for writing serialize functions for TileDB classes is as follows:\n\nOnly use sizeof on fixed-sized types (not variables or types with compiler-dependent size like bool).\nReturn a TileDB shared pointer to the object in deserialize methods. Use exceptions, not Status objects, for errors.\nValidate all types either at time of deserialization or in the constructor used for generating the object.\nAdd unit tests that check the validations throws the expected errors.\n\n\n\nFor validating enums, consider creating a function that converts from the underlying variable type to the enum with validation. The serialization/deserialization should be to an fixed-size integer of the under-lying type. Consider writing a function with validation to get the actual enum from the integer. For example,\ninline ArrayType array_type_from_int(uint8_t array_type_enum) {\n    auto type = ArrayType(array_type_enum);\n    if (type != ArrayType::DENSE && type != ArrayType::SPARSE) {\n        throw std::runtime_error(\n            \"Invalid ArrayType (\" + std::to_string(array_type_enum) + \")\");\n    }\n    return type;\n}\nIf the enum has many types, use a switch statement to validate the types."
  },
  {
    "objectID": "experimental/tiledb/common/dag/doc/miscellaneous.html",
    "href": "experimental/tiledb/common/dag/doc/miscellaneous.html",
    "title": "",
    "section": "",
    "text": "Data items are data blocks."
  },
  {
    "objectID": "experimental/tiledb/common/dag/doc/miscellaneous.html#miscellaneous",
    "href": "experimental/tiledb/common/dag/doc/miscellaneous.html#miscellaneous",
    "title": "",
    "section": "",
    "text": "Data items are data blocks."
  },
  {
    "objectID": "experimental/tiledb/common/dag/state_machine/doc/stop2.html",
    "href": "experimental/tiledb/common/dag/state_machine/doc/stop2.html",
    "title": "",
    "section": "",
    "text": "Case I: Assuming only the producer node can issue a stop, and that only as provided by the user-defined function at a well-defined location.\nThe basic logic of a producer node is:\n   tmp = f()\n   inject(tmp)\n   source_fill()\n   source_push()\nNow we include a mechanism so that user-supplied function f can indicate it’s time to stop.\n  bool token = false;\n  auto tmp = f(token);\n  if (token == stop) {\n    port_exhausted()\n    return\n  }\n  source_fill()\n  source_push()\nExpanded and with proof outline\n  init: { state = 00 ∧ stop = 0 }\n\n  while (not done)\n\n     /* { state = 00 ∨ state = 01 } ∧ { stop = 0 } */\n     bool token = false;\n     auto tmp = f(token);\n\n     /* { state = 00 ∨ state = 01 } ∧ { stop = 0 } */\n\n     if (token == stop) {\n\n       /* { state = 00 ∨ state = 01 } ∧ { stop = 0 } */\n       event(stop):\n\n         /* { state = 00 ∨ state = 01 } ∧ { stop = 0 } */\n         exit action: (none)\n\n         /* { state = 00 ∨ state = 01 } ∧ { stop = 0 } */\n         state transition: { stop = 0 } → { stop = 1 }\n\n         /* { state = 00 ∨ state = 01 } ∧ { stop = 1 } */\n         entry action: { state = 00 } → term_source\n                       { state = 01 } → term_source\n\n         /* { state = 00 ∨ state = 01 } ∧ { stop = 1 } */\n     }\n\n     event(source_fill):\n       \n     event(source_push):\nThe basic logic of a consumer node is:\n   sink_pull()\n   tmp = extract()\n   f(tmp)\n   sink_drain()\nNote that with respect to the sink, a stop event can happen at any time. If there is still data in the pipeline, when there is a stop, we want it to continue to flow to the sink until everything has been transferred and handled, i.e., until the state is 00. Basically, this means that everything continues in the same way regardless of whether we are in a stopping state – except when { state == 00 } ∧ { stop = 1 } on the sink_pull event. In that case, rather than waiting, we just continue, transition to { state = done }, and then invoke the term_sink action.\n  init: { state = 00 } ∧ { stop = 0 }\n\n  while (not done)\n\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n     event(sink_pull):\n\n       /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n       exit action: { state = 00 }              ∧ { stop = 0 }            → sink_wait\n                    { state = 00 }              ∧ { stop = 1 }            → none\n                    { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 } → none\n                    { state = 10 }              ∧ { stop = 0 ∨ stop = 1 } → sink_swap\n\n       state transition: { state = 00 }              ∧ { stop = 0 }            → n/a (wait)\n                         { state = 00 }              ∧ { stop = 1 }            → { state = done }\n                         { state = 01 ∨ state = 10 } ∧ { stop = 0 ∨ stop = 1 } → { state = 01 }   ∧ { stop = 0 ∨ stop = 1 }\n                         { state = 11 }              ∧ { stop = 0 ∨ stop = 1 } → { state = 11 }   ∧ { stop = 0 ∨ stop = 1 }\n\n       entry action: { state = done }                                                   → term_sink\n                     { state = 10 }                           ∧ { stop = 0 ∨ stop = 1 } → sink_swap\n                     { state = 00 ∨ state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 } → none\n\n       /* { state = 01 ∨ state = 11 } ∧ { stop = 0 }\n\n     /* { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n     auto tmp = extract()\n\n     /* { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n     f(tmp)\n\n     /* { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n     event(sink_drain):\n\n     /* { state = 00 ∨ state = 10 } ∧ { stop = 0 ∨ stop = 1 }\nHere, we are essentially processing the stop in the sink_pull event.\nAn alternative would be to do it in sink_drain, as follows:\n  init: { state = 00 } ∧ { stop = 0 }\n\n  while (not done)\n\n     /* { state = 00 ∨ state = 01 ∨ state = 10 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n     event(sink_pull):\n\n     /* { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n     auto tmp = extract()\n\n     /* { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n     f(tmp)\n\n     /* { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n     event(sink_drain):\n\n       /* { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n       exit action: (none) \n\n       /* { state = 01 ∨ state = 11 } ∧ { stop = 0 ∨ stop = 1 }\n       state transition: { state = 01 } ∧ { stop = 0 }            → { state = 00 } ∧ { stop = 0 }\n                         { state = 01 } ∧ { stop = 1 }            → { state = done }\n                         { state = 11 } ∧ { stop = 0 ∨ stop = 1 } → { state = 10 } ∧ { stop = 0 ∨ stop = 1 }\n\n       entry action: { state = 00 ∨ state = 10 } ∧ { stop = 0 } → notify_source\n                     { state = 10 }              ∧ { stop = 1 } → none\n                     { state = done }                           → term_sink\n\n\n       /* { state = 01 ∨ state = 11 } ∧ { stop = 0 }\n\n     /* { state = 00 ∨ state = 10 } ∧ { stop = 0 ∨ stop = 1 }\nThese are essentially equivalent – we’ve just moved the code “back” from the sink_pull to the sink_drain case. However, for reasons of symmetry, restricting scheduler-type actions to push and pull and state updates to fill and drain seems to make sense."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Public code docs",
    "section": "",
    "text": "This is public documentation of the code within this repository."
  },
  {
    "objectID": "format_spec/filters/webp.html",
    "href": "format_spec/filters/webp.html",
    "title": "WEBP Filter",
    "section": "",
    "text": "The WebP filter compresses image data using libwebp. The current input formats supported are RGB, RGBA, BGR, and BGRA. The colorspace format can be configured for this filter by setting the TILEDB_WEBP_INPUT_FORMAT filter option. Tile-based compression is determined by dimension extents. Extents equal to dimension bounds will compress the image in one pass.\nFor attributes, the WebP filter supports only the uint8_t data type. Dimension data types can be any two matching integral types, such as {uint32_t, uint32_t} or {int64_t, int64_t} for example.\n# Data from 2x3 pixel image\ninput = [[255,62,83, 149,43,67, 138,43,67]\n         [255,62,83, 149,43,67, 138,43,67]]\n# Ingest and read with lossless compression\noutput = [[255,62,83, 149,43,67, 138,43,67]\n          [255,62,83, 149,43,67, 138,43,67]]\n# Ingest and read with lossy compression\noutput = [[251,60,84, 148,51,61, 142,42,64]\n          [252,63,85, 150,46,67, 139,44,68]]\nThe figure below from WebP API documentation may help to understand the format of data passed to and read from the WebP filter.\n\nPhoto by Google licensed under CC BY 4.0\nUsing the figure above from WebP API documentation, height should correspond with dimension 0 upper bound, while stride corresponds with dimension 1 upper bound. The pixel width of the image is internally calculated based on colorspace format and tile extents.\nTile extents are used to configure tile-based compression. We should note that the maximum WebP image size is 16383x16383. Because of this, our extents can not exceed 16383 for dimension 0, and 16383 * pixel_depth for dimension 1. pixel_depth is defined by our colorspace format selection. RGB and BGR images have a depth of 3, while RGBA and BGRA have a depth of 4 for the additional alpha value provided for each pixel.\n\nFilter Enum Value\nThe filter enum value for the WEBP filter is 17 (TILEDB_FILTER_WEBP enum).\n\n\nInput and Output Layout\nInput is a single array of colorspace values in RGB, BGR, RGBA, or BGRA format. For RGB format we would expect the first pixel to store R, G, B values at input[0], input[1], and input[2] respectively, followed by the remaining pixels in the image. Ingesting a 10x10 image would expect 300 total input values for 10 rows of 30 colorspace values per-row.\nOutput is organized the same as input. If we compress an image with lossless enabled, all outputs match inputs exactly. If we instead use lossy compression with a quality of 0, output colorspace values will vary due to data lost during lossy WebP compression but the organization of the data remains the same."
  },
  {
    "objectID": "format_spec/filters/xor.html",
    "href": "format_spec/filters/xor.html",
    "title": "XOR Filter",
    "section": "",
    "text": "The XOR filter applies the XOR operation sequentially to the input data, in chunks of 1-4 bytes, depending on the sizeof the attribute’s type representation. For example, given data as a NumPy int64 array:\ndata = np.random.rand(npts)\ndata_b = data.view(np.int64)\nfor i in range(1, len(data)):\n  data_b[i] = data_b[i] ^ data_b[i-1]\n\nFilter Enum Value\nThe filter enum value for the XOR filter is 16 (TILEDB_FILTER_XOR enum).\n\n\nInput and Output Layout\nThe input and output data layout is identical for the XOR filter."
  },
  {
    "objectID": "format_spec/group.html",
    "href": "format_spec/group.html",
    "title": "Group",
    "section": "",
    "text": "A group consists of metadata and a file containing group members.\nThe current group format version is 2.\nmy_group                       # Group folder\n    |_ __tiledb_group.tdb      # Empty group file\n    |_ __group                 # Group folder\n        |_ &lt;timestamped_name&gt;  # Timestamped group file detailing members\n    |_ __meta                  # group metadata folder\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nVersion\nuint32_t\nFormat version number of the group\n\n\nNumber of Group Member\nuint32_t\nThe number of group members.\n\n\nGroup Member 1\nGroup Member\nFirst group member\n\n\n…\n…\n…\n\n\nGroup Member N\nGroup Member\nNth group member\n\n\n\n\n\n\nThe group member is the content inside a group.\nThe current group member format version is 2.\n\n\n\nField\nType\nDescription\n\n\n\n\nVersion\nuint32_t\nFormat version number of the group member\n\n\nObject type\nuint8_t\nObject type of the member\n\n\nRelative\nuint8_t\nIs the URI relative to the group\n\n\nURI length\nuint32_t\nNumber of characters in URI\n\n\nURI\nuint8_t[]\nURI character array\n\n\nDeleted\nuint8_t\nIs the member deleted"
  },
  {
    "objectID": "format_spec/group.html#group-file",
    "href": "format_spec/group.html#group-file",
    "title": "Group",
    "section": "",
    "text": "Field\nType\nDescription\n\n\n\n\nVersion\nuint32_t\nFormat version number of the group\n\n\nNumber of Group Member\nuint32_t\nThe number of group members.\n\n\nGroup Member 1\nGroup Member\nFirst group member\n\n\n…\n…\n…\n\n\nGroup Member N\nGroup Member\nNth group member"
  },
  {
    "objectID": "format_spec/group.html#group-member",
    "href": "format_spec/group.html#group-member",
    "title": "Group",
    "section": "",
    "text": "The group member is the content inside a group.\nThe current group member format version is 2.\n\n\n\nField\nType\nDescription\n\n\n\n\nVersion\nuint32_t\nFormat version number of the group member\n\n\nObject type\nuint8_t\nObject type of the member\n\n\nRelative\nuint8_t\nIs the URI relative to the group\n\n\nURI length\nuint32_t\nNumber of characters in URI\n\n\nURI\nuint8_t[]\nURI character array\n\n\nDeleted\nuint8_t\nIs the member deleted"
  },
  {
    "objectID": "format_spec/generic_tile.html",
    "href": "format_spec/generic_tile.html",
    "title": "Generic Tile",
    "section": "",
    "text": "The generic tile is a tile prepended with some extra header data, so that it can be used stand-alone without requiring extra information from the array schema. A generic tile has the following on-disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nVersion number\nuint32_t\nFormat version number of the generic tile\n\n\nPersisted size\nuint64_t\nPersisted (e.g. compressed) size of the tile\n\n\nTile size\nuint64_t\nIn-memory (e.g. uncompressed) size of the tile\n\n\nDatatype\nuint8_t\nDatatype of the tile\n\n\nCell size\nuint64_t\nCell size of the tile\n\n\nEncryption type\nuint8_t\nType of encryption used in filtering the tile\n\n\nFilter pipeline size\nuint32_t\nNumber of bytes in the serialized filter pipeline\n\n\nFilter pipeline\nFilter Pipeline\nFilter pipeline used to filter the tile\n\n\nTile data\nTile\nThe serialized tile data"
  },
  {
    "objectID": "format_spec/fragment.html",
    "href": "format_spec/fragment.html",
    "title": "Fragment",
    "section": "",
    "text": "A fragment metadata folder is called &lt;timestamped_name&gt; and located here:\nmy_array                                    # array folder\n   |  ...\n   |_ __fragments                           # array fragments folder\n         |_ &lt;timestamped_name&gt;              # fragment folder\n         |      |_ __fragment_metadata.tdb  # fragment metadata\n         |      |_ a0.tdb                   # fixed-sized attribute \n         |      |_ a1.tdb                   # var-sized attribute (offsets) \n         |      |_ a1_var.tdb               # var-sized attribute (values)\n         |      |_ a2.tdb                   # fixed-sized nullable attribute\n         |      |_ a2_validity.tdb          # fixed-sized nullable attribute (validities)\n         |      |_ ...      \n         |      |_ d0.tdb                   # fixed-sized dimension \n         |      |_ d1.tdb                   # var-sized dimension (offsets) \n         |      |_ d1_var.tdb               # var-sized dimension (values)\n         |      |_ ...      \n         |      |_ t.tdb                    # timestamp attribute\n         |      |_ ...  \n         |      |_ dt.tdb                   # delete timestamp attribute\n         |      |_ ...  \n         |      |_ dci.tdb                  # delete condition index attribute\n         |      |_ ...  \n        |_ ...  \n&lt;timestamped_name&gt; has format __t1_t2_uuid_v, where: * t1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC) * uuid is a unique identifier * v is the format version\nThere can be any number of fragments in an array. The fragment folder contains:\n\nA single fragment metadata file named __fragment_metadata.tdb.\nAny number of data files. For each fixed-sized attribute foo1 (or dimension bar1), there is a single data file a0.tdb (d0.tdb) containing the values along this attribute (dimension). For every var-sized attribute foo2 (or dimensions bar2), there are two data files; a1_var.tdb (d1_var.tdb) containing the var-sized values of the attribute (dimension) and a1.tdb (d1.tdb) containing the starting offsets of each value in a1_var.tdb (d1_var.rdb). Both fixed-sized and var-sized attributes can be nullable. A nullable attribute, foo3, will have an additional file a2_validity.tdb that contains its validity vector.\nThe names of the data files are not dependent on the names of the attributes/dimensions. The file names are determined by the order of the attributes and dimensions in the array schema.\nThe timestamp fixed attribute (t.tdb) is, for fragments consolidated with timestamps, the time at which a cell was added.\nThe delete timestamp fixed attribute (dt.tdb) is, for fragments consolidated with delete conditions, the time at which a cell was deleted.\nThe delete condition Delete commit file index fixed attribute (dci.tdb) is, for fragments consolidated with delete conditions, the index of the delete condition (inside of Tile Processed Conditions) that deleted the cell."
  },
  {
    "objectID": "format_spec/fragment.html#main-structure",
    "href": "format_spec/fragment.html#main-structure",
    "title": "Fragment",
    "section": "",
    "text": "A fragment metadata folder is called &lt;timestamped_name&gt; and located here:\nmy_array                                    # array folder\n   |  ...\n   |_ __fragments                           # array fragments folder\n         |_ &lt;timestamped_name&gt;              # fragment folder\n         |      |_ __fragment_metadata.tdb  # fragment metadata\n         |      |_ a0.tdb                   # fixed-sized attribute \n         |      |_ a1.tdb                   # var-sized attribute (offsets) \n         |      |_ a1_var.tdb               # var-sized attribute (values)\n         |      |_ a2.tdb                   # fixed-sized nullable attribute\n         |      |_ a2_validity.tdb          # fixed-sized nullable attribute (validities)\n         |      |_ ...      \n         |      |_ d0.tdb                   # fixed-sized dimension \n         |      |_ d1.tdb                   # var-sized dimension (offsets) \n         |      |_ d1_var.tdb               # var-sized dimension (values)\n         |      |_ ...      \n         |      |_ t.tdb                    # timestamp attribute\n         |      |_ ...  \n         |      |_ dt.tdb                   # delete timestamp attribute\n         |      |_ ...  \n         |      |_ dci.tdb                  # delete condition index attribute\n         |      |_ ...  \n        |_ ...  \n&lt;timestamped_name&gt; has format __t1_t2_uuid_v, where: * t1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC) * uuid is a unique identifier * v is the format version\nThere can be any number of fragments in an array. The fragment folder contains:\n\nA single fragment metadata file named __fragment_metadata.tdb.\nAny number of data files. For each fixed-sized attribute foo1 (or dimension bar1), there is a single data file a0.tdb (d0.tdb) containing the values along this attribute (dimension). For every var-sized attribute foo2 (or dimensions bar2), there are two data files; a1_var.tdb (d1_var.tdb) containing the var-sized values of the attribute (dimension) and a1.tdb (d1.tdb) containing the starting offsets of each value in a1_var.tdb (d1_var.rdb). Both fixed-sized and var-sized attributes can be nullable. A nullable attribute, foo3, will have an additional file a2_validity.tdb that contains its validity vector.\nThe names of the data files are not dependent on the names of the attributes/dimensions. The file names are determined by the order of the attributes and dimensions in the array schema.\nThe timestamp fixed attribute (t.tdb) is, for fragments consolidated with timestamps, the time at which a cell was added.\nThe delete timestamp fixed attribute (dt.tdb) is, for fragments consolidated with delete conditions, the time at which a cell was deleted.\nThe delete condition Delete commit file index fixed attribute (dci.tdb) is, for fragments consolidated with delete conditions, the index of the delete condition (inside of Tile Processed Conditions) that deleted the cell."
  },
  {
    "objectID": "format_spec/fragment.html#fragment-metadata-file",
    "href": "format_spec/fragment.html#fragment-metadata-file",
    "title": "Fragment",
    "section": "Fragment Metadata File",
    "text": "Fragment Metadata File\nThe fragment metadata file has the following on-disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nR-Tree\nR-Tree\nThe serialized R-Tree\n\n\nTile offsets for attribute/dimension 1\nTile Offsets\nThe serialized tile offsets for attribute/dimension 1\n\n\n…\n…\n…\n\n\nTile offsets for attribute/dimension N\nTile Offsets\nThe serialized tile offsets for attribute/dimension N\n\n\nVariable tile offsets for attribute/dimension 1\nTile Offsets\nThe serialized variable tile offsets for attribute/dimension 1\n\n\n…\n…\n…\n\n\nVariable tile offsets for attribute/dimension N\nTile Offsets\nThe serialized variable tile offsets for attribute/dimension N\n\n\nVariable tile sizes for attribute/dimension 1\nTile Offsets\nThe serialized variable tile sizes for attribute/dimension 1\n\n\n…\n…\n…\n\n\nVariable tile sizes for attribute/dimension N\nTile Offsets\nThe serialized variable tile sizes for attribute/dimension N\n\n\nValidity tile offsets for attribute/dimension 1\nTile Offsets\nThe serialized validity tile offsets for attribute/dimension 1\n\n\n…\n…\n…\n\n\nValidity tile offsets for attribute/dimension N\nTile Offsets\nThe serialized validity tile offsets for attribute/dimension N\n\n\nTile mins for attribute/dimension 1\nTile Mins/Maxes\nThe serialized mins for attribute/dimension 1\n\n\n…\n…\n…\n\n\nVariable mins for attribute/dimension N\nTile Mins/Maxes\nThe serialized mins for attribute/dimension N\n\n\nTile maxes for attribute/dimension 1\nTile Mins/Maxes\nThe serialized maxes for attribute/dimension 1\n\n\n…\n…\n…\n\n\nVariable maxes for attribute/dimension N\nTile Mins/Maxes\nThe serialized maxes for attribute/dimension N\n\n\nTile sums for attribute/dimension 1\nTile Sums\nThe serialized sums for attribute/dimension 1\n\n\n…\n…\n…\n\n\nVariable sums for attribute/dimension N\nTile Sums\nThe serialized sums for attribute/dimension N\n\n\nTile null counts for attribute/dimension 1\nTile Null Count\nThe serialized null counts for attribute/dimension 1\n\n\n…\n…\n…\n\n\nVariable maxes for attribute/dimension N\n[Tile Null Count\nThe serialized null counts for attribute/dimension N\n\n\nFragment min, max, sum, null count\n[Tile Fragment Min Max Sum Null Count\nThe serialized fragment min max sum null count\n\n\nProcessed conditions\n[Tile Processed Conditions\nThe serialized processed conditions\n\n\nMetadata footer\nFooter\nBasic metadata gathered in the footer\n\n\n\n\nR-Tree\nThe R-Tree is a generic tile with the following internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nFanout\nuint32_t\nThe tree fanout\n\n\nNum levels\nuint32_t\nThe number of levels in the tree\n\n\nNum MBRs at level 1\nuint64_t\nThe number of MBRs at level 1\n\n\nMBR 1 at level 1\nMBR\nFirst MBR at level 1\n\n\n…\n…\n…\n\n\nMBR N at level 1\nMBR\nN-th MBR at level 1\n\n\n…\n…\n…\n\n\nNum MBRs at level L\nuint64_t\nThe number of MBRs at level L\n\n\nMBR 1 at level L\nMBR\nFirst MBR at level L\n\n\n…\n…\n…\n\n\nMBR N at level L\nMBR\nN-th MBR at level L\n\n\n\n\n\nMBR\nEach MBR entry has format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\n1D range for dimension 1\n1DRange\nThe 1-dimensional range for dimension 1\n\n\n…\n…\n…\n\n\n1D range for dimension D\n1DRange\nThe 1-dimensional range for dimension D\n\n\n\nFor fixed-sized dimensions, the 1DRange format is:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nRange minimum\nuint8_t\nThe minimum value with the same datatype as the dimension\n\n\nRange maximum\nuint8_t\nThe maximum value with the same datatype as the dimension\n\n\n\nFor var-sized dimensions, the 1DRange format is:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nRange length\nuint64_t\nThe number of bytes of the 1D range\n\n\nMinimum value length\nuint64_t\nThe number of bytes of the minimum value\n\n\nRange minimum\nuint8_t\nThe minimum (var-sized) value with the same datatype as the dimension\n\n\nRange maximum\nuint8_t\nThe maximum (var-sized) value with the same datatype as the dimension\n\n\n\n\n\nTile Offsets\nThe tile offsets is a generic tile with the following internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum tile offsets\nuint64_t\nNumber of tile offsets\n\n\nTile offset 1\nuint64_t\nOffset 1\n\n\n…\n…\n…\n\n\nTile offset N\nuint64_t\nOffset N\n\n\n\n\n\nTile Sizes\nThe tile sizes is a generic tile with the following internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum tile sizes\nuint64_t\nNumber of tile sizes\n\n\nTile size 1\nuint64_t\nSize 1\n\n\n…\n…\n…\n\n\nTile size N\nuint64_t\nSize N\n\n\n\n\n\nTile Mins Maxes\nThe tile mins maxes is a generic tile with the following internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum values\nuint64_t\nNumber of values\n\n\nValue 1\ntype\nValue 1 or Offset 1\n\n\n…\n…\n…\n\n\nValue N\ntype\nValue N or Offset N\n\n\nVar buffer size\nuint64_t\nVar buffer size\n\n\nVar buffer\nuint8_t\nVar buffer\n\n\n\n\n\nTile Sums\nThe tile sums is a generic tile with the following internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum values\nuint64_t\nNumber of values\n\n\nValue 1\nuint64_t\nSum 1\n\n\n…\n…\n…\n\n\nValue N\nuint64_t\nSum N\n\n\n\n\n\nTile Null Count\nThe tile null count is a generic tile with the following internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum values\nuint64_t\nNumber of values\n\n\nValue 1\nuint64_t\nCount 1\n\n\n…\n…\n…\n\n\nValue N\nuint64_t\nCount N\n\n\n\n\n\nTile Fragment Min Max Sum Null Count\nThe fragment min max sum null count is a generic tile with the following internal format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nMin size\nuint64_t\nSize of the min value for attribute/dimension 1\n\n\nMin value\nuint8_t\nBuffer for min value for attribute/dimension 1\n\n\nMax size\nuint64_t\nSize of the max value for attribute/dimension 1\n\n\nMax value\nuint8_t\nBuffer for max value for attribute/dimension 1\n\n\nSum\nuint64_t\nSum value for attribute/dimension 1\n\n\nNull count\nuint64_t\nNull count value for attribute/dimension 1\n\n\n…\n…\n…\n\n\nMin size\nuint64_t\nSize of the min value for attribute/dimension N\n\n\nMin value\nuint8_t\nBuffer for min value for attribute/dimension N\n\n\nMax size\nuint64_t\nSize of the max value for attribute/dimension N\n\n\nMax value\nuint8_t\nBuffer for max value for attribute/dimension N\n\n\nSum\nuint64_t\nSum value for attribute/dimension N\n\n\nNull count\nuint64_t\nNull count value for attribute/dimension N\n\n\n\n\n\nTile Processed Conditions\nThe processed conditions is a generic tile and is the list of delete/update conditions that have already been applied for this fragment and don’t need to be applied again, sorted by filename, with the following internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum\nuint64_t\nNumber of processed conditions\n\n\nCondition size\nuint64_t\nCondition size 1\n\n\nCondition\nuint8_t\nCondition marker filename 1\n\n\n…\n…\n…\n\n\nCondition size\nuint64_t\nCondition size N\n\n\nCondition\nuint8_t\nCondition marker filename N\n\n\n\n\n\nFooter\nThe footer is a simple blob (i.e., not a generic tile) with the following internal format:\n\n\n\nField\nType\nDescription\n\n\n\n\nVersion number\nuint32_t\nFormat version number of the fragment\n\n\nArray schema name size\nuint64_t\nSize of the array schema name\n\n\nArray schema name\nstring\nArray schema name\n\n\nDense\nuint8_t\nWhether the array is dense (1) or not (0)\n\n\nNull non-empty domain\nuint8_t\nIndicates whether the non-empty domain is null (1) or not (0)\n\n\nNon-empty domain\nMBR\nAn MBR denoting the non-empty domain\n\n\nNumber of sparse tiles\nuint64_t\nNumber of sparse tiles\n\n\nLast tile cell num\nuint64_t\nFor sparse arrays, the number of cells in the last tile in the fragment\n\n\nIncludes timestamps\nuint8_t\nWhether the fragment includes timestamps (1) or not (0)\n\n\nIncludes delete metadata\nuint8_t\nWhether the fragment includes delete metadata (1) or not (0)\n\n\nFile sizes\nuint64_t[]\nThe size in bytes of each attribute/dimension file in the fragment. For var-length attributes/dimensions, this is the size of the offsets file.\n\n\nFile var sizes\nuint64_t[]\nThe size in bytes of each var-length attribute/dimension file in the fragment.\n\n\nFile validity sizes\nuint64_t[]\nThe size in bytes of each attribute/dimension validity vector file in the fragment.\n\n\nR-Tree offset\nuint64_t\nThe offset to the generic tile storing the R-Tree in the metadata file.\n\n\nTile offset for attribute/dimension 1\nuint64_t\nThe offset to the generic tile storing the tile offsets for attribute/dimension 1.\n\n\n…\n…\n…\n\n\nTile offset for attribute/dimension N\nuint64_t\nThe offset to the generic tile storing the tile offsets for attribute/dimension N\n\n\nTile var offset for attribute/dimension 1\nuint64_t\nThe offset to the generic tile storing the variable tile offsets for attribute/dimension 1.\n\n\n…\n…\n…\n\n\nTile var offset for attribute/dimension N\nuint64_t\nThe offset to the generic tile storing the variable tile offsets for attribute/dimension N.\n\n\nTile var sizes offset for attribute/dimension 1\nuint64_t\nThe offset to the generic tile storing the variable tile sizes for attribute/dimension 1.\n\n\n…\n…\n…\n\n\nTile var sizes offset for attribute/dimension N\nuint64_t\nThe offset to the generic tile storing the variable tile sizes for attribute/dimension N.\n\n\nTile validity offset for attribute/dimension 1\nuint64_t\nThe offset to the generic tile storing the tile validity offsets for attribute/dimension 1.\n\n\n…\n…\n…\n\n\nTile validity offset for attribute/dimension N\nuint64_t\nThe offset to the generic tile storing the tile validity offsets for attribute/dimension N\n\n\nTile mins offset for attribute/dimension 1\nuint64_t\nThe offset to the generic tile storing the tile mins for attribute/dimension 1.\n\n\n…\n…\n…\n\n\nTile mins offset for attribute/dimension N\nuint64_t\nThe offset to the generic tile storing the tile mins for attribute/dimension N\n\n\nTile maxes offset for attribute/dimension 1\nuint64_t\nThe offset to the generic tile storing the tile maxes for attribute/dimension 1.\n\n\n…\n…\n…\n\n\nTile maxes offset for attribute/dimension N\nuint64_t\nThe offset to the generic tile storing the tile maxes for attribute/dimension N\n\n\nTile sums offset for attribute/dimension 1\nuint64_t\nThe offset to the generic tile storing the tile sums for attribute/dimension 1.\n\n\n…\n…\n…\n\n\nTile sums offset for attribute/dimension N\nuint64_t\nThe offset to the generic tile storing the tile sums for attribute/dimension N\n\n\nTile null counts offset for attribute/dimension 1\nuint64_t\nThe offset to the generic tile storing the tile null counts for attribute/dimension 1.\n\n\n…\n…\n…\n\n\nTile null counts offset for attribute/dimension N\nuint64_t\nThe offset to the generic tile storing the tile null counts for attribute/dimension N\n\n\nFragment min max sum null count offset\nuint64_t\nThe offset to the generic tile storing the fragment min max sum null count data.\n\n\nProcessed conditions offset\nuint64_t\nThe offset to the generic tile storing the processed conditions.\n\n\nArray schema name size\nuint64_t\nThe total number of characters of the array schema name.\n\n\nArray schema name\nuint8_t[]\nThe array schema name.\n\n\nFooter length\nuint64_t\nSum of bytes of the above fields. Only present when there is at least one var-sized dimension."
  },
  {
    "objectID": "format_spec/fragment.html#data-file",
    "href": "format_spec/fragment.html#data-file",
    "title": "Fragment",
    "section": "Data File",
    "text": "Data File\nThe on-disk format of each data file is:\n\n\n\nField\nType\nDescription\n\n\n\n\nTile 1\nTile\nThe data of tile 1\n\n\n…\n…\n…\n\n\nTile N\nTile\nThe data of tile N"
  },
  {
    "objectID": "format_spec/update_commit_file.html",
    "href": "format_spec/update_commit_file.html",
    "title": "Update Commit File",
    "section": "",
    "text": "A update commit file has name &lt;timestamped_name&gt;.upd and is located here:\nmy_array                              # array folder\n   |_ ....\n   |_ __commits                       # array commits folder\n         |_ &lt;timestamped_name&gt;.upd    # update commit file\n         |_ ...\n&lt;timestamped_name&gt; has format __t1_t2_uuid_v, where:\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nThere may be multiple such files in the array commits folder. Each update commit file contains a tile with a serialized update condition, which is a tree of nodes followed by update values, which is a list of values. Each node for the condition can be a value node or expression node. Expression nodes have the following on disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNode type\nuint8_t\n0 for expression node\n\n\nCombination op\nuint8_t\nAND(0), OR(1), NOT(2)\n\n\nNum children\nuint64_t[]\nNumber of child nodes\n\n\nChildren 1\nNODE\nchildren 1\n\n\n…\n…\n…\n\n\nChildren N\nNODE\nChildren N\n\n\n\nValue nodes have the following on disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNode type\nuint8_t\n1 for value node\n\n\nOp\nuint8_t\nLT(0), LE(1), GT(2), GE(3), EQ(4), NE(5)\n\n\nField name size\nuint32_t\nSize of the field name\n\n\nField name value\nuint8_t[]\nField name value\n\n\nValue size\nuint64_t\nValue size\n\n\nValue content\nuint8_t[]\nValue\n\n\n\nUpdate values are serialized in the following format: | Field | Type | Description | | :— | :— | :— | | Number of values | uint64_t | Number of values N | | Field name size 1 | uint64_t | Size of the field name for the first update value | | Field name value 1 | uint8_t[] | Field name value for the first update value | | Value size 1 | uint64_t | Value size for first update value | | Value content 1 | uint8_t[] | Value for first update value | | … | … | … | | Field name size N | uint64_t | Size of the field name for update value N | | Field name value N | uint8_t[] | Field name value for update value N | | Value size N | uint64_t | Value size for update value N | | Value content N | uint8_t[] | Value for update value N |"
  },
  {
    "objectID": "format_spec/vacuum_file.html",
    "href": "format_spec/vacuum_file.html",
    "title": "Vacuum File",
    "section": "",
    "text": "A vacuum file has name __t1_t2_uuid_v.vac and can be located either in the array commit folder:\nmy_array                        # array folder\n   |_ ....\n   |_ __commits                 # array commit folder\n         |___t1_t2_uuid_v.vac   # vacuum file\nor in the array metadata folder:\nmy_array                        # array folder\n   |  ...            \n   |_ __meta                    # array metadata folder\n         |_ ...\n         |_ __t1_t2_uuid_v.vac  # vacuum file\n         |_ ...\nWhen located in the commits folder, it will include the URI of fragments (in the __fragments folder) that can be vaccumed. When located in the array metadata folder, it will include the URI or array metadata files that can be vaccumed.\nIn the file name:\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nThe vacuum file is a simple text file where each line contains a URI string:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nURI 1 followed by a new line character\nuint8_t[]\nURI 1 to be vacuumed\n\n\n…\n…\n…\n\n\nURI N followed by a new line character\nuint8_t[]\nURI N to be vacuumed"
  },
  {
    "objectID": "format_spec/tile.html",
    "href": "format_spec/tile.html",
    "title": "Tile",
    "section": "",
    "text": "Internally tile data is divided into “chunks.” Every tile is at least one chunk. Each tile has the following on-disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum chunks\nuint64_t\nNumber of chunks in the tile\n\n\nChunk 1\nChunk\nFirst chunk in the tile\n\n\n…\n…\n…\n\n\nChunk N\nChunk\nN-th chunk in the tile"
  },
  {
    "objectID": "format_spec/tile.html#main-structure",
    "href": "format_spec/tile.html#main-structure",
    "title": "Tile",
    "section": "",
    "text": "Internally tile data is divided into “chunks.” Every tile is at least one chunk. Each tile has the following on-disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum chunks\nuint64_t\nNumber of chunks in the tile\n\n\nChunk 1\nChunk\nFirst chunk in the tile\n\n\n…\n…\n…\n\n\nChunk N\nChunk\nN-th chunk in the tile"
  },
  {
    "objectID": "format_spec/tile.html#chunk-format",
    "href": "format_spec/tile.html#chunk-format",
    "title": "Tile",
    "section": "Chunk Format",
    "text": "Chunk Format\nA chunk has the following on-disk format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nOriginal length of chunk\nuint32_t\nThe original (unfiltered) number of bytes of chunk data\n\n\nFiltered chunk length\nuint32_t\nThe serialized (filtered) number of bytes of chunk data\n\n\nChunk metadata length\nuint32_t\nNumber of bytes in the chunk metadata\n\n\nChunk metadata\nuint8_t[]\nChunk metadata bytes\n\n\nChunk filtered data\nuint8_t[]\nFiltered chunk bytes\n\n\n\nThe metadata added to a chunk depends on the sequence of filters in the pipeline used to filter the containing tile.\nIf a pipeline used to filter tiles is empty (contains no filters), the tile is still divided into chunks and serialized according to the above format. In this case there are no chunk metadata bytes (since there are no filters to add metadata), and the filtered bytes are the same as original bytes.\nThe “chunk metadata” before the actual “chunk filtered data” depend on the particular sequence of filters in the pipeline. In the simple case, each filter will simply concatenate its metadata to the chunk metadata region. Because some filters in the pipeline may wish to filter the metadata of previous filters (e.g. compression, where it is beneficial to compress previous filters’ metadata in addition to the actual chunk data), the ordering of filters also impacts the metadata that is eventually written to disk.\nThe “chunk filtered data” bytes contain the final bytes of the chunk after being passed through the entire pipeline. When reading tiles from disk, the filter pipeline is run in the reverse order.\nInternally, any filter in a filter pipeline produces two arrays of data as output: a metadata byte array and a filtered data byte array. Additionally, these output byte arrays can be arbitrarily separated into “parts” by any filter. Typically, when a next filter receives the output of the previous filter as its input, it will filter each “part” independently.\n\nByteshuffle Filter\nThe byteshuffle filter does not filter input metadata, and the output data is guaranteed to be the same length as the input data.\nThe byteshuffle filter produces output metadata in the format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNumber of parts\nuint32_t\nNumber of data parts\n\n\nLength of part 1\nuint32_t\nNumber of bytes in data part 1\n\n\n…\n…\n…\n\n\nLength of part N\nuint32_t\nNumber of bytes in data part N\n\n\n\nThe byteshuffle filter produces output data in the format:\n\n\n\nField\nType\nDescription\n\n\n\n\nPart 1\nuint8_t[]\nByteshuffled data part 1\n\n\n…\n…\n…\n\n\nPart N\nuint8_t[]\nByteshuffled data part N\n\n\n\n\n\nBitshuffle Filter\nThe bitshuffle filter does not filter input metadata. It produces output metadata in the format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNumber of parts\nuint32_t\nNumber of data parts\n\n\nLength of part 1\nuint32_t\nNumber of bytes in data part 1\n\n\n…\n…\n…\n\n\nLength of part N\nuint32_t\nNumber of bytes in data part N\n\n\n\nThe bitshuffle filter produces output data in the format:\n\n\n\nField\nType\nDescription\n\n\n\n\nPart 1\nuint8_t[]\nBitshuffled data part 1\n\n\n…\n…\n…\n\n\nPart N\nuint8_t[]\nBitshuffled data part N\n\n\n\n\n\nBit Width Reduction Filter\nThe bit width reduction filter does not filter input metadata. It produces output metadata in the format:\n\n\n\nField\nType\nDescription\n\n\n\n\nLength of input\nuint32_t\nOriginal input number of bytes\n\n\nNumber of windows\nuint32_t\nNumber of windows in output\n\n\nWindow 1 metadata\nWindowMD\nMetadata for window 1\n\n\n…\n…\n…\n\n\nWindow N metadata\nWindowMD\nMetadata for window N\n\n\n\nThe type WindowMD has the format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nWindow value offset\nT\nOffset applied to values in the output window, where T is the original datatype of the tile values.\n\n\nBit width of reduced type\nuint8_t\nNumber of bits in the new datatype of the values in the output window\n\n\nWindow length\nuint32_t\nNumber of bytes in output window data.\n\n\n\nThe bit width reduction filter produces output data in the format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nWindow 1\nuint8_t[]\nWindow 1 data (possibly-reduced width elements)\n\n\n…\n…\n…\n\n\nWindow N\nuint8_t[]\nWindow N data (possibly-reduced width elements)\n\n\n\n\n\nPositive Delta Encoding Filter\nThe positive-delta encoding filter does not filter input metadata. It produces output metadata in the format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNumber of windows\nuint32_t\nNumber of windows in output\n\n\nWindow 1 metadata\nWindowMD\nMetadata for window 1\n\n\n…\n…\n…\n\n\nWindow N metadata\nWindowMD\nMetadata for window N\n\n\n\nThe type WindowMD has the format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nWindow value delta offset\nT\nOffset applied to values in the output window, where T is the datatype of the tile values.\n\n\nWindow length\nuint32_t\nNumber of bytes in output window data.\n\n\n\nThe positive-delta encoding filter produces output data in the format:\n\n\n\nField\nType\nDescription\n\n\n\n\nWindow 1\nT[]\nWindow 1 delta-encoded data\n\n\n…\n…\n…\n\n\nWindow N\nT[]\nWindow N delta-encoded data\n\n\n\n\n\nCompression Filters\nThe compression filters do filter input metadata. They produce output metadata in the format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nNumber of metadata parts\nuint32_t\nNumber of input metadata parts that were compressed\n\n\nNumber of data parts\nuint32_t\nNumber of input data parts that were compressed\n\n\nMetadata part 1\nCompressedPartMD\nMetadata about the first metadata\n\n\n…\n…\n…\n\n\nMetadata part N\nCompressedPartMD\nMetadata about the nth metadata part\n\n\nData part 1\nCompressedPartMD\nMetadata about the first data part\n\n\n…\n…\n…\n\n\nData part N\nCompressedPartMD\nMetadata about the nth data part\n\n\n\nThe type CompressedPartMD has the format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nPart original length\nuint32_t\nInput length of the part (before compression)\n\n\nPart compressed length\nuint32_t\nCompressed length of the part\n\n\n\nThe compression filters then produce output data in the format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nMetadata part 0 compressed bytes\nuint8_t[]\nCompressed bytes of the first metadata part\n\n\n…\n…\n…\n\n\nMetadata part N compressed bytes\nuint8_t[]\nCompressed bytes of the nth metadata part\n\n\nData part 0 compressed bytes\nuint8_t[]\nCompressed bytes of the first data part\n\n\n…\n…\n…\n\n\nData part N compressed bytes\nuint8_t[]\nCompressed bytes of the nth data part\n\n\n\n\n\nChecksum Filters\nThe filter metadata for TILEDB_FILTER_CHECKSUM_{MD5,SHA256} has internal format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nNum metadata checksums\nuint32_t\nNumber of checksums computed on input metadata\n\n\nNum data checksums\nuint32_t\nNumber of checksums computed on input data\n\n\nNum input bytes for metadata checksum 1\nuint64_t\nNumber of bytes of metadata input to the 1st metadata checksum\n\n\nMetadata checksum 1\nuint8_t[{16,32}] (MD5/SHA256)\nChecksum produced on first metadata input\n\n\n…\n…\n…\n\n\nNum input bytes for metadata checksum N\nuint64_t\nNumber of bytes of metadata input to the N-th metadata checksum\n\n\nMetadata checksum N\nuint8_t[{16,32}] (MD5/SHA256)\nChecksum produced on N-th metadata input\n\n\nNum input bytes for data checksum 1\nuint64_t\nNumber of bytes of data input to the 1st data checksum\n\n\nData checksum 1\nuint8_t[{16,32}] (MD5/SHA256)\nChecksum produced on first data input\n\n\n…\n…\n…\n\n\nNum input bytes for data checksum N\nuint64_t\nNumber of bytes of data input to the N-th data checksum\n\n\nData checksum N\nuint8_t[{16,32}] (MD5/SHA256)\nChecksum produced on N-th data input\n\n\nInput metadata\nuint8_t[]\nOriginal input metadata, copied intact\n\n\n\n\n\nEncryption Filters\nIf the array is encrypted, TileDB uses an extra internal filter INTERNAL_FILTER_AES_256_GCM for AES encryption.\nThe encryption filter metadata have the following on-disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNum metadata parts\nuint32_t\nNumber of encrypted metadata parts\n\n\nNum data parts\nuint32_t\nNumber of encrypted data parts\n\n\nAES Metadata Part 1\nAESPartMD\nMetadata part 1\n\n\n…\n…\n…\n\n\nAES Metadata Part N\nAESPartMD\nMetadata part N\n\n\nAES Data Part 1\nAESPart\nData part 1\n\n\n…\n…\n…\n\n\nAES Data Part N\nAESPart\nData part N\n\n\n\nThe AESPartMD field has the following on-disk format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nNum metadata parts\nuint32_t\nNumber of metadata parts\n\n\nNum data parts\nuint32_t\nNumber of data parts\n\n\nPlaintext length for metadata part 1\nuint32_t\nNumber of bytes of plaintext metadata part 1\n\n\nCiphertext length for metadata part 1\nuint32_t\nNumber of bytes of ciphertext metadata part 1\n\n\nIV bytes for metadata part 1\nuint32_t\nNumber of bytes of AES-256-GCM IV bytes for metadata part 1\n\n\nTag bytes for metadata part 1\nuint32_t\nNumber of bytes of AES-256-GCM tag for metadata part 1\n\n\n…\n…\n…\n\n\nPlaintext length for metadata part N\nuint32_t\nNumber of bytes of plaintext metadata part N\n\n\nCiphertext length for metadata part N\nuint32_t\nNumber of bytes of ciphertext metadata part N\n\n\nIV bytes for metadata part N\nuint32_t\nNumber of bytes of AES-256-GCM IV bytes for metadata part N\n\n\nTag bytes for metadata part N\nuint32_t\nNumber of bytes of AES-256-GCM tag for metadata part N\n\n\nPlaintext length for data part 1\nuint32_t\nNumber of bytes of plaintext data part 1\n\n\nCiphertext length for data part 1\nuint32_t\nNumber of bytes of ciphertext data part 1\n\n\nIV bytes for data part 1\nuint32_t\nNumber of bytes of AES-256-GCM IV bytes for data part 1\n\n\nTag bytes for data part 1\nuint32_t\nNumber of bytes of AES-256-GCM tag for data part 1\n\n\n…\n…\n…\n\n\nPlaintext length for data part N\nuint32_t\nNumber of bytes of plaintext data part N\n\n\nCiphertext length for data part N\nuint32_t\nNumber of bytes of ciphertext data part N\n\n\nIV bytes for data part N\nuint32_t\nNumber of bytes of AES-256-GCM IV bytes for data part N\n\n\nTag bytes for data part N\nuint32_t\nNumber of bytes of AES-256-GCM tag for data part N\n\n\n\nThe original metadata is not included in the metadata output.\nThe AESPart field has the following on-disk format:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nPlaintext length\nuint32_t\nThe original unencrypted length of the part\n\n\nEncrypted length\nuint32_t\nThe encrypted length of the part\n\n\nIV Bytes\nuint8_t[12]\nAES-256-GCM IV bytes\n\n\nTag Bytes\nuint8_t[16]\nAES-256-GCM tag bytes\n\n\n\nThe data output of the encryption filter is:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nMetadata part 1\nuint8_t[]\nThe encrypted bytes of metadata part 1\n\n\n…\n…\n…\n\n\nMetadata part N\nuint8_t[]\nThe encrypted bytes of metadata part N\n\n\nData part 1\nuint8_t[]\nThe encrypted bytes of data part 1\n\n\n…\n…\n…\n\n\nMetadata part N\nuint8_t[]\nThe encrypted bytes of data part N\n\n\n\nNote that the original input metadata in not part of the output."
  },
  {
    "objectID": "format_spec/delete_commit_file.html",
    "href": "format_spec/delete_commit_file.html",
    "title": "Delete Commit File",
    "section": "",
    "text": "A delete commit file has name &lt;timestamped_name&gt;.del and is located here:\nmy_array                              # array folder\n   |_ ....\n   |_ __commits                       # array commits folder\n         |_ &lt;timestamped_name&gt;.del    # delete commit file\n         |_ ...\n&lt;timestamped_name&gt; has format __t1_t2_uuid_v, where:\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nThere may be multiple such files in the array commits folder. Each delete commit file contains a tile with a serialized delete condition, which is a tree of nodes. Each node can be a value node or expression node. Expression nodes have the following on disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNode type\nuint8_t\n0 for expression node\n\n\nCombination op\nuint8_t\nAND(0), OR(1), NOT(2)\n\n\nNum children\nuint64_t\nNumber of child nodes\n\n\nChildren 1\nNODE\nchildren 1\n\n\n…\n…\n…\n\n\nChildren N\nNODE\nChildren N\n\n\n\nValue nodes have the following on disk format:\n\n\n\nField\nType\nDescription\n\n\n\n\nNode type\nuint8_t\n1 for value node\n\n\nOp\nuint8_t\nLT(0), LE(1), GT(2), GE(3), EQ(4), NE(5)\n\n\nField name size\nuint32_t\nSize of the field name\n\n\nField name value\nuint8_t[]\nField name value\n\n\nValue size\nuint64_t\nValue size\n\n\nValue content\nuint8_t[]\nValue"
  },
  {
    "objectID": "format_spec/metadata.html",
    "href": "format_spec/metadata.html",
    "title": "Array Metadata",
    "section": "",
    "text": "The metadata is a folder called __meta located here:\nmy_array                            # array folder\n   |  ...\n   |_ __meta                        # metadata folder\n         |_ &lt;timestamped_name&gt;      # metadata file\n         |_ ...\n         |_ &lt;timestamped_name&gt;.vac  # vacuum file\n         |_ ...\n&lt;timestamped_name&gt; has format __t1_t2_uuid_v, where:\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nThe metadata folder can contain: * Any number of metadata files * Any number of vacuum files"
  },
  {
    "objectID": "format_spec/metadata.html#main-structure",
    "href": "format_spec/metadata.html#main-structure",
    "title": "Array Metadata",
    "section": "",
    "text": "The metadata is a folder called __meta located here:\nmy_array                            # array folder\n   |  ...\n   |_ __meta                        # metadata folder\n         |_ &lt;timestamped_name&gt;      # metadata file\n         |_ ...\n         |_ &lt;timestamped_name&gt;.vac  # vacuum file\n         |_ ...\n&lt;timestamped_name&gt; has format __t1_t2_uuid_v, where:\n\nt1 and t2 are timestamps in milliseconds elapsed since 1970-01-01 00:00:00 +0000 (UTC)\nuuid is a unique identifier\nv is the format version\n\nThe metadata folder can contain: * Any number of metadata files * Any number of vacuum files"
  },
  {
    "objectID": "format_spec/metadata.html#metadata-file",
    "href": "format_spec/metadata.html#metadata-file",
    "title": "Array Metadata",
    "section": "Metadata File",
    "text": "Metadata File\nThe metadata file consists of a single generic tile, containing multiple entries with the following data:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nKey length\nuint32_t\nThe length of the key.\n\n\nKey\nuint8_t[]\nThe key.\n\n\nDeletion\nuint8_t\n1/0 if it is a deletion/insertion.\n\n\nValue type\nuint8_t\nThe value data type. Present only if del is 0.\n\n\nNumber of values\nuint32_t\nThe number of values. Present only if del is 0.\n\n\nValue\nuint8_t[]\nThe value. Present only if del is 0."
  },
  {
    "objectID": "tiledb/api/c_api/DIRECTORY.html",
    "href": "tiledb/api/c_api/DIRECTORY.html",
    "title": "",
    "section": "",
    "text": "The C API code is in sections, roughly one for each type exposed through the API. Each section has its own directory, containing two headers, at least one API source file, and unit tests.\n\nExternal header. This header is included in the top-level tiledb.h header. These headers will be included in both C and C++ programs and must compile appropriately.\n\nDeclares C API functions.\nOnly included in tiledb.h. Should not appear elsewhere, including in other external headers.\nHas extern \"C\" linkage.\nUses macros such as TILEDB_EXPORT and NOEXCEPT to allow for differences between languages and platforms.\nUses name convention &lt;section&gt;_api_external.h\n\nInternal header. This header is not user-visible and is for inclusion in API source files.\n\nOnly included in API sources and API white-box unit tests. Since these headers are not user-visible, they do not appear in integration tests; such tests should only use external headers.\nIncluded source files in other sections as the arguments of C API function dictate.\nUses name convention &lt;section&gt;_api_internal.h\n\nEnumeration header. This header is user-visible by inclusion through the external header, but should never be included directly. This header contains code fragments that define values for enumeration symbols; the fragments require a particular pattern of use for the preprocessor.\n\nIncluded in the external header, where it defines a plain enum for C.\nIncluded in the internal header, where it defines an enum class for C++\nUses name convention &lt;section&gt;_api_enum.h\n\nAPI source\n\nDefines C API functions with an exception wrapper around a matching implementation function.\nDefines implementations functions, one for each C API function. These implementation functions do not have separate header declarations.\nUses name convention &lt;section&gt;_api.cc"
  },
  {
    "objectID": "tiledb/api/c_api/DIRECTORY.html#c-api",
    "href": "tiledb/api/c_api/DIRECTORY.html#c-api",
    "title": "",
    "section": "",
    "text": "The C API code is in sections, roughly one for each type exposed through the API. Each section has its own directory, containing two headers, at least one API source file, and unit tests.\n\nExternal header. This header is included in the top-level tiledb.h header. These headers will be included in both C and C++ programs and must compile appropriately.\n\nDeclares C API functions.\nOnly included in tiledb.h. Should not appear elsewhere, including in other external headers.\nHas extern \"C\" linkage.\nUses macros such as TILEDB_EXPORT and NOEXCEPT to allow for differences between languages and platforms.\nUses name convention &lt;section&gt;_api_external.h\n\nInternal header. This header is not user-visible and is for inclusion in API source files.\n\nOnly included in API sources and API white-box unit tests. Since these headers are not user-visible, they do not appear in integration tests; such tests should only use external headers.\nIncluded source files in other sections as the arguments of C API function dictate.\nUses name convention &lt;section&gt;_api_internal.h\n\nEnumeration header. This header is user-visible by inclusion through the external header, but should never be included directly. This header contains code fragments that define values for enumeration symbols; the fragments require a particular pattern of use for the preprocessor.\n\nIncluded in the external header, where it defines a plain enum for C.\nIncluded in the internal header, where it defines an enum class for C++\nUses name convention &lt;section&gt;_api_enum.h\n\nAPI source\n\nDefines C API functions with an exception wrapper around a matching implementation function.\nDefines implementations functions, one for each C API function. These implementation functions do not have separate header declarations.\nUses name convention &lt;section&gt;_api.cc"
  },
  {
    "objectID": "tiledb/api/C_API_STRUCTURE.html",
    "href": "tiledb/api/C_API_STRUCTURE.html",
    "title": "Structure of the C API",
    "section": "",
    "text": "The C API consists of layers starting with the API functions themselves and ending with classes that call into the non-API parts of the core library. There are five layers, each with its own responsibilities.\n\n\nPublic API functions themselves are declared with extern \"C\" linkage. They are implemented as wrappers around an API implementation function. There is a one-to-one relationship between public API functions and API implementation functions.\nPublic API functions are responsible for uniform error processing through handling exceptions. The wrapper provides uniformity of behavior for (1) return values, (2) logging format, (3) error return. Non-uniform error handling may be done outside this layer, although that’s not usually necessary.\n\n\n\nAPI implementation functions translate C calling sequences into C++ calling sequences. At the foundation is translating between C-style pointers and C++ objects. This is sometimes much more than simple argument translation. This translation may require conversion between the differing lifespans of core objects and API-visible objects that allow access to them. Different classes may be used to represent the same kind of core object, depending on life cycle; API implementation functions are responsible for selecting the correct one.\nThese functions are the first line for argument validation. They have principal responsibility for the ways that argument may be invalid as C arguments, the most notable of which is null pointers. They have secondary responsibility for validating arguments in other ways; in these cases the underlying classes have the primary responsibility.\nThese functions have primary responsibility for memory allocation of API-visible objects. The legacy pattern was raw new and delete with explicit error recovery. The new pattern is to use a handle class. Once the conversion to handles is complete, it won’t be necessary to state that memory is a responsibility here.\n\n\n\nHandle classes manage memory allocation for API-visible objects, deriving from a generic class api:handle base class. Handle classes hide the details of memory allocation from API implementation functions\nHandles are carriers for facade instances. There’s a one-to-one relationship between a handle class and a facade base class (see below). Handle classes forward all functions used by implementation functions to the facade.\nThere’s a one-to-one relationship between handle objects and facade objects. Handles are constructed carrying facades, and facades are not created unless through a handle.\nAn API implementation function may not naively assume that an argument pointer to a C API function is a valid pointer to a handle; such an assumption constitutes a security risk. Associated with each handle type is a warehouse function (parallel to a factory function) that converts an API argument pointer into a facade pointer. At root this is simply getting the pointer value, but the warehouse function may refuse a request for an unknown object.\n[Future] Handle classes are the nexus for API object registration, a facility that keeps track of all outstanding objects obtained through the C API.\n\n\n\nFacade classes present a uniform interface to core objects regardless of the life cycle of the object. In particular, there’s a difference between newly-created objects, such as those created with *_alloc functions, and pre-existing objects, such as those retrieved from aggregates like an array schema. Thus facade classes generally derive from an abstract base class and implement them in accordance with life cycle concerns.\nThe motivation for this structure is to be able to interconvert between C.41-compliant core objects and a C.41-hostile set C API functions. Depending on lifecycle state, some operations may be prohibited. For example, array schema data is considered immutable once it’s been created, so set_* C API functions must fail on objects that have been (C.41 fully-) constructed and must succeed on “pre-construction” objects (those that have been constructed in the C API but not yet constructed in core).\nFor each facade base class, there’s an optional proxy class, that is, a {0,1} to 1 relationship. If there’s an *_alloc function in the C API, there will be a proxy class, but otherwise there need not be one.\n\n\n\nA proxy class represent a C API object that is newly created by collecting constructor arguments through API set_ calls.\n\n\n\n\n\nIn order to facilitate correct SWIG wrapper generation, non-TileDB headers included by the external header files must be wrapped as follows:\n#ifndef TILEDB_CAPI_WRAPPING\n#include &lt;stdint.h&gt;\n#endif"
  },
  {
    "objectID": "tiledb/api/C_API_STRUCTURE.html#layers",
    "href": "tiledb/api/C_API_STRUCTURE.html#layers",
    "title": "Structure of the C API",
    "section": "",
    "text": "The C API consists of layers starting with the API functions themselves and ending with classes that call into the non-API parts of the core library. There are five layers, each with its own responsibilities.\n\n\nPublic API functions themselves are declared with extern \"C\" linkage. They are implemented as wrappers around an API implementation function. There is a one-to-one relationship between public API functions and API implementation functions.\nPublic API functions are responsible for uniform error processing through handling exceptions. The wrapper provides uniformity of behavior for (1) return values, (2) logging format, (3) error return. Non-uniform error handling may be done outside this layer, although that’s not usually necessary.\n\n\n\nAPI implementation functions translate C calling sequences into C++ calling sequences. At the foundation is translating between C-style pointers and C++ objects. This is sometimes much more than simple argument translation. This translation may require conversion between the differing lifespans of core objects and API-visible objects that allow access to them. Different classes may be used to represent the same kind of core object, depending on life cycle; API implementation functions are responsible for selecting the correct one.\nThese functions are the first line for argument validation. They have principal responsibility for the ways that argument may be invalid as C arguments, the most notable of which is null pointers. They have secondary responsibility for validating arguments in other ways; in these cases the underlying classes have the primary responsibility.\nThese functions have primary responsibility for memory allocation of API-visible objects. The legacy pattern was raw new and delete with explicit error recovery. The new pattern is to use a handle class. Once the conversion to handles is complete, it won’t be necessary to state that memory is a responsibility here.\n\n\n\nHandle classes manage memory allocation for API-visible objects, deriving from a generic class api:handle base class. Handle classes hide the details of memory allocation from API implementation functions\nHandles are carriers for facade instances. There’s a one-to-one relationship between a handle class and a facade base class (see below). Handle classes forward all functions used by implementation functions to the facade.\nThere’s a one-to-one relationship between handle objects and facade objects. Handles are constructed carrying facades, and facades are not created unless through a handle.\nAn API implementation function may not naively assume that an argument pointer to a C API function is a valid pointer to a handle; such an assumption constitutes a security risk. Associated with each handle type is a warehouse function (parallel to a factory function) that converts an API argument pointer into a facade pointer. At root this is simply getting the pointer value, but the warehouse function may refuse a request for an unknown object.\n[Future] Handle classes are the nexus for API object registration, a facility that keeps track of all outstanding objects obtained through the C API.\n\n\n\nFacade classes present a uniform interface to core objects regardless of the life cycle of the object. In particular, there’s a difference between newly-created objects, such as those created with *_alloc functions, and pre-existing objects, such as those retrieved from aggregates like an array schema. Thus facade classes generally derive from an abstract base class and implement them in accordance with life cycle concerns.\nThe motivation for this structure is to be able to interconvert between C.41-compliant core objects and a C.41-hostile set C API functions. Depending on lifecycle state, some operations may be prohibited. For example, array schema data is considered immutable once it’s been created, so set_* C API functions must fail on objects that have been (C.41 fully-) constructed and must succeed on “pre-construction” objects (those that have been constructed in the C API but not yet constructed in core).\nFor each facade base class, there’s an optional proxy class, that is, a {0,1} to 1 relationship. If there’s an *_alloc function in the C API, there will be a proxy class, but otherwise there need not be one.\n\n\n\nA proxy class represent a C API object that is newly created by collecting constructor arguments through API set_ calls."
  },
  {
    "objectID": "tiledb/api/C_API_STRUCTURE.html#implementation-restrictions",
    "href": "tiledb/api/C_API_STRUCTURE.html#implementation-restrictions",
    "title": "Structure of the C API",
    "section": "",
    "text": "In order to facilitate correct SWIG wrapper generation, non-TileDB headers included by the external header files must be wrapped as follows:\n#ifndef TILEDB_CAPI_WRAPPING\n#include &lt;stdint.h&gt;\n#endif"
  },
  {
    "objectID": "tiledb/api/c_api_test_support/DIRECTORY.html",
    "href": "tiledb/api/c_api_test_support/DIRECTORY.html",
    "title": "",
    "section": "",
    "text": "This directory provides support classes for testing the C API at the unit level\n\n\nThe class StorageManager is too large and unwieldy to support unit testing, yet there’s member variable of that type in each Context. This directory defines a stub class for StorageManager that allows testing with Context in cases that don’t require anything but a stub."
  },
  {
    "objectID": "tiledb/api/c_api_test_support/DIRECTORY.html#c-api-test-support",
    "href": "tiledb/api/c_api_test_support/DIRECTORY.html#c-api-test-support",
    "title": "",
    "section": "",
    "text": "This directory provides support classes for testing the C API at the unit level\n\n\nThe class StorageManager is too large and unwieldy to support unit testing, yet there’s member variable of that type in each Context. This directory defines a stub class for StorageManager that allows testing with Context in cases that don’t require anything but a stub."
  },
  {
    "objectID": "tiledb/storage_format/DIRECTORY.html",
    "href": "tiledb/storage_format/DIRECTORY.html",
    "title": "Storage Format",
    "section": "",
    "text": "TileDB arrays are stored within a directory structure on some object store or file system. The information of an array is stored in three different forms. The data of the database is within files, of course, but there’s also necessary information in other places.\n\nDirectory. Certain files and other directories must be present or may be present within the content of a directory.\nFile Name. File names are an encoded set of fields.\nFile. Each type of file has its own data layout.\n\nEach of these entities may be regarded as having syntax and semantics: what a valid entity is and what it means. The scope of this directory is to support these two aspects of array storage.\n\n\nHere’s a partial list of concerns that are out of scope:\n\nStorage access. Nothing in this directory has direct access to an object store or a file system. A user of these classes must provide the results of a storage operation to this code for parsing and interpretation.\nState maintenance. This code has the notion of an “array in a directory”, but it does not provide any notion of “the current state” of an array in storage. The concept here is that of an array a data type, not as a variable of that data type.\n\n\n\n\nThe following is the list of Datatypes that are not supported by the Dimension class. Please note that this information is repeated in the C API, C++ API, and the Dimension class.\n\nTILEDB_CHAR\nTILEDB_BLOB\nTILEDB_BOOL\nTILEDB_STRING_UTF8\nTILEDB_STRING_UTF16\nTILEDB_STRING_UTF32\nTILEDB_STRING_UCS2\nTILEDB_STRING_UCS4\nTILEDB_ANY"
  },
  {
    "objectID": "tiledb/storage_format/DIRECTORY.html#out-of-scope",
    "href": "tiledb/storage_format/DIRECTORY.html#out-of-scope",
    "title": "Storage Format",
    "section": "",
    "text": "Here’s a partial list of concerns that are out of scope:\n\nStorage access. Nothing in this directory has direct access to an object store or a file system. A user of these classes must provide the results of a storage operation to this code for parsing and interpretation.\nState maintenance. This code has the notion of an “array in a directory”, but it does not provide any notion of “the current state” of an array in storage. The concept here is that of an array a data type, not as a variable of that data type."
  },
  {
    "objectID": "tiledb/storage_format/DIRECTORY.html#useful-information",
    "href": "tiledb/storage_format/DIRECTORY.html#useful-information",
    "title": "Storage Format",
    "section": "",
    "text": "The following is the list of Datatypes that are not supported by the Dimension class. Please note that this information is repeated in the C API, C++ API, and the Dimension class.\n\nTILEDB_CHAR\nTILEDB_BLOB\nTILEDB_BOOL\nTILEDB_STRING_UTF8\nTILEDB_STRING_UTF16\nTILEDB_STRING_UTF32\nTILEDB_STRING_UCS2\nTILEDB_STRING_UCS4\nTILEDB_ANY"
  },
  {
    "objectID": "tiledb/common/exception/DIRECTORY.html",
    "href": "tiledb/common/exception/DIRECTORY.html",
    "title": "Directory /tiledb/common/exception",
    "section": "",
    "text": "This directory defines tiledb::common::StatusException and tiledb::common::Status. class Status is a legacy class, originally used as a poor substitute for exceptions. class StatusException is a proper exception class derived from std::exception. StatusException is interconvertible with error Status objects.\n\n\nThe goal is to incrementally replace Status objects with an appropriate replacement. In many cases this will be to throw StatusException instead of returning an error Status, but not in all cases. In some cases a bool may be an appropriate return. In others there might be (or might be created) a semantically appropriate return type that distinguishes between different kinds of failure condititions.\nThere is no particular urgency to this goal. The code has used Status for years. This change is not being made to solve any particular technical defect.\n\n\n\nElimination of code-junk macros RETURN_NOT_OK and their ilk. Error handling is fundamental. It can’t be avoided; it must be faced. In C++ there’s a basic choice to use exceptions or to use return values. There’s syntactic overhead with both. The choice is between try statements with exceptions and return-if-return statements otherwise.\nUsing try statement syntactically marks off error handling code to readily identifiable blocks. Using if statements (or a macro encapsulation) intermixes code for each possible failure in with the code that might fail. Since most failures are rare-to-nonexistent, particularly possible logic errors, try statements end up with code that is far easier to read.\nPromotion of RAII and C.41. When there’s a policy of avoiding exceptions, constructors can’t throw to enforce class invariants. This means that having class invariants requires private constructors and dedicated factory functions in all cases. Of course it’s easier not to do this, which leads to the practice of not defining class invariants. This bypasses the benefits of RAII and obviates the utility of C.41 compliance.\n\n\n\n\n\n\nclass StatusException is available for use. Functions that currently return Status but only do so to signal logic errors are immediate candidates for conversion.\nStatus factory classes are still defined in status.h. Prior to beginning this goal, class Status contained a number of member functions to create Status objects with designated origins. These functions required global declaration where local declaration would have been more appropriate. These functions are still declared in status.h but only because the task of moving all the declarations to more appropriate source files has not yet started.\nStatus still has its legacy implementation pattern. Status is due for one more rewrite. Its current internals can be replaced with a single member variable of type optional&lt;StatusException&gt;."
  },
  {
    "objectID": "tiledb/common/exception/DIRECTORY.html#goal-replace-status-objects",
    "href": "tiledb/common/exception/DIRECTORY.html#goal-replace-status-objects",
    "title": "Directory /tiledb/common/exception",
    "section": "",
    "text": "The goal is to incrementally replace Status objects with an appropriate replacement. In many cases this will be to throw StatusException instead of returning an error Status, but not in all cases. In some cases a bool may be an appropriate return. In others there might be (or might be created) a semantically appropriate return type that distinguishes between different kinds of failure condititions.\nThere is no particular urgency to this goal. The code has used Status for years. This change is not being made to solve any particular technical defect.\n\n\n\nElimination of code-junk macros RETURN_NOT_OK and their ilk. Error handling is fundamental. It can’t be avoided; it must be faced. In C++ there’s a basic choice to use exceptions or to use return values. There’s syntactic overhead with both. The choice is between try statements with exceptions and return-if-return statements otherwise.\nUsing try statement syntactically marks off error handling code to readily identifiable blocks. Using if statements (or a macro encapsulation) intermixes code for each possible failure in with the code that might fail. Since most failures are rare-to-nonexistent, particularly possible logic errors, try statements end up with code that is far easier to read.\nPromotion of RAII and C.41. When there’s a policy of avoiding exceptions, constructors can’t throw to enforce class invariants. This means that having class invariants requires private constructors and dedicated factory functions in all cases. Of course it’s easier not to do this, which leads to the practice of not defining class invariants. This bypasses the benefits of RAII and obviates the utility of C.41 compliance."
  },
  {
    "objectID": "tiledb/common/exception/DIRECTORY.html#progress-toward-the-goal",
    "href": "tiledb/common/exception/DIRECTORY.html#progress-toward-the-goal",
    "title": "Directory /tiledb/common/exception",
    "section": "",
    "text": "class StatusException is available for use. Functions that currently return Status but only do so to signal logic errors are immediate candidates for conversion.\nStatus factory classes are still defined in status.h. Prior to beginning this goal, class Status contained a number of member functions to create Status objects with designated origins. These functions required global declaration where local declaration would have been more appropriate. These functions are still declared in status.h but only because the task of moving all the declarations to more appropriate source files has not yet started.\nStatus still has its legacy implementation pattern. Status is due for one more rewrite. Its current internals can be replaced with a single member variable of type optional&lt;StatusException&gt;."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to TileDB",
    "section": "",
    "text": "Hi! Thanks for your interest in TileDB. The following notes are intended to help you file issues, bug reports, or contribute code to the open source TileDB project.\n\n\n\nReporting a bug? Please read how to file a bug report section to make sure sufficient information is included.\nContributing code? You rock! Be sure to review the contributor section for helpful tips on the tools we use to build TileDB, format code, and issue pull requests (PR)’s.\n\n\n\n\nA useful bug report filed as a GitHub issue provides information about how to reproduce the error.\n\nBefore opening a new GitHub issue try searching the existing issues to see if someone else has already noticed the same problem.\nWhen filing a bug report, provide where possible:\n\n\nThe version TileDB (tiledb_version()) or if a dev version, specific commit that triggers the error.\nThe full error message, including the backtrace (if possible). Verbose error reporting is enabled by building TileDB with ../bootstrap --enable-verbose.\nA minimal working example, i.e. the smallest chunk of code that triggers the error. Ideally, this should be code that can be a small reduced C / C++ source file. If the code to reproduce is somewhat long, consider putting it in a gist.\n\n\nWhen pasting code blocks or output, put triple backquotes (```) around the text so GitHub will format it nicely. Code statements should be surrounded by single backquotes (`). See GitHub’s guide on Markdown for more formatting tricks.\n\n\n\n\nBy contributing code to TileDB, you are agreeing to release it under the MIT License.\n\n\nPlease see building from source for dependencies and detailed build instructions.\nFrom a fork of TileDB\ngit clone https://github.com/username/TileDB\ncd TileDB && mkdir build && cd build\n../bootstrap\nmake && make check\ncd ../\ngit checkout -b &lt;my_initials&gt;/&lt;my_bugfix_branch&gt;\n... code changes ...\nmake -C build format\nmake -C build check\ngit commit -a -m \"my commit message\"\ngit push --set-upstream origin &lt;my_initials&gt;/&lt;my_bugfix_branch&gt;\nIssue a PR from your updated TileDB fork\nBranch conventions: - dev is the development branch of TileDB, all PR’s are merged into dev. - master tracks the latest stable / released version of TileDB. - release-x.y.z are major / bugfix release branches of TileDB.\nFormatting conventions: - 2 spaces per indentation level not tabs - class names use CamelCase - member functions, variables use lowercase with underscores - class member variables use a trailing underscore foo_ - comments are good, TileDB uses doxygen for class doc strings. - format code using clang-format\n\n\n\n\ndev is the development branch, all PR’s should be rebased on top of the latest dev commit.\nCommit changes to a local branch. The convention is to use your initials to identify branches: (ex. “Fred Jones” , fj/my_bugfix_branch). Branch names should be identifiable and reflect the feature or bug that they want to address / fix. This helps in deleting old branches later.\nMake sure the test suite passes by running make check.\nWhen ready to submit a PR, git rebase the branch on top of the latest dev commit. Be sure to squash / cleanup the commit history so that the PR preferably one, or a couple commits at most. Each atomic commit in a PR should be able to pass the test suite.\nRun the limiting / code format tooling (make format) before submitting a final PR. Make sure that your contribution generally follows the format and naming conventions used by surrounding code.\nIf the PR changes/adds/removes user-facing API or system behavior (such as API changes), add a note to the TileDB/HISTORY.md file.\nSubmit a PR, writing a descriptive message. If a PR closes an open issue, reference the issue in the PR message (ex. If an issue closes issue number 10, you would write closes #10)\nMake sure CI (continuous integration) is passing for your PR – click Show all checks in the pull request status box at the bottom of each PR page. The continous integration project pages will also list all recently-built PRs:\n\nAzure Pipelines\n\n\n\n\n\n\nTileDB uses Sphinx as its documentation generator.\nDocumentation is written in reStructuredText markup.\n\nBuilding the docs locally:\n$ cd TileDB/doc\n$ ./local-build.sh\nThis will install all the required packages in a Python virtual environment, and build the docs locally. You can then open the doc/source/_build/html/index.html file in your browser.\nNote: If there are additions to the C/C++ API in between builds, rerun bootstrap:\n$ cd ../build\n$ ../bootstrap &lt;flags&gt;\n\n\n\n\nTileDB\n\nHomepage\nDocumentation\nIssues\nForum\nOrganization\n\nGithub / Git\n\nGit cheatsheet\nGithub Documentation\nForking a Repo\nMore Learning Resources"
  },
  {
    "objectID": "CONTRIBUTING.html#contribution-checklist",
    "href": "CONTRIBUTING.html#contribution-checklist",
    "title": "Contributing to TileDB",
    "section": "",
    "text": "Reporting a bug? Please read how to file a bug report section to make sure sufficient information is included.\nContributing code? You rock! Be sure to review the contributor section for helpful tips on the tools we use to build TileDB, format code, and issue pull requests (PR)’s."
  },
  {
    "objectID": "CONTRIBUTING.html#reporting-a-bug",
    "href": "CONTRIBUTING.html#reporting-a-bug",
    "title": "Contributing to TileDB",
    "section": "",
    "text": "A useful bug report filed as a GitHub issue provides information about how to reproduce the error.\n\nBefore opening a new GitHub issue try searching the existing issues to see if someone else has already noticed the same problem.\nWhen filing a bug report, provide where possible:\n\n\nThe version TileDB (tiledb_version()) or if a dev version, specific commit that triggers the error.\nThe full error message, including the backtrace (if possible). Verbose error reporting is enabled by building TileDB with ../bootstrap --enable-verbose.\nA minimal working example, i.e. the smallest chunk of code that triggers the error. Ideally, this should be code that can be a small reduced C / C++ source file. If the code to reproduce is somewhat long, consider putting it in a gist.\n\n\nWhen pasting code blocks or output, put triple backquotes (```) around the text so GitHub will format it nicely. Code statements should be surrounded by single backquotes (`). See GitHub’s guide on Markdown for more formatting tricks."
  },
  {
    "objectID": "CONTRIBUTING.html#contributing-code",
    "href": "CONTRIBUTING.html#contributing-code",
    "title": "Contributing to TileDB",
    "section": "",
    "text": "By contributing code to TileDB, you are agreeing to release it under the MIT License.\n\n\nPlease see building from source for dependencies and detailed build instructions.\nFrom a fork of TileDB\ngit clone https://github.com/username/TileDB\ncd TileDB && mkdir build && cd build\n../bootstrap\nmake && make check\ncd ../\ngit checkout -b &lt;my_initials&gt;/&lt;my_bugfix_branch&gt;\n... code changes ...\nmake -C build format\nmake -C build check\ngit commit -a -m \"my commit message\"\ngit push --set-upstream origin &lt;my_initials&gt;/&lt;my_bugfix_branch&gt;\nIssue a PR from your updated TileDB fork\nBranch conventions: - dev is the development branch of TileDB, all PR’s are merged into dev. - master tracks the latest stable / released version of TileDB. - release-x.y.z are major / bugfix release branches of TileDB.\nFormatting conventions: - 2 spaces per indentation level not tabs - class names use CamelCase - member functions, variables use lowercase with underscores - class member variables use a trailing underscore foo_ - comments are good, TileDB uses doxygen for class doc strings. - format code using clang-format\n\n\n\n\ndev is the development branch, all PR’s should be rebased on top of the latest dev commit.\nCommit changes to a local branch. The convention is to use your initials to identify branches: (ex. “Fred Jones” , fj/my_bugfix_branch). Branch names should be identifiable and reflect the feature or bug that they want to address / fix. This helps in deleting old branches later.\nMake sure the test suite passes by running make check.\nWhen ready to submit a PR, git rebase the branch on top of the latest dev commit. Be sure to squash / cleanup the commit history so that the PR preferably one, or a couple commits at most. Each atomic commit in a PR should be able to pass the test suite.\nRun the limiting / code format tooling (make format) before submitting a final PR. Make sure that your contribution generally follows the format and naming conventions used by surrounding code.\nIf the PR changes/adds/removes user-facing API or system behavior (such as API changes), add a note to the TileDB/HISTORY.md file.\nSubmit a PR, writing a descriptive message. If a PR closes an open issue, reference the issue in the PR message (ex. If an issue closes issue number 10, you would write closes #10)\nMake sure CI (continuous integration) is passing for your PR – click Show all checks in the pull request status box at the bottom of each PR page. The continous integration project pages will also list all recently-built PRs:\n\nAzure Pipelines\n\n\n\n\n\n\nTileDB uses Sphinx as its documentation generator.\nDocumentation is written in reStructuredText markup.\n\nBuilding the docs locally:\n$ cd TileDB/doc\n$ ./local-build.sh\nThis will install all the required packages in a Python virtual environment, and build the docs locally. You can then open the doc/source/_build/html/index.html file in your browser.\nNote: If there are additions to the C/C++ API in between builds, rerun bootstrap:\n$ cd ../build\n$ ../bootstrap &lt;flags&gt;\n\n\n\n\nTileDB\n\nHomepage\nDocumentation\nIssues\nForum\nOrganization\n\nGithub / Git\n\nGit cheatsheet\nGithub Documentation\nForking a Repo\nMore Learning Resources"
  }
]